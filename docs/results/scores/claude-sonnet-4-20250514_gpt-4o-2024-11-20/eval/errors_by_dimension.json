{
    "unconditional": [
        {
            "id": 0,
            "desc": "您可以使用一系列functions。",
            "other_info": {
                "function": [
                    "get_company_register",
                    "get_company_register_name",
                    "get_sub_company_info",
                    "get_company_info",
                    "get_legal_document_company_list",
                    "finish"
                ],
                "from": "system_para_1"
            },
            "dimension": "unconditional",
            "type": "resource",
            "is_meta": false,
            "evaluation": [
                {
                    "type": "llm",
                    "required_keys": [
                        "response"
                    ],
                    "exec": "请帮我提取出下面这段话中**当前步**使用的函数名称，不关注未来要做的事。\n请仅回答函数名称，并以list形式直接返回给我，不要回答其他的内容。\n\n{response}\n"
                },
                {
                    "type": "code",
                    "exec": "\ndef check_following(response):\n    import re\n\n    def extract_tool(text):\n        pattern = r'\\[([^]]+)\\]'\n        matches = re.findall(pattern, text)\n        if not matches:\n            return []\n        try:\n            tools = eval(f\"[{matches[-1]}]\")\n            return tools\n        except Exception as e:\n            # print(\"error in extract_tool:\", e)\n            return []\n\n    try:\n        tools = ['get_company_register', 'get_company_register_name', 'get_sub_company_info', 'get_company_info', 'get_legal_document_company_list', 'finish']\n        functions = extract_tool(response)\n        if isinstance(functions, list):\n            for item in functions:\n                if item not in tools:\n                    return False\n        return True\n    except Exception as e:\n        print(\"error in resource checker:\", e)\n        return False\n"
                }
            ],
            "score": false
        },
        {
            "id": 1,
            "desc": "注意不要在一次thought中连续使用多个functions。",
            "other_info": {
                "function": [
                    "get_company_register",
                    "get_company_register_name",
                    "get_sub_company_info",
                    "get_company_info",
                    "get_legal_document_company_list",
                    "finish"
                ],
                "from": "system_para_3"
            },
            "dimension": "unconditional",
            "type": "resource",
            "is_meta": false,
            "evaluation": [
                {
                    "type": "llm",
                    "required_keys": [
                        "response"
                    ],
                    "exec": "请帮我提取出下面这段话中**当前步**使用的函数名称，不关注未来要做的事。\n请仅回答函数名称，并以list形式直接返回给我，不要回答其他的内容。\n\n{response}\n"
                },
                {
                    "type": "code",
                    "exec": "\ndef check_following(response):\n    import re\n\n    def extract_tool(text):\n        pattern = r'\\[([^]]+)\\]'\n        matches = re.findall(pattern, text)\n        if not matches:\n            return []\n        try:\n            tools = eval(f\"[{matches[-1]}]\")\n            return tools\n        except Exception as e:\n            # print(\"error in extract_tool:\", e)\n            return []\n\n    try:\n        tools = ['get_company_register', 'get_company_register_name', 'get_sub_company_info', 'get_company_info', 'get_legal_document_company_list', 'finish']\n        functions = extract_tool(response)\n        if len(functions) <= 1:\n            return True\n        return False\n    except Exception as e:\n        print(\"error in resource checker count:\", e)\n        return False\n"
                }
            ],
            "score": false
        },
        {
            "id": 3,
            "desc": "</thought> 结束",
            "other_info": {
                "from": "system_para_1"
            },
            "dimension": "unconditional",
            "type": "formatting",
            "is_meta": false,
            "evaluation": [
                {
                    "type": "code",
                    "exec": "\ndef check_following(response):\n    return response.rstrip().endswith(\"</thought>\")\n"
                }
            ],
            "score": false
        },
        {
            "id": 5,
            "desc": "您可以在thought中使用python packages,但只能从以下packages列表中选择：",
            "other_info": {
                "available_packages": [
                    "math",
                    "time",
                    "stat",
                    "queue",
                    "itertools",
                    "statistics",
                    "random",
                    "collections",
                    "unicodedata"
                ]
            },
            "dimension": "unconditional",
            "type": "resource",
            "is_meta": false,
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "提取下面文本中的所有的调用package，直接返回list格式结果，不要任何解释、前缀或赋值语句。\n输出格式要求：[\"package1\", \"package2\", ...]:\n\nHere is model response: {response}"
                },
                {
                    "type": "code",
                    "exec": "\nimport ast\n\ndef check_following(response: str) -> bool:\n    available_packages = ['math', 'time', 'stat', 'queue', 'itertools', 'statistics', 'random', 'collections', 'unicodedata']\n    try:\n        items = ast.literal_eval(response)\n        return isinstance(items, list) and all(item in available_packages for item in items)\n    except:\n        return False\n"
                }
            ],
            "score": false
        },
        {
            "id": 4,
            "desc": "除了第一轮外，对上一轮的<observation></observation>进行总结。",
            "other_info": {
                "from": "system_para_2"
            },
            "dimension": "unconditional",
            "type": "semantic",
            "is_meta": false,
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "response是否在一开始就总结了上一轮的<observation></observation>？直接回答 YES 或 NO。\n\nresponse: {response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": 0,
            "desc": "您可以使用一系列functions。",
            "other_info": {
                "function": [
                    "get_company_info",
                    "get_sub_company_info",
                    "get_company_register",
                    "get_sub_company_info_list",
                    "get_company_register_name",
                    "finish"
                ],
                "from": "system_para_1"
            },
            "dimension": "unconditional",
            "type": "resource",
            "is_meta": false,
            "evaluation": [
                {
                    "type": "llm",
                    "required_keys": [
                        "response"
                    ],
                    "exec": "请帮我提取出下面这段话中**当前步**使用的函数名称，不关注未来要做的事。\n请仅回答函数名称，并以list形式直接返回给我，不要回答其他的内容。\n\n{response}\n"
                },
                {
                    "type": "code",
                    "exec": "\ndef check_following(response):\n    import re\n\n    def extract_tool(text):\n        pattern = r'\\[([^]]+)\\]'\n        matches = re.findall(pattern, text)\n        if not matches:\n            return []\n        try:\n            tools = eval(f\"[{matches[-1]}]\")\n            return tools\n        except Exception as e:\n            # print(\"error in extract_tool:\", e)\n            return []\n\n    try:\n        tools = ['get_company_info', 'get_sub_company_info', 'get_company_register', 'get_sub_company_info_list', 'get_company_register_name', 'finish']\n        functions = extract_tool(response)\n        if isinstance(functions, list):\n            for item in functions:\n                if item not in tools:\n                    return False\n        return True\n    except Exception as e:\n        print(\"error in resource checker:\", e)\n        return False\n"
                }
            ],
            "score": false
        },
        {
            "id": 1,
            "desc": "注意不要在一次thought中连续使用多个functions。",
            "other_info": {
                "function": [
                    "get_company_info",
                    "get_sub_company_info",
                    "get_company_register",
                    "get_sub_company_info_list",
                    "get_company_register_name",
                    "finish"
                ],
                "from": "system_para_3"
            },
            "dimension": "unconditional",
            "type": "resource",
            "is_meta": false,
            "evaluation": [
                {
                    "type": "llm",
                    "required_keys": [
                        "response"
                    ],
                    "exec": "请帮我提取出下面这段话中**当前步**使用的函数名称，不关注未来要做的事。\n请仅回答函数名称，并以list形式直接返回给我，不要回答其他的内容。\n\n{response}\n"
                },
                {
                    "type": "code",
                    "exec": "\ndef check_following(response):\n    import re\n\n    def extract_tool(text):\n        pattern = r'\\[([^]]+)\\]'\n        matches = re.findall(pattern, text)\n        if not matches:\n            return []\n        try:\n            tools = eval(f\"[{matches[-1]}]\")\n            return tools\n        except Exception as e:\n            # print(\"error in extract_tool:\", e)\n            return []\n\n    try:\n        tools = ['get_company_info', 'get_sub_company_info', 'get_company_register', 'get_sub_company_info_list', 'get_company_register_name', 'finish']\n        functions = extract_tool(response)\n        if len(functions) <= 1:\n            return True\n        return False\n    except Exception as e:\n        print(\"error in resource checker count:\", e)\n        return False\n"
                }
            ],
            "score": false
        },
        {
            "id": 3,
            "desc": "</thought> 结束",
            "other_info": {
                "from": "system_para_1"
            },
            "dimension": "unconditional",
            "type": "formatting",
            "is_meta": false,
            "evaluation": [
                {
                    "type": "code",
                    "exec": "\ndef check_following(response):\n    return response.rstrip().endswith(\"</thought>\")\n"
                }
            ],
            "score": false
        },
        {
            "id": 5,
            "desc": "您可以在thought中使用python packages,但只能从以下packages列表中选择：",
            "other_info": {
                "available_packages": [
                    "math",
                    "time",
                    "stat",
                    "queue",
                    "itertools",
                    "statistics",
                    "random",
                    "collections",
                    "unicodedata"
                ]
            },
            "dimension": "unconditional",
            "type": "resource",
            "is_meta": false,
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "提取下面文本中的所有的调用package，直接返回list格式结果，不要任何解释、前缀或赋值语句。\n输出格式要求：[\"package1\", \"package2\", ...]:\n\nHere is model response: {response}"
                },
                {
                    "type": "code",
                    "exec": "\nimport ast\n\ndef check_following(response: str) -> bool:\n    available_packages = ['math', 'time', 'stat', 'queue', 'itertools', 'statistics', 'random', 'collections', 'unicodedata']\n    try:\n        items = ast.literal_eval(response)\n        return isinstance(items, list) and all(item in available_packages for item in items)\n    except:\n        return False\n"
                }
            ],
            "score": false
        },
        {
            "id": 4,
            "desc": "除了第一轮外，对上一轮的<observation></observation>进行总结。",
            "other_info": {
                "from": "system_para_2"
            },
            "dimension": "unconditional",
            "type": "semantic",
            "is_meta": false,
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "response是否在一开始就总结了上一轮的<observation></observation>？直接回答 YES 或 NO。\n\nresponse: {response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": 0,
            "desc": "您可以使用一系列functions。",
            "other_info": {
                "function": [
                    "get_company_register_name",
                    "get_company_register",
                    "get_company_info",
                    "get_sub_company_info",
                    "get_sub_company_info_list",
                    "finish"
                ],
                "from": "system_para_1"
            },
            "dimension": "unconditional",
            "type": "resource",
            "is_meta": false,
            "evaluation": [
                {
                    "type": "llm",
                    "required_keys": [
                        "response"
                    ],
                    "exec": "请帮我提取出下面这段话中**当前步**使用的函数名称，不关注未来要做的事。\n请仅回答函数名称，并以list形式直接返回给我，不要回答其他的内容。\n\n{response}\n"
                },
                {
                    "type": "code",
                    "exec": "\ndef check_following(response):\n    import re\n\n    def extract_tool(text):\n        pattern = r'\\[([^]]+)\\]'\n        matches = re.findall(pattern, text)\n        if not matches:\n            return []\n        try:\n            tools = eval(f\"[{matches[-1]}]\")\n            return tools\n        except Exception as e:\n            # print(\"error in extract_tool:\", e)\n            return []\n\n    try:\n        tools = ['get_company_register_name', 'get_company_register', 'get_company_info', 'get_sub_company_info', 'get_sub_company_info_list', 'finish']\n        functions = extract_tool(response)\n        if isinstance(functions, list):\n            for item in functions:\n                if item not in tools:\n                    return False\n        return True\n    except Exception as e:\n        print(\"error in resource checker:\", e)\n        return False\n"
                }
            ],
            "score": false
        },
        {
            "id": 1,
            "desc": "注意不要在一次thought中连续使用多个functions。",
            "other_info": {
                "function": [
                    "get_company_register_name",
                    "get_company_register",
                    "get_company_info",
                    "get_sub_company_info",
                    "get_sub_company_info_list",
                    "finish"
                ],
                "from": "system_para_3"
            },
            "dimension": "unconditional",
            "type": "resource",
            "is_meta": false,
            "evaluation": [
                {
                    "type": "llm",
                    "required_keys": [
                        "response"
                    ],
                    "exec": "请帮我提取出下面这段话中**当前步**使用的函数名称，不关注未来要做的事。\n请仅回答函数名称，并以list形式直接返回给我，不要回答其他的内容。\n\n{response}\n"
                },
                {
                    "type": "code",
                    "exec": "\ndef check_following(response):\n    import re\n\n    def extract_tool(text):\n        pattern = r'\\[([^]]+)\\]'\n        matches = re.findall(pattern, text)\n        if not matches:\n            return []\n        try:\n            tools = eval(f\"[{matches[-1]}]\")\n            return tools\n        except Exception as e:\n            # print(\"error in extract_tool:\", e)\n            return []\n\n    try:\n        tools = ['get_company_register_name', 'get_company_register', 'get_company_info', 'get_sub_company_info', 'get_sub_company_info_list', 'finish']\n        functions = extract_tool(response)\n        if len(functions) <= 1:\n            return True\n        return False\n    except Exception as e:\n        print(\"error in resource checker count:\", e)\n        return False\n"
                }
            ],
            "score": false
        },
        {
            "id": 3,
            "desc": "</thought> 结束",
            "other_info": {
                "from": "system_para_1"
            },
            "dimension": "unconditional",
            "type": "formatting",
            "is_meta": false,
            "evaluation": [
                {
                    "type": "code",
                    "exec": "\ndef check_following(response):\n    return response.rstrip().endswith(\"</thought>\")\n"
                }
            ],
            "score": false
        },
        {
            "id": 5,
            "desc": "您可以在thought中使用python packages,但只能从以下packages列表中选择：",
            "other_info": {
                "available_packages": [
                    "math",
                    "time",
                    "stat",
                    "queue",
                    "itertools",
                    "statistics",
                    "random",
                    "collections",
                    "unicodedata"
                ]
            },
            "dimension": "unconditional",
            "type": "resource",
            "is_meta": false,
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "提取下面文本中的所有的调用package，直接返回list格式结果，不要任何解释、前缀或赋值语句。\n输出格式要求：[\"package1\", \"package2\", ...]:\n\nHere is model response: {response}"
                },
                {
                    "type": "code",
                    "exec": "\nimport ast\n\ndef check_following(response: str) -> bool:\n    available_packages = ['math', 'time', 'stat', 'queue', 'itertools', 'statistics', 'random', 'collections', 'unicodedata']\n    try:\n        items = ast.literal_eval(response)\n        return isinstance(items, list) and all(item in available_packages for item in items)\n    except:\n        return False\n"
                }
            ],
            "score": false
        },
        {
            "id": 4,
            "desc": "除了第一轮外，对上一轮的<observation></observation>进行总结。",
            "other_info": {
                "from": "system_para_2"
            },
            "dimension": "unconditional",
            "type": "semantic",
            "is_meta": false,
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "response是否在一开始就总结了上一轮的<observation></observation>？直接回答 YES 或 NO。\n\nresponse: {response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": 3,
            "desc": "</thought> 结束",
            "other_info": {
                "from": "system_para_1"
            },
            "dimension": "unconditional",
            "type": "formatting",
            "is_meta": false,
            "evaluation": [
                {
                    "type": "code",
                    "exec": "\ndef check_following(response):\n    return response.rstrip().endswith(\"</thought>\")\n"
                }
            ],
            "score": false
        },
        {
            "id": 5,
            "desc": "您可以在thought中使用python packages,但只能从以下packages列表中选择：",
            "other_info": {
                "available_packages": [
                    "math",
                    "time",
                    "stat",
                    "queue",
                    "itertools",
                    "statistics",
                    "random",
                    "collections",
                    "unicodedata"
                ]
            },
            "dimension": "unconditional",
            "type": "resource",
            "is_meta": false,
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "提取下面文本中的所有的调用package，直接返回list格式结果，不要任何解释、前缀或赋值语句。\n输出格式要求：[\"package1\", \"package2\", ...]:\n\nHere is model response: {response}"
                },
                {
                    "type": "code",
                    "exec": "\nimport ast\n\ndef check_following(response: str) -> bool:\n    available_packages = ['math', 'time', 'stat', 'queue', 'itertools', 'statistics', 'random', 'collections', 'unicodedata']\n    try:\n        items = ast.literal_eval(response)\n        return isinstance(items, list) and all(item in available_packages for item in items)\n    except:\n        return False\n"
                }
            ],
            "score": false
        },
        {
            "id": 4,
            "desc": "除了第一轮外，对上一轮的<observation></observation>进行总结。",
            "other_info": {
                "from": "system_para_2"
            },
            "dimension": "unconditional",
            "type": "semantic",
            "is_meta": false,
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "response是否在一开始就总结了上一轮的<observation></observation>？直接回答 YES 或 NO。\n\nresponse: {response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": 0,
            "desc": "您可以使用一系列functions。",
            "other_info": {
                "function": [
                    "get_company_register",
                    "get_company_register_name",
                    "get_company_info",
                    "get_sub_company_info",
                    "get_legal_document_company_list",
                    "finish"
                ],
                "from": "system_para_1"
            },
            "dimension": "unconditional",
            "type": "resource",
            "is_meta": false,
            "evaluation": [
                {
                    "type": "llm",
                    "required_keys": [
                        "response"
                    ],
                    "exec": "请帮我提取出下面这段话中**当前步**使用的函数名称，不关注未来要做的事。\n请仅回答函数名称，并以list形式直接返回给我，不要回答其他的内容。\n\n{response}\n"
                },
                {
                    "type": "code",
                    "exec": "\ndef check_following(response):\n    import re\n\n    def extract_tool(text):\n        pattern = r'\\[([^]]+)\\]'\n        matches = re.findall(pattern, text)\n        if not matches:\n            return []\n        try:\n            tools = eval(f\"[{matches[-1]}]\")\n            return tools\n        except Exception as e:\n            # print(\"error in extract_tool:\", e)\n            return []\n\n    try:\n        tools = ['get_company_register', 'get_company_register_name', 'get_company_info', 'get_sub_company_info', 'get_legal_document_company_list', 'finish']\n        functions = extract_tool(response)\n        if isinstance(functions, list):\n            for item in functions:\n                if item not in tools:\n                    return False\n        return True\n    except Exception as e:\n        print(\"error in resource checker:\", e)\n        return False\n"
                }
            ],
            "score": false
        },
        {
            "id": 3,
            "desc": "</thought> 结束",
            "other_info": {
                "from": "system_para_1"
            },
            "dimension": "unconditional",
            "type": "formatting",
            "is_meta": false,
            "evaluation": [
                {
                    "type": "code",
                    "exec": "\ndef check_following(response):\n    return response.rstrip().endswith(\"</thought>\")\n"
                }
            ],
            "score": false
        },
        {
            "id": 5,
            "desc": "您可以在thought中使用python packages,但只能从以下packages列表中选择：",
            "other_info": {
                "available_packages": [
                    "math",
                    "time",
                    "stat",
                    "queue",
                    "itertools",
                    "statistics",
                    "random",
                    "collections",
                    "unicodedata"
                ]
            },
            "dimension": "unconditional",
            "type": "resource",
            "is_meta": false,
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "提取下面文本中的所有的调用package，直接返回list格式结果，不要任何解释、前缀或赋值语句。\n输出格式要求：[\"package1\", \"package2\", ...]:\n\nHere is model response: {response}"
                },
                {
                    "type": "code",
                    "exec": "\nimport ast\n\ndef check_following(response: str) -> bool:\n    available_packages = ['math', 'time', 'stat', 'queue', 'itertools', 'statistics', 'random', 'collections', 'unicodedata']\n    try:\n        items = ast.literal_eval(response)\n        return isinstance(items, list) and all(item in available_packages for item in items)\n    except:\n        return False\n"
                }
            ],
            "score": false
        },
        {
            "id": 4,
            "desc": "除了第一轮外，对上一轮的<observation></observation>进行总结。",
            "other_info": {
                "from": "system_para_2"
            },
            "dimension": "unconditional",
            "type": "semantic",
            "is_meta": false,
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "response是否在一开始就总结了上一轮的<observation></observation>？直接回答 YES 或 NO。\n\nresponse: {response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": 3,
            "desc": "</thought> 结束",
            "other_info": {
                "from": "system_para_1"
            },
            "dimension": "unconditional",
            "type": "formatting",
            "is_meta": false,
            "evaluation": [
                {
                    "type": "code",
                    "exec": "\ndef check_following(response):\n    return response.rstrip().endswith(\"</thought>\")\n"
                }
            ],
            "score": false
        },
        {
            "id": 5,
            "desc": "您可以在thought中使用python packages,但只能从以下packages列表中选择：",
            "other_info": {
                "available_packages": [
                    "math",
                    "time",
                    "stat",
                    "queue",
                    "itertools",
                    "statistics",
                    "random",
                    "collections",
                    "unicodedata"
                ]
            },
            "dimension": "unconditional",
            "type": "resource",
            "is_meta": false,
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "提取下面文本中的所有的调用package，直接返回list格式结果，不要任何解释、前缀或赋值语句。\n输出格式要求：[\"package1\", \"package2\", ...]:\n\nHere is model response: {response}"
                },
                {
                    "type": "code",
                    "exec": "\nimport ast\n\ndef check_following(response: str) -> bool:\n    available_packages = ['math', 'time', 'stat', 'queue', 'itertools', 'statistics', 'random', 'collections', 'unicodedata']\n    try:\n        items = ast.literal_eval(response)\n        return isinstance(items, list) and all(item in available_packages for item in items)\n    except:\n        return False\n"
                }
            ],
            "score": false
        },
        {
            "id": 0,
            "desc": "您可以使用一系列functions。",
            "other_info": {
                "function": [
                    "get_sub_company_info_list",
                    "get_sub_company_info",
                    "get_company_register",
                    "get_company_info",
                    "get_finalized_case_company_list",
                    "finish"
                ],
                "from": "system_para_1"
            },
            "dimension": "unconditional",
            "type": "resource",
            "is_meta": false,
            "evaluation": [
                {
                    "type": "llm",
                    "required_keys": [
                        "response"
                    ],
                    "exec": "请帮我提取出下面这段话中**当前步**使用的函数名称，不关注未来要做的事。\n请仅回答函数名称，并以list形式直接返回给我，不要回答其他的内容。\n\n{response}\n"
                },
                {
                    "type": "code",
                    "exec": "\ndef check_following(response):\n    import re\n\n    def extract_tool(text):\n        pattern = r'\\[([^]]+)\\]'\n        matches = re.findall(pattern, text)\n        if not matches:\n            return []\n        try:\n            tools = eval(f\"[{matches[-1]}]\")\n            return tools\n        except Exception as e:\n            # print(\"error in extract_tool:\", e)\n            return []\n\n    try:\n        tools = ['get_sub_company_info_list', 'get_sub_company_info', 'get_company_register', 'get_company_info', 'get_finalized_case_company_list', 'finish']\n        functions = extract_tool(response)\n        if isinstance(functions, list):\n            for item in functions:\n                if item not in tools:\n                    return False\n        return True\n    except Exception as e:\n        print(\"error in resource checker:\", e)\n        return False\n"
                }
            ],
            "score": false
        },
        {
            "id": 1,
            "desc": "注意不要在一次thought中连续使用多个functions。",
            "other_info": {
                "function": [
                    "get_sub_company_info_list",
                    "get_sub_company_info",
                    "get_company_register",
                    "get_company_info",
                    "get_finalized_case_company_list",
                    "finish"
                ],
                "from": "system_para_3"
            },
            "dimension": "unconditional",
            "type": "resource",
            "is_meta": false,
            "evaluation": [
                {
                    "type": "llm",
                    "required_keys": [
                        "response"
                    ],
                    "exec": "请帮我提取出下面这段话中**当前步**使用的函数名称，不关注未来要做的事。\n请仅回答函数名称，并以list形式直接返回给我，不要回答其他的内容。\n\n{response}\n"
                },
                {
                    "type": "code",
                    "exec": "\ndef check_following(response):\n    import re\n\n    def extract_tool(text):\n        pattern = r'\\[([^]]+)\\]'\n        matches = re.findall(pattern, text)\n        if not matches:\n            return []\n        try:\n            tools = eval(f\"[{matches[-1]}]\")\n            return tools\n        except Exception as e:\n            # print(\"error in extract_tool:\", e)\n            return []\n\n    try:\n        tools = ['get_sub_company_info_list', 'get_sub_company_info', 'get_company_register', 'get_company_info', 'get_finalized_case_company_list', 'finish']\n        functions = extract_tool(response)\n        if len(functions) <= 1:\n            return True\n        return False\n    except Exception as e:\n        print(\"error in resource checker count:\", e)\n        return False\n"
                }
            ],
            "score": false
        },
        {
            "id": 3,
            "desc": "</thought> 结束",
            "other_info": {
                "from": "system_para_1"
            },
            "dimension": "unconditional",
            "type": "formatting",
            "is_meta": false,
            "evaluation": [
                {
                    "type": "code",
                    "exec": "\ndef check_following(response):\n    return response.rstrip().endswith(\"</thought>\")\n"
                }
            ],
            "score": false
        },
        {
            "id": 5,
            "desc": "您可以在thought中使用python packages,但只能从以下packages列表中选择：",
            "other_info": {
                "available_packages": [
                    "math",
                    "time",
                    "stat",
                    "queue",
                    "itertools",
                    "statistics",
                    "random",
                    "collections",
                    "unicodedata"
                ]
            },
            "dimension": "unconditional",
            "type": "resource",
            "is_meta": false,
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "提取下面文本中的所有的调用package，直接返回list格式结果，不要任何解释、前缀或赋值语句。\n输出格式要求：[\"package1\", \"package2\", ...]:\n\nHere is model response: {response}"
                },
                {
                    "type": "code",
                    "exec": "\nimport ast\n\ndef check_following(response: str) -> bool:\n    available_packages = ['math', 'time', 'stat', 'queue', 'itertools', 'statistics', 'random', 'collections', 'unicodedata']\n    try:\n        items = ast.literal_eval(response)\n        return isinstance(items, list) and all(item in available_packages for item in items)\n    except:\n        return False\n"
                }
            ],
            "score": false
        },
        {
            "id": 4,
            "desc": "除了第一轮外，对上一轮的<observation></observation>进行总结。",
            "other_info": {
                "from": "system_para_2"
            },
            "dimension": "unconditional",
            "type": "semantic",
            "is_meta": false,
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "response是否在一开始就总结了上一轮的<observation></observation>？直接回答 YES 或 NO。\n\nresponse: {response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": 0,
            "desc": "您可以使用一系列functions。",
            "other_info": {
                "function": [
                    "get_company_info",
                    "get_company_register_name",
                    "get_sub_company_info",
                    "get_sub_company_info_list",
                    "get_company_register",
                    "finish"
                ],
                "from": "system_para_1"
            },
            "dimension": "unconditional",
            "type": "resource",
            "is_meta": false,
            "evaluation": [
                {
                    "type": "llm",
                    "required_keys": [
                        "response"
                    ],
                    "exec": "请帮我提取出下面这段话中**当前步**使用的函数名称，不关注未来要做的事。\n请仅回答函数名称，并以list形式直接返回给我，不要回答其他的内容。\n\n{response}\n"
                },
                {
                    "type": "code",
                    "exec": "\ndef check_following(response):\n    import re\n\n    def extract_tool(text):\n        pattern = r'\\[([^]]+)\\]'\n        matches = re.findall(pattern, text)\n        if not matches:\n            return []\n        try:\n            tools = eval(f\"[{matches[-1]}]\")\n            return tools\n        except Exception as e:\n            # print(\"error in extract_tool:\", e)\n            return []\n\n    try:\n        tools = ['get_company_info', 'get_company_register_name', 'get_sub_company_info', 'get_sub_company_info_list', 'get_company_register', 'finish']\n        functions = extract_tool(response)\n        if isinstance(functions, list):\n            for item in functions:\n                if item not in tools:\n                    return False\n        return True\n    except Exception as e:\n        print(\"error in resource checker:\", e)\n        return False\n"
                }
            ],
            "score": false
        },
        {
            "id": 1,
            "desc": "注意不要在一次thought中连续使用多个functions。",
            "other_info": {
                "function": [
                    "get_company_info",
                    "get_company_register_name",
                    "get_sub_company_info",
                    "get_sub_company_info_list",
                    "get_company_register",
                    "finish"
                ],
                "from": "system_para_3"
            },
            "dimension": "unconditional",
            "type": "resource",
            "is_meta": false,
            "evaluation": [
                {
                    "type": "llm",
                    "required_keys": [
                        "response"
                    ],
                    "exec": "请帮我提取出下面这段话中**当前步**使用的函数名称，不关注未来要做的事。\n请仅回答函数名称，并以list形式直接返回给我，不要回答其他的内容。\n\n{response}\n"
                },
                {
                    "type": "code",
                    "exec": "\ndef check_following(response):\n    import re\n\n    def extract_tool(text):\n        pattern = r'\\[([^]]+)\\]'\n        matches = re.findall(pattern, text)\n        if not matches:\n            return []\n        try:\n            tools = eval(f\"[{matches[-1]}]\")\n            return tools\n        except Exception as e:\n            # print(\"error in extract_tool:\", e)\n            return []\n\n    try:\n        tools = ['get_company_info', 'get_company_register_name', 'get_sub_company_info', 'get_sub_company_info_list', 'get_company_register', 'finish']\n        functions = extract_tool(response)\n        if len(functions) <= 1:\n            return True\n        return False\n    except Exception as e:\n        print(\"error in resource checker count:\", e)\n        return False\n"
                }
            ],
            "score": false
        },
        {
            "id": 3,
            "desc": "</thought> 结束",
            "other_info": {
                "from": "system_para_1"
            },
            "dimension": "unconditional",
            "type": "formatting",
            "is_meta": false,
            "evaluation": [
                {
                    "type": "code",
                    "exec": "\ndef check_following(response):\n    return response.rstrip().endswith(\"</thought>\")\n"
                }
            ],
            "score": false
        },
        {
            "id": 5,
            "desc": "您可以在thought中使用python packages,但只能从以下packages列表中选择：",
            "other_info": {
                "available_packages": [
                    "math",
                    "time",
                    "stat",
                    "queue",
                    "itertools",
                    "statistics",
                    "random",
                    "collections",
                    "unicodedata"
                ]
            },
            "dimension": "unconditional",
            "type": "resource",
            "is_meta": false,
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "提取下面文本中的所有的调用package，直接返回list格式结果，不要任何解释、前缀或赋值语句。\n输出格式要求：[\"package1\", \"package2\", ...]:\n\nHere is model response: {response}"
                },
                {
                    "type": "code",
                    "exec": "\nimport ast\n\ndef check_following(response: str) -> bool:\n    available_packages = ['math', 'time', 'stat', 'queue', 'itertools', 'statistics', 'random', 'collections', 'unicodedata']\n    try:\n        items = ast.literal_eval(response)\n        return isinstance(items, list) and all(item in available_packages for item in items)\n    except:\n        return False\n"
                }
            ],
            "score": false
        },
        {
            "id": 4,
            "desc": "除了第一轮外，对上一轮的<observation></observation>进行总结。",
            "other_info": {
                "from": "system_para_2"
            },
            "dimension": "unconditional",
            "type": "semantic",
            "is_meta": false,
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "response是否在一开始就总结了上一轮的<observation></observation>？直接回答 YES 或 NO。\n\nresponse: {response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": 3,
            "desc": "</thought> 结束",
            "other_info": {
                "from": "system_para_1"
            },
            "dimension": "unconditional",
            "type": "formatting",
            "is_meta": false,
            "evaluation": [
                {
                    "type": "code",
                    "exec": "\ndef check_following(response):\n    return response.rstrip().endswith(\"</thought>\")\n"
                }
            ],
            "score": false
        },
        {
            "id": 5,
            "desc": "您可以在thought中使用python packages,但只能从以下packages列表中选择：",
            "other_info": {
                "available_packages": [
                    "math",
                    "time",
                    "stat",
                    "queue",
                    "itertools",
                    "statistics",
                    "random",
                    "collections",
                    "unicodedata"
                ]
            },
            "dimension": "unconditional",
            "type": "resource",
            "is_meta": false,
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "提取下面文本中的所有的调用package，直接返回list格式结果，不要任何解释、前缀或赋值语句。\n输出格式要求：[\"package1\", \"package2\", ...]:\n\nHere is model response: {response}"
                },
                {
                    "type": "code",
                    "exec": "\nimport ast\n\ndef check_following(response: str) -> bool:\n    available_packages = ['math', 'time', 'stat', 'queue', 'itertools', 'statistics', 'random', 'collections', 'unicodedata']\n    try:\n        items = ast.literal_eval(response)\n        return isinstance(items, list) and all(item in available_packages for item in items)\n    except:\n        return False\n"
                }
            ],
            "score": false
        },
        {
            "id": 4,
            "desc": "除了第一轮外，对上一轮的<observation></observation>进行总结。",
            "other_info": {
                "from": "system_para_2"
            },
            "dimension": "unconditional",
            "type": "semantic",
            "is_meta": false,
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "response是否在一开始就总结了上一轮的<observation></observation>？直接回答 YES 或 NO。\n\nresponse: {response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": 3,
            "desc": "</thought> 结束",
            "other_info": {
                "from": "system_para_1"
            },
            "dimension": "unconditional",
            "type": "formatting",
            "is_meta": false,
            "evaluation": [
                {
                    "type": "code",
                    "exec": "\ndef check_following(response):\n    return response.rstrip().endswith(\"</thought>\")\n"
                }
            ],
            "score": false
        },
        {
            "id": 5,
            "desc": "您可以在thought中使用python packages,但只能从以下packages列表中选择：",
            "other_info": {
                "available_packages": [
                    "math",
                    "time",
                    "stat",
                    "queue",
                    "itertools",
                    "statistics",
                    "random",
                    "collections",
                    "unicodedata"
                ]
            },
            "dimension": "unconditional",
            "type": "resource",
            "is_meta": false,
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "提取下面文本中的所有的调用package，直接返回list格式结果，不要任何解释、前缀或赋值语句。\n输出格式要求：[\"package1\", \"package2\", ...]:\n\nHere is model response: {response}"
                },
                {
                    "type": "code",
                    "exec": "\nimport ast\n\ndef check_following(response: str) -> bool:\n    available_packages = ['math', 'time', 'stat', 'queue', 'itertools', 'statistics', 'random', 'collections', 'unicodedata']\n    try:\n        items = ast.literal_eval(response)\n        return isinstance(items, list) and all(item in available_packages for item in items)\n    except:\n        return False\n"
                }
            ],
            "score": false
        },
        {
            "id": 4,
            "desc": "除了第一轮外，对上一轮的<observation></observation>进行总结。",
            "other_info": {
                "from": "system_para_2"
            },
            "dimension": "unconditional",
            "type": "semantic",
            "is_meta": false,
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "response是否在一开始就总结了上一轮的<observation></observation>？直接回答 YES 或 NO。\n\nresponse: {response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": 0,
            "desc": "您可以使用一系列functions。",
            "other_info": {
                "function": [
                    "get_sub_company_info_list",
                    "get_sub_company_info",
                    "get_company_info",
                    "get_company_register",
                    "get_court_info_list",
                    "finish"
                ],
                "from": "system_para_1"
            },
            "dimension": "unconditional",
            "type": "resource",
            "is_meta": false,
            "evaluation": [
                {
                    "type": "llm",
                    "required_keys": [
                        "response"
                    ],
                    "exec": "请帮我提取出下面这段话中**当前步**使用的函数名称，不关注未来要做的事。\n请仅回答函数名称，并以list形式直接返回给我，不要回答其他的内容。\n\n{response}\n"
                },
                {
                    "type": "code",
                    "exec": "\ndef check_following(response):\n    import re\n\n    def extract_tool(text):\n        pattern = r'\\[([^]]+)\\]'\n        matches = re.findall(pattern, text)\n        if not matches:\n            return []\n        try:\n            tools = eval(f\"[{matches[-1]}]\")\n            return tools\n        except Exception as e:\n            # print(\"error in extract_tool:\", e)\n            return []\n\n    try:\n        tools = ['get_sub_company_info_list', 'get_sub_company_info', 'get_company_info', 'get_company_register', 'get_court_info_list', 'finish']\n        functions = extract_tool(response)\n        if isinstance(functions, list):\n            for item in functions:\n                if item not in tools:\n                    return False\n        return True\n    except Exception as e:\n        print(\"error in resource checker:\", e)\n        return False\n"
                }
            ],
            "score": false
        },
        {
            "id": 3,
            "desc": "</thought> 结束",
            "other_info": {
                "from": "system_para_1"
            },
            "dimension": "unconditional",
            "type": "formatting",
            "is_meta": false,
            "evaluation": [
                {
                    "type": "code",
                    "exec": "\ndef check_following(response):\n    return response.rstrip().endswith(\"</thought>\")\n"
                }
            ],
            "score": false
        },
        {
            "id": 5,
            "desc": "您可以在thought中使用python packages,但只能从以下packages列表中选择：",
            "other_info": {
                "available_packages": [
                    "math",
                    "time",
                    "stat",
                    "queue",
                    "itertools",
                    "statistics",
                    "random",
                    "collections",
                    "unicodedata"
                ]
            },
            "dimension": "unconditional",
            "type": "resource",
            "is_meta": false,
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "提取下面文本中的所有的调用package，直接返回list格式结果，不要任何解释、前缀或赋值语句。\n输出格式要求：[\"package1\", \"package2\", ...]:\n\nHere is model response: {response}"
                },
                {
                    "type": "code",
                    "exec": "\nimport ast\n\ndef check_following(response: str) -> bool:\n    available_packages = ['math', 'time', 'stat', 'queue', 'itertools', 'statistics', 'random', 'collections', 'unicodedata']\n    try:\n        items = ast.literal_eval(response)\n        return isinstance(items, list) and all(item in available_packages for item in items)\n    except:\n        return False\n"
                }
            ],
            "score": false
        },
        {
            "id": 4,
            "desc": "除了第一轮外，对上一轮的<observation></observation>进行总结。",
            "other_info": {
                "from": "system_para_2"
            },
            "dimension": "unconditional",
            "type": "semantic",
            "is_meta": false,
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "response是否在一开始就总结了上一轮的<observation></observation>？直接回答 YES 或 NO。\n\nresponse: {response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": 0,
            "desc": "您可以使用一系列functions。",
            "other_info": {
                "function": [
                    "get_sub_company_info",
                    "get_company_info",
                    "get_company_register",
                    "get_sub_company_info_list",
                    "get_company_register_name",
                    "finish"
                ],
                "from": "system_para_1"
            },
            "dimension": "unconditional",
            "type": "resource",
            "is_meta": false,
            "evaluation": [
                {
                    "type": "llm",
                    "required_keys": [
                        "response"
                    ],
                    "exec": "请帮我提取出下面这段话中**当前步**使用的函数名称，不关注未来要做的事。\n请仅回答函数名称，并以list形式直接返回给我，不要回答其他的内容。\n\n{response}\n"
                },
                {
                    "type": "code",
                    "exec": "\ndef check_following(response):\n    import re\n\n    def extract_tool(text):\n        pattern = r'\\[([^]]+)\\]'\n        matches = re.findall(pattern, text)\n        if not matches:\n            return []\n        try:\n            tools = eval(f\"[{matches[-1]}]\")\n            return tools\n        except Exception as e:\n            # print(\"error in extract_tool:\", e)\n            return []\n\n    try:\n        tools = ['get_sub_company_info', 'get_company_info', 'get_company_register', 'get_sub_company_info_list', 'get_company_register_name', 'finish']\n        functions = extract_tool(response)\n        if isinstance(functions, list):\n            for item in functions:\n                if item not in tools:\n                    return False\n        return True\n    except Exception as e:\n        print(\"error in resource checker:\", e)\n        return False\n"
                }
            ],
            "score": false
        },
        {
            "id": 1,
            "desc": "注意不要在一次thought中连续使用多个functions。",
            "other_info": {
                "function": [
                    "get_sub_company_info",
                    "get_company_info",
                    "get_company_register",
                    "get_sub_company_info_list",
                    "get_company_register_name",
                    "finish"
                ],
                "from": "system_para_3"
            },
            "dimension": "unconditional",
            "type": "resource",
            "is_meta": false,
            "evaluation": [
                {
                    "type": "llm",
                    "required_keys": [
                        "response"
                    ],
                    "exec": "请帮我提取出下面这段话中**当前步**使用的函数名称，不关注未来要做的事。\n请仅回答函数名称，并以list形式直接返回给我，不要回答其他的内容。\n\n{response}\n"
                },
                {
                    "type": "code",
                    "exec": "\ndef check_following(response):\n    import re\n\n    def extract_tool(text):\n        pattern = r'\\[([^]]+)\\]'\n        matches = re.findall(pattern, text)\n        if not matches:\n            return []\n        try:\n            tools = eval(f\"[{matches[-1]}]\")\n            return tools\n        except Exception as e:\n            # print(\"error in extract_tool:\", e)\n            return []\n\n    try:\n        tools = ['get_sub_company_info', 'get_company_info', 'get_company_register', 'get_sub_company_info_list', 'get_company_register_name', 'finish']\n        functions = extract_tool(response)\n        if len(functions) <= 1:\n            return True\n        return False\n    except Exception as e:\n        print(\"error in resource checker count:\", e)\n        return False\n"
                }
            ],
            "score": false
        },
        {
            "id": 3,
            "desc": "</thought> 结束",
            "other_info": {
                "from": "system_para_1"
            },
            "dimension": "unconditional",
            "type": "formatting",
            "is_meta": false,
            "evaluation": [
                {
                    "type": "code",
                    "exec": "\ndef check_following(response):\n    return response.rstrip().endswith(\"</thought>\")\n"
                }
            ],
            "score": false
        },
        {
            "id": 0,
            "desc": "您可以使用一系列functions。",
            "other_info": {
                "function": [
                    "get_sub_company_info_list",
                    "get_sub_company_info",
                    "get_company_info",
                    "get_court_info_list",
                    "get_administrative_case_company_list",
                    "finish"
                ],
                "from": "system_para_1"
            },
            "dimension": "unconditional",
            "type": "resource",
            "is_meta": false,
            "evaluation": [
                {
                    "type": "llm",
                    "required_keys": [
                        "response"
                    ],
                    "exec": "请帮我提取出下面这段话中**当前步**使用的函数名称，不关注未来要做的事。\n请仅回答函数名称，并以list形式直接返回给我，不要回答其他的内容。\n\n{response}\n"
                },
                {
                    "type": "code",
                    "exec": "\ndef check_following(response):\n    import re\n\n    def extract_tool(text):\n        pattern = r'\\[([^]]+)\\]'\n        matches = re.findall(pattern, text)\n        if not matches:\n            return []\n        try:\n            tools = eval(f\"[{matches[-1]}]\")\n            return tools\n        except Exception as e:\n            # print(\"error in extract_tool:\", e)\n            return []\n\n    try:\n        tools = ['get_sub_company_info_list', 'get_sub_company_info', 'get_company_info', 'get_court_info_list', 'get_administrative_case_company_list', 'finish']\n        functions = extract_tool(response)\n        if isinstance(functions, list):\n            for item in functions:\n                if item not in tools:\n                    return False\n        return True\n    except Exception as e:\n        print(\"error in resource checker:\", e)\n        return False\n"
                }
            ],
            "score": false
        },
        {
            "id": 1,
            "desc": "注意不要在一次thought中连续使用多个functions。",
            "other_info": {
                "function": [
                    "get_sub_company_info_list",
                    "get_sub_company_info",
                    "get_company_info",
                    "get_court_info_list",
                    "get_administrative_case_company_list",
                    "finish"
                ],
                "from": "system_para_3"
            },
            "dimension": "unconditional",
            "type": "resource",
            "is_meta": false,
            "evaluation": [
                {
                    "type": "llm",
                    "required_keys": [
                        "response"
                    ],
                    "exec": "请帮我提取出下面这段话中**当前步**使用的函数名称，不关注未来要做的事。\n请仅回答函数名称，并以list形式直接返回给我，不要回答其他的内容。\n\n{response}\n"
                },
                {
                    "type": "code",
                    "exec": "\ndef check_following(response):\n    import re\n\n    def extract_tool(text):\n        pattern = r'\\[([^]]+)\\]'\n        matches = re.findall(pattern, text)\n        if not matches:\n            return []\n        try:\n            tools = eval(f\"[{matches[-1]}]\")\n            return tools\n        except Exception as e:\n            # print(\"error in extract_tool:\", e)\n            return []\n\n    try:\n        tools = ['get_sub_company_info_list', 'get_sub_company_info', 'get_company_info', 'get_court_info_list', 'get_administrative_case_company_list', 'finish']\n        functions = extract_tool(response)\n        if len(functions) <= 1:\n            return True\n        return False\n    except Exception as e:\n        print(\"error in resource checker count:\", e)\n        return False\n"
                }
            ],
            "score": false
        },
        {
            "id": 3,
            "desc": "</thought> 结束",
            "other_info": {
                "from": "system_para_1"
            },
            "dimension": "unconditional",
            "type": "formatting",
            "is_meta": false,
            "evaluation": [
                {
                    "type": "code",
                    "exec": "\ndef check_following(response):\n    return response.rstrip().endswith(\"</thought>\")\n"
                }
            ],
            "score": false
        },
        {
            "id": 5,
            "desc": "您可以在thought中使用python packages,但只能从以下packages列表中选择：",
            "other_info": {
                "available_packages": [
                    "math",
                    "time",
                    "stat",
                    "queue",
                    "itertools",
                    "statistics",
                    "random",
                    "collections",
                    "unicodedata"
                ]
            },
            "dimension": "unconditional",
            "type": "resource",
            "is_meta": false,
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "提取下面文本中的所有的调用package，直接返回list格式结果，不要任何解释、前缀或赋值语句。\n输出格式要求：[\"package1\", \"package2\", ...]:\n\nHere is model response: {response}"
                },
                {
                    "type": "code",
                    "exec": "\nimport ast\n\ndef check_following(response: str) -> bool:\n    available_packages = ['math', 'time', 'stat', 'queue', 'itertools', 'statistics', 'random', 'collections', 'unicodedata']\n    try:\n        items = ast.literal_eval(response)\n        return isinstance(items, list) and all(item in available_packages for item in items)\n    except:\n        return False\n"
                }
            ],
            "score": false
        },
        {
            "id": 4,
            "desc": "除了第一轮外，对上一轮的<observation></observation>进行总结。",
            "other_info": {
                "from": "system_para_2"
            },
            "dimension": "unconditional",
            "type": "semantic",
            "is_meta": false,
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "response是否在一开始就总结了上一轮的<observation></observation>？直接回答 YES 或 NO。\n\nresponse: {response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": 0,
            "desc": "您可以使用一系列functions。",
            "other_info": {
                "function": [
                    "get_sub_company_info",
                    "get_company_register_name",
                    "get_company_info",
                    "get_company_register",
                    "get_sub_company_info_list",
                    "finish"
                ],
                "from": "system_para_1"
            },
            "dimension": "unconditional",
            "type": "resource",
            "is_meta": false,
            "evaluation": [
                {
                    "type": "llm",
                    "required_keys": [
                        "response"
                    ],
                    "exec": "请帮我提取出下面这段话中**当前步**使用的函数名称，不关注未来要做的事。\n请仅回答函数名称，并以list形式直接返回给我，不要回答其他的内容。\n\n{response}\n"
                },
                {
                    "type": "code",
                    "exec": "\ndef check_following(response):\n    import re\n\n    def extract_tool(text):\n        pattern = r'\\[([^]]+)\\]'\n        matches = re.findall(pattern, text)\n        if not matches:\n            return []\n        try:\n            tools = eval(f\"[{matches[-1]}]\")\n            return tools\n        except Exception as e:\n            # print(\"error in extract_tool:\", e)\n            return []\n\n    try:\n        tools = ['get_sub_company_info', 'get_company_register_name', 'get_company_info', 'get_company_register', 'get_sub_company_info_list', 'finish']\n        functions = extract_tool(response)\n        if isinstance(functions, list):\n            for item in functions:\n                if item not in tools:\n                    return False\n        return True\n    except Exception as e:\n        print(\"error in resource checker:\", e)\n        return False\n"
                }
            ],
            "score": false
        },
        {
            "id": 1,
            "desc": "注意不要在一次thought中连续使用多个functions。",
            "other_info": {
                "function": [
                    "get_sub_company_info",
                    "get_company_register_name",
                    "get_company_info",
                    "get_company_register",
                    "get_sub_company_info_list",
                    "finish"
                ],
                "from": "system_para_3"
            },
            "dimension": "unconditional",
            "type": "resource",
            "is_meta": false,
            "evaluation": [
                {
                    "type": "llm",
                    "required_keys": [
                        "response"
                    ],
                    "exec": "请帮我提取出下面这段话中**当前步**使用的函数名称，不关注未来要做的事。\n请仅回答函数名称，并以list形式直接返回给我，不要回答其他的内容。\n\n{response}\n"
                },
                {
                    "type": "code",
                    "exec": "\ndef check_following(response):\n    import re\n\n    def extract_tool(text):\n        pattern = r'\\[([^]]+)\\]'\n        matches = re.findall(pattern, text)\n        if not matches:\n            return []\n        try:\n            tools = eval(f\"[{matches[-1]}]\")\n            return tools\n        except Exception as e:\n            # print(\"error in extract_tool:\", e)\n            return []\n\n    try:\n        tools = ['get_sub_company_info', 'get_company_register_name', 'get_company_info', 'get_company_register', 'get_sub_company_info_list', 'finish']\n        functions = extract_tool(response)\n        if len(functions) <= 1:\n            return True\n        return False\n    except Exception as e:\n        print(\"error in resource checker count:\", e)\n        return False\n"
                }
            ],
            "score": false
        },
        {
            "id": 3,
            "desc": "</thought> 结束",
            "other_info": {
                "from": "system_para_1"
            },
            "dimension": "unconditional",
            "type": "formatting",
            "is_meta": false,
            "evaluation": [
                {
                    "type": "code",
                    "exec": "\ndef check_following(response):\n    return response.rstrip().endswith(\"</thought>\")\n"
                }
            ],
            "score": false
        },
        {
            "id": 5,
            "desc": "您可以在thought中使用python packages,但只能从以下packages列表中选择：",
            "other_info": {
                "available_packages": [
                    "math",
                    "time",
                    "stat",
                    "queue",
                    "itertools",
                    "statistics",
                    "random",
                    "collections",
                    "unicodedata"
                ]
            },
            "dimension": "unconditional",
            "type": "resource",
            "is_meta": false,
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "提取下面文本中的所有的调用package，直接返回list格式结果，不要任何解释、前缀或赋值语句。\n输出格式要求：[\"package1\", \"package2\", ...]:\n\nHere is model response: {response}"
                },
                {
                    "type": "code",
                    "exec": "\nimport ast\n\ndef check_following(response: str) -> bool:\n    available_packages = ['math', 'time', 'stat', 'queue', 'itertools', 'statistics', 'random', 'collections', 'unicodedata']\n    try:\n        items = ast.literal_eval(response)\n        return isinstance(items, list) and all(item in available_packages for item in items)\n    except:\n        return False\n"
                }
            ],
            "score": false
        },
        {
            "id": 4,
            "desc": "除了第一轮外，对上一轮的<observation></observation>进行总结。",
            "other_info": {
                "from": "system_para_2"
            },
            "dimension": "unconditional",
            "type": "semantic",
            "is_meta": false,
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "response是否在一开始就总结了上一轮的<observation></observation>？直接回答 YES 或 NO。\n\nresponse: {response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": 0,
            "desc": "您可以使用一系列functions。",
            "other_info": {
                "function": [
                    "get_legal_document",
                    "get_legal_document_company_list",
                    "get_finalized_case",
                    "extract_case_number",
                    "get_legal_document_law_list",
                    "finish"
                ],
                "from": "system_para_1"
            },
            "dimension": "unconditional",
            "type": "resource",
            "is_meta": false,
            "evaluation": [
                {
                    "type": "llm",
                    "required_keys": [
                        "response"
                    ],
                    "exec": "请帮我提取出下面这段话中**当前步**使用的函数名称，不关注未来要做的事。\n请仅回答函数名称，并以list形式直接返回给我，不要回答其他的内容。\n\n{response}\n"
                },
                {
                    "type": "code",
                    "exec": "\ndef check_following(response):\n    import re\n\n    def extract_tool(text):\n        pattern = r'\\[([^]]+)\\]'\n        matches = re.findall(pattern, text)\n        if not matches:\n            return []\n        try:\n            tools = eval(f\"[{matches[-1]}]\")\n            return tools\n        except Exception as e:\n            # print(\"error in extract_tool:\", e)\n            return []\n\n    try:\n        tools = ['get_legal_document', 'get_legal_document_company_list', 'get_finalized_case', 'extract_case_number', 'get_legal_document_law_list', 'finish']\n        functions = extract_tool(response)\n        if isinstance(functions, list):\n            for item in functions:\n                if item not in tools:\n                    return False\n        return True\n    except Exception as e:\n        print(\"error in resource checker:\", e)\n        return False\n"
                }
            ],
            "score": false
        },
        {
            "id": 1,
            "desc": "注意不要在一次thought中连续使用多个functions。",
            "other_info": {
                "function": [
                    "get_legal_document",
                    "get_legal_document_company_list",
                    "get_finalized_case",
                    "extract_case_number",
                    "get_legal_document_law_list",
                    "finish"
                ],
                "from": "system_para_3"
            },
            "dimension": "unconditional",
            "type": "resource",
            "is_meta": false,
            "evaluation": [
                {
                    "type": "llm",
                    "required_keys": [
                        "response"
                    ],
                    "exec": "请帮我提取出下面这段话中**当前步**使用的函数名称，不关注未来要做的事。\n请仅回答函数名称，并以list形式直接返回给我，不要回答其他的内容。\n\n{response}\n"
                },
                {
                    "type": "code",
                    "exec": "\ndef check_following(response):\n    import re\n\n    def extract_tool(text):\n        pattern = r'\\[([^]]+)\\]'\n        matches = re.findall(pattern, text)\n        if not matches:\n            return []\n        try:\n            tools = eval(f\"[{matches[-1]}]\")\n            return tools\n        except Exception as e:\n            # print(\"error in extract_tool:\", e)\n            return []\n\n    try:\n        tools = ['get_legal_document', 'get_legal_document_company_list', 'get_finalized_case', 'extract_case_number', 'get_legal_document_law_list', 'finish']\n        functions = extract_tool(response)\n        if len(functions) <= 1:\n            return True\n        return False\n    except Exception as e:\n        print(\"error in resource checker count:\", e)\n        return False\n"
                }
            ],
            "score": false
        },
        {
            "id": 3,
            "desc": "</thought> 结束",
            "other_info": {
                "from": "system_para_1"
            },
            "dimension": "unconditional",
            "type": "formatting",
            "is_meta": false,
            "evaluation": [
                {
                    "type": "code",
                    "exec": "\ndef check_following(response):\n    return response.rstrip().endswith(\"</thought>\")\n"
                }
            ],
            "score": false
        },
        {
            "id": 5,
            "desc": "您可以在thought中使用python packages,但只能从以下packages列表中选择：",
            "other_info": {
                "available_packages": [
                    "math",
                    "time",
                    "stat",
                    "queue",
                    "itertools",
                    "statistics",
                    "random",
                    "collections",
                    "unicodedata"
                ]
            },
            "dimension": "unconditional",
            "type": "resource",
            "is_meta": false,
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "提取下面文本中的所有的调用package，直接返回list格式结果，不要任何解释、前缀或赋值语句。\n输出格式要求：[\"package1\", \"package2\", ...]:\n\nHere is model response: {response}"
                },
                {
                    "type": "code",
                    "exec": "\nimport ast\n\ndef check_following(response: str) -> bool:\n    available_packages = ['math', 'time', 'stat', 'queue', 'itertools', 'statistics', 'random', 'collections', 'unicodedata']\n    try:\n        items = ast.literal_eval(response)\n        return isinstance(items, list) and all(item in available_packages for item in items)\n    except:\n        return False\n"
                }
            ],
            "score": false
        },
        {
            "id": 4,
            "desc": "除了第一轮外，对上一轮的<observation></observation>进行总结。",
            "other_info": {
                "from": "system_para_2"
            },
            "dimension": "unconditional",
            "type": "semantic",
            "is_meta": false,
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "response是否在一开始就总结了上一轮的<observation></observation>？直接回答 YES 或 NO。\n\nresponse: {response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": 0,
            "desc": "您可以使用一系列functions。",
            "other_info": {
                "function": [
                    "get_legal_document",
                    "get_lawfirm_info",
                    "get_legal_document_company_list",
                    "get_court_info",
                    "get_legal_document_law_list",
                    "finish"
                ],
                "from": "system_para_1"
            },
            "dimension": "unconditional",
            "type": "resource",
            "is_meta": false,
            "evaluation": [
                {
                    "type": "llm",
                    "required_keys": [
                        "response"
                    ],
                    "exec": "请帮我提取出下面这段话中**当前步**使用的函数名称，不关注未来要做的事。\n请仅回答函数名称，并以list形式直接返回给我，不要回答其他的内容。\n\n{response}\n"
                },
                {
                    "type": "code",
                    "exec": "\ndef check_following(response):\n    import re\n\n    def extract_tool(text):\n        pattern = r'\\[([^]]+)\\]'\n        matches = re.findall(pattern, text)\n        if not matches:\n            return []\n        try:\n            tools = eval(f\"[{matches[-1]}]\")\n            return tools\n        except Exception as e:\n            # print(\"error in extract_tool:\", e)\n            return []\n\n    try:\n        tools = ['get_legal_document', 'get_lawfirm_info', 'get_legal_document_company_list', 'get_court_info', 'get_legal_document_law_list', 'finish']\n        functions = extract_tool(response)\n        if isinstance(functions, list):\n            for item in functions:\n                if item not in tools:\n                    return False\n        return True\n    except Exception as e:\n        print(\"error in resource checker:\", e)\n        return False\n"
                }
            ],
            "score": false
        },
        {
            "id": 1,
            "desc": "注意不要在一次thought中连续使用多个functions。",
            "other_info": {
                "function": [
                    "get_legal_document",
                    "get_lawfirm_info",
                    "get_legal_document_company_list",
                    "get_court_info",
                    "get_legal_document_law_list",
                    "finish"
                ],
                "from": "system_para_3"
            },
            "dimension": "unconditional",
            "type": "resource",
            "is_meta": false,
            "evaluation": [
                {
                    "type": "llm",
                    "required_keys": [
                        "response"
                    ],
                    "exec": "请帮我提取出下面这段话中**当前步**使用的函数名称，不关注未来要做的事。\n请仅回答函数名称，并以list形式直接返回给我，不要回答其他的内容。\n\n{response}\n"
                },
                {
                    "type": "code",
                    "exec": "\ndef check_following(response):\n    import re\n\n    def extract_tool(text):\n        pattern = r'\\[([^]]+)\\]'\n        matches = re.findall(pattern, text)\n        if not matches:\n            return []\n        try:\n            tools = eval(f\"[{matches[-1]}]\")\n            return tools\n        except Exception as e:\n            # print(\"error in extract_tool:\", e)\n            return []\n\n    try:\n        tools = ['get_legal_document', 'get_lawfirm_info', 'get_legal_document_company_list', 'get_court_info', 'get_legal_document_law_list', 'finish']\n        functions = extract_tool(response)\n        if len(functions) <= 1:\n            return True\n        return False\n    except Exception as e:\n        print(\"error in resource checker count:\", e)\n        return False\n"
                }
            ],
            "score": false
        },
        {
            "id": 3,
            "desc": "</thought> 结束",
            "other_info": {
                "from": "system_para_1"
            },
            "dimension": "unconditional",
            "type": "formatting",
            "is_meta": false,
            "evaluation": [
                {
                    "type": "code",
                    "exec": "\ndef check_following(response):\n    return response.rstrip().endswith(\"</thought>\")\n"
                }
            ],
            "score": false
        },
        {
            "id": 5,
            "desc": "您可以在thought中使用python packages,但只能从以下packages列表中选择：",
            "other_info": {
                "available_packages": [
                    "math",
                    "time",
                    "stat",
                    "queue",
                    "itertools",
                    "statistics",
                    "random",
                    "collections",
                    "unicodedata"
                ]
            },
            "dimension": "unconditional",
            "type": "resource",
            "is_meta": false,
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "提取下面文本中的所有的调用package，直接返回list格式结果，不要任何解释、前缀或赋值语句。\n输出格式要求：[\"package1\", \"package2\", ...]:\n\nHere is model response: {response}"
                },
                {
                    "type": "code",
                    "exec": "\nimport ast\n\ndef check_following(response: str) -> bool:\n    available_packages = ['math', 'time', 'stat', 'queue', 'itertools', 'statistics', 'random', 'collections', 'unicodedata']\n    try:\n        items = ast.literal_eval(response)\n        return isinstance(items, list) and all(item in available_packages for item in items)\n    except:\n        return False\n"
                }
            ],
            "score": false
        },
        {
            "id": 4,
            "desc": "除了第一轮外，对上一轮的<observation></observation>进行总结。",
            "other_info": {
                "from": "system_para_2"
            },
            "dimension": "unconditional",
            "type": "semantic",
            "is_meta": false,
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "response是否在一开始就总结了上一轮的<observation></observation>？直接回答 YES 或 NO。\n\nresponse: {response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": 0,
            "desc": "您可以使用一系列functions。",
            "other_info": {
                "function": [
                    "get_legal_document_company_list",
                    "get_legal_document",
                    "get_administrative_case_company_list",
                    "get_legal_document_law_list",
                    "get_finalized_case_company_list",
                    "finish"
                ],
                "from": "system_para_1"
            },
            "dimension": "unconditional",
            "type": "resource",
            "is_meta": false,
            "evaluation": [
                {
                    "type": "llm",
                    "required_keys": [
                        "response"
                    ],
                    "exec": "请帮我提取出下面这段话中**当前步**使用的函数名称，不关注未来要做的事。\n请仅回答函数名称，并以list形式直接返回给我，不要回答其他的内容。\n\n{response}\n"
                },
                {
                    "type": "code",
                    "exec": "\ndef check_following(response):\n    import re\n\n    def extract_tool(text):\n        pattern = r'\\[([^]]+)\\]'\n        matches = re.findall(pattern, text)\n        if not matches:\n            return []\n        try:\n            tools = eval(f\"[{matches[-1]}]\")\n            return tools\n        except Exception as e:\n            # print(\"error in extract_tool:\", e)\n            return []\n\n    try:\n        tools = ['get_legal_document_company_list', 'get_legal_document', 'get_administrative_case_company_list', 'get_legal_document_law_list', 'get_finalized_case_company_list', 'finish']\n        functions = extract_tool(response)\n        if isinstance(functions, list):\n            for item in functions:\n                if item not in tools:\n                    return False\n        return True\n    except Exception as e:\n        print(\"error in resource checker:\", e)\n        return False\n"
                }
            ],
            "score": false
        },
        {
            "id": 1,
            "desc": "注意不要在一次thought中连续使用多个functions。",
            "other_info": {
                "function": [
                    "get_legal_document_company_list",
                    "get_legal_document",
                    "get_administrative_case_company_list",
                    "get_legal_document_law_list",
                    "get_finalized_case_company_list",
                    "finish"
                ],
                "from": "system_para_3"
            },
            "dimension": "unconditional",
            "type": "resource",
            "is_meta": false,
            "evaluation": [
                {
                    "type": "llm",
                    "required_keys": [
                        "response"
                    ],
                    "exec": "请帮我提取出下面这段话中**当前步**使用的函数名称，不关注未来要做的事。\n请仅回答函数名称，并以list形式直接返回给我，不要回答其他的内容。\n\n{response}\n"
                },
                {
                    "type": "code",
                    "exec": "\ndef check_following(response):\n    import re\n\n    def extract_tool(text):\n        pattern = r'\\[([^]]+)\\]'\n        matches = re.findall(pattern, text)\n        if not matches:\n            return []\n        try:\n            tools = eval(f\"[{matches[-1]}]\")\n            return tools\n        except Exception as e:\n            # print(\"error in extract_tool:\", e)\n            return []\n\n    try:\n        tools = ['get_legal_document_company_list', 'get_legal_document', 'get_administrative_case_company_list', 'get_legal_document_law_list', 'get_finalized_case_company_list', 'finish']\n        functions = extract_tool(response)\n        if len(functions) <= 1:\n            return True\n        return False\n    except Exception as e:\n        print(\"error in resource checker count:\", e)\n        return False\n"
                }
            ],
            "score": false
        },
        {
            "id": 3,
            "desc": "</thought> 结束",
            "other_info": {
                "from": "system_para_1"
            },
            "dimension": "unconditional",
            "type": "formatting",
            "is_meta": false,
            "evaluation": [
                {
                    "type": "code",
                    "exec": "\ndef check_following(response):\n    return response.rstrip().endswith(\"</thought>\")\n"
                }
            ],
            "score": false
        },
        {
            "id": 5,
            "desc": "您可以在thought中使用python packages,但只能从以下packages列表中选择：",
            "other_info": {
                "available_packages": [
                    "math",
                    "time",
                    "stat",
                    "queue",
                    "itertools",
                    "statistics",
                    "random",
                    "collections",
                    "unicodedata"
                ]
            },
            "dimension": "unconditional",
            "type": "resource",
            "is_meta": false,
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "提取下面文本中的所有的调用package，直接返回list格式结果，不要任何解释、前缀或赋值语句。\n输出格式要求：[\"package1\", \"package2\", ...]:\n\nHere is model response: {response}"
                },
                {
                    "type": "code",
                    "exec": "\nimport ast\n\ndef check_following(response: str) -> bool:\n    available_packages = ['math', 'time', 'stat', 'queue', 'itertools', 'statistics', 'random', 'collections', 'unicodedata']\n    try:\n        items = ast.literal_eval(response)\n        return isinstance(items, list) and all(item in available_packages for item in items)\n    except:\n        return False\n"
                }
            ],
            "score": false
        },
        {
            "id": 4,
            "desc": "除了第一轮外，对上一轮的<observation></observation>进行总结。",
            "other_info": {
                "from": "system_para_2"
            },
            "dimension": "unconditional",
            "type": "semantic",
            "is_meta": false,
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "response是否在一开始就总结了上一轮的<observation></observation>？直接回答 YES 或 NO。\n\nresponse: {response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": 0,
            "desc": "您可以使用一系列functions。",
            "other_info": {
                "function": [
                    "get_sub_company_info_list",
                    "get_sub_company_info",
                    "get_company_info",
                    "get_court_info_list",
                    "get_company_register",
                    "finish"
                ],
                "from": "system_para_1"
            },
            "dimension": "unconditional",
            "type": "resource",
            "is_meta": false,
            "evaluation": [
                {
                    "type": "llm",
                    "required_keys": [
                        "response"
                    ],
                    "exec": "请帮我提取出下面这段话中**当前步**使用的函数名称，不关注未来要做的事。\n请仅回答函数名称，并以list形式直接返回给我，不要回答其他的内容。\n\n{response}\n"
                },
                {
                    "type": "code",
                    "exec": "\ndef check_following(response):\n    import re\n\n    def extract_tool(text):\n        pattern = r'\\[([^]]+)\\]'\n        matches = re.findall(pattern, text)\n        if not matches:\n            return []\n        try:\n            tools = eval(f\"[{matches[-1]}]\")\n            return tools\n        except Exception as e:\n            # print(\"error in extract_tool:\", e)\n            return []\n\n    try:\n        tools = ['get_sub_company_info_list', 'get_sub_company_info', 'get_company_info', 'get_court_info_list', 'get_company_register', 'finish']\n        functions = extract_tool(response)\n        if isinstance(functions, list):\n            for item in functions:\n                if item not in tools:\n                    return False\n        return True\n    except Exception as e:\n        print(\"error in resource checker:\", e)\n        return False\n"
                }
            ],
            "score": false
        },
        {
            "id": 1,
            "desc": "注意不要在一次thought中连续使用多个functions。",
            "other_info": {
                "function": [
                    "get_sub_company_info_list",
                    "get_sub_company_info",
                    "get_company_info",
                    "get_court_info_list",
                    "get_company_register",
                    "finish"
                ],
                "from": "system_para_3"
            },
            "dimension": "unconditional",
            "type": "resource",
            "is_meta": false,
            "evaluation": [
                {
                    "type": "llm",
                    "required_keys": [
                        "response"
                    ],
                    "exec": "请帮我提取出下面这段话中**当前步**使用的函数名称，不关注未来要做的事。\n请仅回答函数名称，并以list形式直接返回给我，不要回答其他的内容。\n\n{response}\n"
                },
                {
                    "type": "code",
                    "exec": "\ndef check_following(response):\n    import re\n\n    def extract_tool(text):\n        pattern = r'\\[([^]]+)\\]'\n        matches = re.findall(pattern, text)\n        if not matches:\n            return []\n        try:\n            tools = eval(f\"[{matches[-1]}]\")\n            return tools\n        except Exception as e:\n            # print(\"error in extract_tool:\", e)\n            return []\n\n    try:\n        tools = ['get_sub_company_info_list', 'get_sub_company_info', 'get_company_info', 'get_court_info_list', 'get_company_register', 'finish']\n        functions = extract_tool(response)\n        if len(functions) <= 1:\n            return True\n        return False\n    except Exception as e:\n        print(\"error in resource checker count:\", e)\n        return False\n"
                }
            ],
            "score": false
        },
        {
            "id": 3,
            "desc": "</thought> 结束",
            "other_info": {
                "from": "system_para_1"
            },
            "dimension": "unconditional",
            "type": "formatting",
            "is_meta": false,
            "evaluation": [
                {
                    "type": "code",
                    "exec": "\ndef check_following(response):\n    return response.rstrip().endswith(\"</thought>\")\n"
                }
            ],
            "score": false
        },
        {
            "id": 5,
            "desc": "您可以在thought中使用python packages,但只能从以下packages列表中选择：",
            "other_info": {
                "available_packages": [
                    "math",
                    "time",
                    "stat",
                    "queue",
                    "itertools",
                    "statistics",
                    "random",
                    "collections",
                    "unicodedata"
                ]
            },
            "dimension": "unconditional",
            "type": "resource",
            "is_meta": false,
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "提取下面文本中的所有的调用package，直接返回list格式结果，不要任何解释、前缀或赋值语句。\n输出格式要求：[\"package1\", \"package2\", ...]:\n\nHere is model response: {response}"
                },
                {
                    "type": "code",
                    "exec": "\nimport ast\n\ndef check_following(response: str) -> bool:\n    available_packages = ['math', 'time', 'stat', 'queue', 'itertools', 'statistics', 'random', 'collections', 'unicodedata']\n    try:\n        items = ast.literal_eval(response)\n        return isinstance(items, list) and all(item in available_packages for item in items)\n    except:\n        return False\n"
                }
            ],
            "score": false
        },
        {
            "id": 4,
            "desc": "除了第一轮外，对上一轮的<observation></observation>进行总结。",
            "other_info": {
                "from": "system_para_2"
            },
            "dimension": "unconditional",
            "type": "semantic",
            "is_meta": false,
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "response是否在一开始就总结了上一轮的<observation></observation>？直接回答 YES 或 NO。\n\nresponse: {response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": 0,
            "desc": "Format messages clearly with headers.",
            "other_info": {
                "from": "system_para_0",
                "type_explanation": "The constraint specifies how messages should be presented, focusing on their structure and clarity by requiring the use of headers. This directly relates to the 'formatting' category, which governs the structure and presentation format of the output.",
                "meta_expalnation": "The input constraint directly specifies how messages should be formatted by stating 'Format messages clearly with headers.' This is an operational constraint that constrains the model's output directly in terms of its content or format and does not deal with managing, selecting, prioritizing, or composing multiple constraints. Therefore, it is not a Meta Constraint.",
                "evaluation_type": {
                    "constraint_type": "code",
                    "explanation": "The requirement to format messages clearly with headers can be directly validated through code by checking for the presence and structure of headers in the text."
                },
                "evaluation_generation_success": true
            },
            "dimension": "unconditional",
            "is_meta": false,
            "type": [
                "formatting"
            ],
            "evaluation": [
                {
                    "type": "code",
                    "exec": "import re\n\ndef check_following(response: str) -> bool:\n    return bool(re.match(r'^#[A-Za-z0-9_ -]+$', response.split('\\n')[0]))\n"
                }
            ],
            "state": "success",
            "score": false
        },
        {
            "id": 0,
            "desc": "Format messages clearly with headers.",
            "other_info": {
                "from": "system_para_0",
                "type_explanation": "The constraint specifies how messages should be presented, focusing on their structure and clarity by requiring the use of headers. This directly relates to the 'formatting' category, which governs the structure and presentation format of the output.",
                "meta_expalnation": "The input constraint directly specifies how messages should be formatted by stating 'Format messages clearly with headers.' This is an operational constraint that constrains the model's output directly in terms of its content or format and does not deal with managing, selecting, prioritizing, or composing multiple constraints. Therefore, it is not a Meta Constraint.",
                "evaluation_type": {
                    "constraint_type": "code",
                    "explanation": "The requirement to format messages clearly with headers can be directly validated through code by checking for the presence and structure of headers in the text."
                },
                "evaluation_generation_success": true
            },
            "dimension": "unconditional",
            "is_meta": false,
            "type": [
                "formatting"
            ],
            "evaluation": [
                {
                    "type": "code",
                    "exec": "import re\n\ndef check_following(response: str) -> bool:\n    return bool(re.match(r'^#[A-Za-z0-9_ -]+$', response.split('\\n')[0]))\n"
                }
            ],
            "state": "success",
            "score": false
        },
        {
            "id": 0,
            "desc": "Format messages clearly with headers.",
            "other_info": {
                "from": "system_para_0",
                "type_explanation": "The constraint specifies how messages should be presented, focusing on their structure and clarity by requiring the use of headers. This directly relates to the 'formatting' category, which governs the structure and presentation format of the output.",
                "meta_expalnation": "The input constraint directly specifies how messages should be formatted by stating 'Format messages clearly with headers.' This is an operational constraint that constrains the model's output directly in terms of its content or format and does not deal with managing, selecting, prioritizing, or composing multiple constraints. Therefore, it is not a Meta Constraint.",
                "evaluation_type": {
                    "constraint_type": "code",
                    "explanation": "The requirement to format messages clearly with headers can be directly validated through code by checking for the presence and structure of headers in the text."
                },
                "evaluation_generation_success": true
            },
            "dimension": "unconditional",
            "is_meta": false,
            "type": [
                "formatting"
            ],
            "evaluation": [
                {
                    "type": "code",
                    "exec": "import re\n\ndef check_following(response: str) -> bool:\n    return bool(re.match(r'^#[A-Za-z0-9_ -]+$', response.split('\\n')[0]))\n"
                }
            ],
            "state": "success",
            "score": false
        },
        {
            "id": 0,
            "desc": "Format messages clearly with headers.",
            "other_info": {
                "from": "system_para_0",
                "type_explanation": "The constraint specifies how messages should be presented, focusing on their structure and clarity by requiring the use of headers. This directly relates to the 'formatting' category, which governs the structure and presentation format of the output.",
                "meta_expalnation": "The input constraint directly specifies how messages should be formatted by stating 'Format messages clearly with headers.' This is an operational constraint that constrains the model's output directly in terms of its content or format and does not deal with managing, selecting, prioritizing, or composing multiple constraints. Therefore, it is not a Meta Constraint.",
                "evaluation_type": {
                    "constraint_type": "code",
                    "explanation": "The requirement to format messages clearly with headers can be directly validated through code by checking for the presence and structure of headers in the text."
                },
                "evaluation_generation_success": true
            },
            "dimension": "unconditional",
            "is_meta": false,
            "type": [
                "formatting"
            ],
            "evaluation": [
                {
                    "type": "code",
                    "exec": "import re\n\ndef check_following(response: str) -> bool:\n    return bool(re.match(r'^#[A-Za-z0-9_ -]+$', response.split('\\n')[0]))\n"
                }
            ],
            "state": "success",
            "score": false
        },
        {
            "id": 0,
            "desc": "Format messages clearly with headers.",
            "other_info": {
                "from": "system_para_0",
                "type_explanation": "The constraint specifies how messages should be presented, focusing on their structure and clarity by requiring the use of headers. This directly relates to the 'formatting' category, which governs the structure and presentation format of the output.",
                "meta_expalnation": "The input constraint directly specifies how messages should be formatted by stating 'Format messages clearly with headers.' This is an operational constraint that constrains the model's output directly in terms of its content or format and does not deal with managing, selecting, prioritizing, or composing multiple constraints. Therefore, it is not a Meta Constraint.",
                "evaluation_type": {
                    "constraint_type": "code",
                    "explanation": "The requirement to format messages clearly with headers can be directly validated through code by checking for the presence and structure of headers in the text."
                },
                "evaluation_generation_success": true
            },
            "dimension": "unconditional",
            "is_meta": false,
            "type": [
                "formatting"
            ],
            "evaluation": [
                {
                    "type": "code",
                    "exec": "import re\n\ndef check_following(response: str) -> bool:\n    return bool(re.match(r'^#[A-Za-z0-9_ -]+$', response.split('\\n')[0]))\n"
                }
            ],
            "state": "success",
            "score": false
        },
        {
            "id": 0,
            "desc": "Format messages clearly with headers.",
            "other_info": {
                "from": "system_para_0",
                "type_explanation": "The constraint specifies how messages should be presented, focusing on their structure and clarity by requiring the use of headers. This directly relates to the 'formatting' category, which governs the structure and presentation format of the output.",
                "meta_expalnation": "The input constraint directly specifies how messages should be formatted by stating 'Format messages clearly with headers.' This is an operational constraint that constrains the model's output directly in terms of its content or format and does not deal with managing, selecting, prioritizing, or composing multiple constraints. Therefore, it is not a Meta Constraint.",
                "evaluation_type": {
                    "constraint_type": "code",
                    "explanation": "The requirement to format messages clearly with headers can be directly validated through code by checking for the presence and structure of headers in the text."
                },
                "evaluation_generation_success": true
            },
            "dimension": "unconditional",
            "is_meta": false,
            "type": [
                "formatting"
            ],
            "evaluation": [
                {
                    "type": "code",
                    "exec": "import re\n\ndef check_following(response: str) -> bool:\n    return bool(re.match(r'^#[A-Za-z0-9_ -]+$', response.split('\\n')[0]))\n"
                }
            ],
            "state": "success",
            "score": false
        },
        {
            "id": 0,
            "desc": "Create a safe space by acknowledging their courage in seeking support.",
            "other_info": {
                "from": "system_para_0",
                "type_explanation": "The constraint focuses on the tone and content of the response, ensuring it is empathetic, supportive, and acknowledges the courage of the individual seeking support. This aligns with the semantic category as it pertains to style, tone, and meaningful content.",
                "meta_expalnation": "The given constraint directly deals with how the output should be structured by asking the model to acknowledge courage and create a safe space, which is a directive related to content and tone. It does not manage or govern other constraints (e.g., selecting, prioritizing, or merging them), so it is not a Meta Constraint.",
                "evaluation_type": {
                    "constraint_type": "llm",
                    "explanation": "Creating a safe space and acknowledging courage involves subjective interpretation, empathetic communication, and a nuanced understanding of tone and context, which require semantic assessment by an LLM."
                },
                "evaluation_generation_success": true
            },
            "dimension": "unconditional",
            "is_meta": false,
            "type": [
                "semantic"
            ],
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "Does the model response explicitly acknowledge the user's courage in seeking support? Please answer YES/NO directly and do not enter anything else.\n\nHere is the model response: {response}"
                }
            ],
            "state": "success",
            "llm_output": "NO",
            "score": false
        },
        {
            "id": 4,
            "desc": "Assess risk levels with validated screening approaches.",
            "other_info": {
                "from": "system_para_0",
                "type_explanation": "The constraint focuses on ensuring meaningful and accurate content by requiring the use of 'validated screening approaches' for risk assessment. This directly pertains to content accuracy and adherence to established methods, which aligns with the 'semantic' category.",
                "meta_expalnation": "The given constraint directly instructs the model to assess risk levels using validated screening approaches, which is an operational rule affecting the content or execution of the task rather than managing multiple constraints. It does not involve selection, prioritization, disabling, deduplication, or composition of other constraints, so it is not a Meta Constraint.",
                "evaluation_type": {
                    "constraint_type": "llm",
                    "explanation": "This constraint involves assessing risk levels, which requires semantic understanding of clinical context, interpretation of nuanced language, and the application of validated screening approaches—a highly subjective and open-ended process. It cannot be directly encoded into logic or extracted in a structured way for validation."
                },
                "evaluation_generation_success": true
            },
            "dimension": "unconditional",
            "is_meta": false,
            "type": [
                "semantic"
            ],
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "Does the response include an assessment of risk levels using validated screening approaches? Please answer YES/NO directly and do not enter anything else.\n\nHere is the model response: {response}"
                }
            ],
            "state": "success",
            "llm_output": "NO",
            "score": false
        },
        {
            "id": 5,
            "desc": "Help them understand their current mental health in accessible language.",
            "other_info": {
                "from": "system_para_0",
                "type_explanation": "This constraint focuses on the style of communication ('accessible language') and the meaningfulness of the content ('help them understand their current mental health'). It emphasizes the tone and clarity, which falls under ensuring the output is appropriate, accurate, and understandable for the intended audience, aligning with the semantic category.",
                "meta_expalnation": "The given constraint directly governs the model's output by specifying content and language requirements, i.e., providing mental health explanations in accessible language. It does not define strategies for managing multiple constraints, and therefore does not qualify as a meta constraint.",
                "evaluation_type": {
                    "constraint_type": "llm",
                    "explanation": "This requires semantic understanding and the ability to explain mental health concepts in an accessible and empathetic manner, which involves open-ended, subjective assessment that only an LLM can accomplish."
                },
                "evaluation_generation_success": true
            },
            "dimension": "unconditional",
            "is_meta": false,
            "type": [
                "semantic"
            ],
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "Does the response explain the user's current mental health in accessible language that is easy to understand without using overly technical or complex terms? Please answer YES/NO directly and do not enter anything else.\n\nHere is the model response: {response}"
                }
            ],
            "state": "success",
            "llm_output": "NO",
            "score": false
        },
        {
            "id": 6,
            "desc": "Validate their experiences without minimizing or catastrophizing.",
            "other_info": {
                "from": "system_para_0",
                "type_explanation": "The constraint ensures that the output does not diminish or exaggerate the described experiences, requiring logical consistency, neutrality of position, and tone management. These are semantic requirements aimed at meaningful and appropriate communication.",
                "meta_expalnation": "The provided constraint directly affects the output by specifying how experiences should be validated (without minimizing or catastrophizing). It does not involve managing or interacting with other constraints, such as selecting, prioritizing, disabling, deduplicating, or combining them, which are the defining characteristics of a meta constraint.",
                "evaluation_type": {
                    "constraint_type": "llm",
                    "explanation": "Validating experiences without minimizing or catastrophizing requires subjective and semantic understanding of tone, intent, and nuance. This cannot be directly coded or reliably extracted for rule-based validation."
                },
                "evaluation_generation_success": true
            },
            "dimension": "unconditional",
            "is_meta": false,
            "type": [
                "semantic"
            ],
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "Does the response validate the user's experiences? Please answer YES/NO directly and do not enter anything else.\n\nHere is the model response: {response}"
                }
            ],
            "state": "modified",
            "llm_output": "NO",
            "score": false
        },
        {
            "id": 7,
            "desc": "Always use \"you\" and \"your\" when addressing the user.",
            "other_info": {
                "from": "system_para_1",
                "type_explanation": "The constraint focuses on the style or tone of addressing the user, requiring consistent usage of 'you' and 'your.' Style and tone fall under semantic requirements as they ensure the content adheres to specific communication norms and expectations.",
                "meta_expalnation": "The given constraint directly governs the output format by specifying how the user should be addressed ('use \"you\" and \"your\"'), rather than managing or prioritizing other constraints. Therefore, it is not a Meta Constraint.",
                "evaluation_type": {
                    "constraint_type": "llm",
                    "explanation": "This constraint requires semantic understanding of the response to determine whether 'you' and 'your' are used consistently and appropriately when addressing the user. This cannot be validated with simple logic or extraction but instead requires subjective language assessment."
                },
                "evaluation_generation_success": true
            },
            "dimension": "unconditional",
            "is_meta": false,
            "type": [
                "semantic"
            ],
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "Does the response consistently use 'you' and 'your' when addressing the user? Please answer YES/NO directly and do not enter anything else.\n\nHere is the model response: {response}"
                }
            ],
            "state": "success",
            "llm_output": "NO",
            "score": false
        },
        {
            "id": 8,
            "desc": "Please **first** provide  a 2-3 sentence **summary** of your ideas on the assessment based on the context provided.",
            "other_info": {
                "from": "system_para_2",
                "type_explanation": "The constraint specifies the structure and presentation of the output by requiring it to be a '2-3 sentence summary,' which governs the format and length of the response rather than its content or resource limitations.",
                "meta_expalnation": "The given constraint directly governs the model's output by specifying content requirements (a 2-3 sentence summary of assessment ideas). It does not include any rules about managing or prioritizing multiple constraints, nor does it concern high-level strategies for selecting, ignoring, or combining constraints. Therefore, it is not a Meta Constraint.",
                "evaluation_type": {
                    "constraint_type": "llm",
                    "explanation": "The constraint is subjective and requires semantic interpretation of whether the assessment summary correctly addresses the context provided. This involves open-ended understanding and cannot be validated directly or through extracting structured elements via code."
                },
                "evaluation_generation_success": false
            },
            "dimension": "unconditional",
            "is_meta": false,
            "type": [
                "formatting"
            ],
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "Does the model response provide a summary firstly? Please answer YES/NO directly and do not enter anything else.\n\nHere is the model response: {response}"
                }
            ],
            "state": "modified",
            "llm_output": "NO",
            "score": false
        },
        {
            "id": 15,
            "desc": "Please first provide a **2-3 sentence summary of your ideas on the assessment based on the context provided**.",
            "other_info": {
                "from": "system_para_2",
                "type_explanation": "The constraint specifies the structure and presentation of the output by requiring it to be a '2-3 sentence summary,' which governs the format and length of the response rather than its content or resource limitations.",
                "meta_expalnation": "The given constraint directly governs the model's output by specifying content requirements (a 2-3 sentence summary of assessment ideas). It does not include any rules about managing or prioritizing multiple constraints, nor does it concern high-level strategies for selecting, ignoring, or combining constraints. Therefore, it is not a Meta Constraint.",
                "evaluation_type": {
                    "constraint_type": "llm_assisted_code",
                    "explanation": "human_modified"
                },
                "evaluation_generation_success": false
            },
            "dimension": "unconditional",
            "is_meta": false,
            "type": [
                "formatting"
            ],
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "Extract the summary where the response author presents their ideas on the assessment based on the given context. Return the extracted content verbatim from the response. If multiple segments are found, return them as a Python-style list of strings. If nothing is found, return an empty string (\"\").\n\nHere is the model response: {response}"
                },
                {
                    "type": "code",
                    "exec": "import re\n\ndef check_following(response: str) -> bool:\n    first_part = response.split('\\n\\n')[0] if '\\n\\n' in response else response\n    sentences = re.split('[.!?]', first_part)\n    sentences = [s.strip() for s in sentences if s.strip()]\n    return 2 <= len(sentences) <= 3"
                }
            ],
            "state": "modified",
            "score": false
        },
        {
            "id": 11,
            "desc": "Start your response with: '## ASSESSMENT Design'.",
            "other_info": {
                "from": "system_para_3",
                "type_explanation": "The constraint specifies the structure and presentation format of the output, requiring the response to start with the exact text '## ASSESSMENT Design'. This aligns with guidelines on syntax and layout norms, characteristic of the formatting category.",
                "meta_expalnation": "The constraint directly specifies the format in which the model's output should begin ('## ASSESSMENT Design'). It does not govern the management of multiple constraints, nor does it define selection, prioritization, disabling, deduplication, or composition rules. Therefore, it is not a Meta Constraint.",
                "evaluation_type": {
                    "constraint_type": "code",
                    "explanation": "This constraint requires checking if the response starts with the specific string '## ASSESSMENT Design'. This is a simple, exact match check and can be directly validated using straightforward string operations."
                },
                "evaluation_generation_success": true
            },
            "dimension": "unconditional",
            "is_meta": false,
            "type": [
                "formatting"
            ],
            "evaluation": [
                {
                    "type": "code",
                    "exec": "import re\n\ndef check_following(response: str) -> bool:\n    return bool(re.match(r'^## ASSESSMENT Design', response))\n"
                }
            ],
            "state": "success",
            "score": false
        },
        {
            "id": 0,
            "desc": "Create a safe space by acknowledging their courage in seeking support.",
            "other_info": {
                "from": "system_para_0",
                "type_explanation": "The constraint focuses on the tone and content of the response, ensuring it is empathetic, supportive, and acknowledges the courage of the individual seeking support. This aligns with the semantic category as it pertains to style, tone, and meaningful content.",
                "meta_expalnation": "The given constraint directly deals with how the output should be structured by asking the model to acknowledge courage and create a safe space, which is a directive related to content and tone. It does not manage or govern other constraints (e.g., selecting, prioritizing, or merging them), so it is not a Meta Constraint.",
                "evaluation_type": {
                    "constraint_type": "llm",
                    "explanation": "Creating a safe space and acknowledging courage involves subjective interpretation, empathetic communication, and a nuanced understanding of tone and context, which require semantic assessment by an LLM."
                },
                "evaluation_generation_success": true
            },
            "dimension": "unconditional",
            "is_meta": false,
            "type": [
                "semantic"
            ],
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "Does the model response explicitly acknowledge the user's courage in seeking support? Please answer YES/NO directly and do not enter anything else.\n\nHere is the model response: {response}"
                }
            ],
            "state": "success",
            "llm_output": "NO",
            "score": false
        },
        {
            "id": 2,
            "desc": "Ask targeted follow-up questions to understand their full situation.",
            "other_info": {
                "from": "system_para_0",
                "type_explanation": "The constraint focuses on ensuring that meaningful and complete information is gathered by asking targeted follow-up questions. This aligns with the goal of maintaining semantic accuracy and completeness in understanding the user's situation.",
                "meta_expalnation": "This constraint directly governs the model's behavior (i.e., asking follow-up questions) and does not define strategies for managing, selecting, or prioritizing other constraints. It impacts the content and process of generating output, rather than providing a high-level rule for handling multiple constraints.",
                "evaluation_type": {
                    "constraint_type": "llm",
                    "explanation": "Asking targeted follow-up questions requires open-ended, semantic understanding of the user's input, emotional state, and overall context, which can only be assessed subjectively by an LLM rather than directly through code."
                },
                "evaluation_generation_success": true
            },
            "dimension": "unconditional",
            "is_meta": false,
            "type": [
                "semantic"
            ],
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "Does the model response include targeted follow-up questions that aim to understand the user's full situation? Please answer YES/NO directly and do not enter anything else.\n\nHere is the model response: {response}"
                }
            ],
            "state": "success",
            "llm_output": "NO",
            "score": false
        },
        {
            "id": 4,
            "desc": "Assess risk levels with validated screening approaches.",
            "other_info": {
                "from": "system_para_0",
                "type_explanation": "The constraint focuses on ensuring meaningful and accurate content by requiring the use of 'validated screening approaches' for risk assessment. This directly pertains to content accuracy and adherence to established methods, which aligns with the 'semantic' category.",
                "meta_expalnation": "The given constraint directly instructs the model to assess risk levels using validated screening approaches, which is an operational rule affecting the content or execution of the task rather than managing multiple constraints. It does not involve selection, prioritization, disabling, deduplication, or composition of other constraints, so it is not a Meta Constraint.",
                "evaluation_type": {
                    "constraint_type": "llm",
                    "explanation": "This constraint involves assessing risk levels, which requires semantic understanding of clinical context, interpretation of nuanced language, and the application of validated screening approaches—a highly subjective and open-ended process. It cannot be directly encoded into logic or extracted in a structured way for validation."
                },
                "evaluation_generation_success": true
            },
            "dimension": "unconditional",
            "is_meta": false,
            "type": [
                "semantic"
            ],
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "Does the response include an assessment of risk levels using validated screening approaches? Please answer YES/NO directly and do not enter anything else.\n\nHere is the model response: {response}"
                }
            ],
            "state": "success",
            "llm_output": "NO",
            "score": false
        },
        {
            "id": 5,
            "desc": "Help them understand their current mental health in accessible language.",
            "other_info": {
                "from": "system_para_0",
                "type_explanation": "This constraint focuses on the style of communication ('accessible language') and the meaningfulness of the content ('help them understand their current mental health'). It emphasizes the tone and clarity, which falls under ensuring the output is appropriate, accurate, and understandable for the intended audience, aligning with the semantic category.",
                "meta_expalnation": "The given constraint directly governs the model's output by specifying content and language requirements, i.e., providing mental health explanations in accessible language. It does not define strategies for managing multiple constraints, and therefore does not qualify as a meta constraint.",
                "evaluation_type": {
                    "constraint_type": "llm",
                    "explanation": "This requires semantic understanding and the ability to explain mental health concepts in an accessible and empathetic manner, which involves open-ended, subjective assessment that only an LLM can accomplish."
                },
                "evaluation_generation_success": true
            },
            "dimension": "unconditional",
            "is_meta": false,
            "type": [
                "semantic"
            ],
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "Does the response explain the user's current mental health in accessible language that is easy to understand without using overly technical or complex terms? Please answer YES/NO directly and do not enter anything else.\n\nHere is the model response: {response}"
                }
            ],
            "state": "success",
            "llm_output": "NO",
            "score": false
        },
        {
            "id": 6,
            "desc": "Validate their experiences without minimizing or catastrophizing.",
            "other_info": {
                "from": "system_para_0",
                "type_explanation": "The constraint ensures that the output does not diminish or exaggerate the described experiences, requiring logical consistency, neutrality of position, and tone management. These are semantic requirements aimed at meaningful and appropriate communication.",
                "meta_expalnation": "The provided constraint directly affects the output by specifying how experiences should be validated (without minimizing or catastrophizing). It does not involve managing or interacting with other constraints, such as selecting, prioritizing, disabling, deduplicating, or combining them, which are the defining characteristics of a meta constraint.",
                "evaluation_type": {
                    "constraint_type": "llm",
                    "explanation": "Validating experiences without minimizing or catastrophizing requires subjective and semantic understanding of tone, intent, and nuance. This cannot be directly coded or reliably extracted for rule-based validation."
                },
                "evaluation_generation_success": true
            },
            "dimension": "unconditional",
            "is_meta": false,
            "type": [
                "semantic"
            ],
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "Does the response validate the user's experiences? Please answer YES/NO directly and do not enter anything else.\n\nHere is the model response: {response}"
                }
            ],
            "state": "modified",
            "llm_output": "NO",
            "score": false
        },
        {
            "id": 11,
            "desc": "Start your response with: '## ASSESSMENT Design'.",
            "other_info": {
                "from": "system_para_3",
                "type_explanation": "The constraint specifies the structure and presentation format of the output, requiring the response to start with the exact text '## ASSESSMENT Design'. This aligns with guidelines on syntax and layout norms, characteristic of the formatting category.",
                "meta_expalnation": "The constraint directly specifies the format in which the model's output should begin ('## ASSESSMENT Design'). It does not govern the management of multiple constraints, nor does it define selection, prioritization, disabling, deduplication, or composition rules. Therefore, it is not a Meta Constraint.",
                "evaluation_type": {
                    "constraint_type": "code",
                    "explanation": "This constraint requires checking if the response starts with the specific string '## ASSESSMENT Design'. This is a simple, exact match check and can be directly validated using straightforward string operations."
                },
                "evaluation_generation_success": true
            },
            "dimension": "unconditional",
            "is_meta": false,
            "type": [
                "formatting"
            ],
            "evaluation": [
                {
                    "type": "code",
                    "exec": "import re\n\ndef check_following(response: str) -> bool:\n    return bool(re.match(r'^## ASSESSMENT Design', response))\n"
                }
            ],
            "state": "success",
            "score": false
        },
        {
            "id": 0,
            "desc": "Create a safe space by acknowledging their courage in seeking support.",
            "other_info": {
                "from": "system_para_0",
                "type_explanation": "The constraint focuses on the tone and content of the response, ensuring it is empathetic, supportive, and acknowledges the courage of the individual seeking support. This aligns with the semantic category as it pertains to style, tone, and meaningful content.",
                "meta_expalnation": "The given constraint directly deals with how the output should be structured by asking the model to acknowledge courage and create a safe space, which is a directive related to content and tone. It does not manage or govern other constraints (e.g., selecting, prioritizing, or merging them), so it is not a Meta Constraint.",
                "evaluation_type": {
                    "constraint_type": "llm",
                    "explanation": "Creating a safe space and acknowledging courage involves subjective interpretation, empathetic communication, and a nuanced understanding of tone and context, which require semantic assessment by an LLM."
                },
                "evaluation_generation_success": true
            },
            "dimension": "unconditional",
            "is_meta": false,
            "type": [
                "semantic"
            ],
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "Does the model response explicitly acknowledge the user's courage in seeking support? Please answer YES/NO directly and do not enter anything else.\n\nHere is the model response: {response}"
                }
            ],
            "state": "success",
            "llm_output": "NO",
            "score": false
        },
        {
            "id": 2,
            "desc": "Ask targeted follow-up questions to understand their full situation.",
            "other_info": {
                "from": "system_para_0",
                "type_explanation": "The constraint focuses on ensuring that meaningful and complete information is gathered by asking targeted follow-up questions. This aligns with the goal of maintaining semantic accuracy and completeness in understanding the user's situation.",
                "meta_expalnation": "This constraint directly governs the model's behavior (i.e., asking follow-up questions) and does not define strategies for managing, selecting, or prioritizing other constraints. It impacts the content and process of generating output, rather than providing a high-level rule for handling multiple constraints.",
                "evaluation_type": {
                    "constraint_type": "llm",
                    "explanation": "Asking targeted follow-up questions requires open-ended, semantic understanding of the user's input, emotional state, and overall context, which can only be assessed subjectively by an LLM rather than directly through code."
                },
                "evaluation_generation_success": true
            },
            "dimension": "unconditional",
            "is_meta": false,
            "type": [
                "semantic"
            ],
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "Does the model response include targeted follow-up questions that aim to understand the user's full situation? Please answer YES/NO directly and do not enter anything else.\n\nHere is the model response: {response}"
                }
            ],
            "state": "success",
            "llm_output": "NO",
            "score": false
        },
        {
            "id": 4,
            "desc": "Assess risk levels with validated screening approaches.",
            "other_info": {
                "from": "system_para_0",
                "type_explanation": "The constraint focuses on ensuring meaningful and accurate content by requiring the use of 'validated screening approaches' for risk assessment. This directly pertains to content accuracy and adherence to established methods, which aligns with the 'semantic' category.",
                "meta_expalnation": "The given constraint directly instructs the model to assess risk levels using validated screening approaches, which is an operational rule affecting the content or execution of the task rather than managing multiple constraints. It does not involve selection, prioritization, disabling, deduplication, or composition of other constraints, so it is not a Meta Constraint.",
                "evaluation_type": {
                    "constraint_type": "llm",
                    "explanation": "This constraint involves assessing risk levels, which requires semantic understanding of clinical context, interpretation of nuanced language, and the application of validated screening approaches—a highly subjective and open-ended process. It cannot be directly encoded into logic or extracted in a structured way for validation."
                },
                "evaluation_generation_success": true
            },
            "dimension": "unconditional",
            "is_meta": false,
            "type": [
                "semantic"
            ],
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "Does the response include an assessment of risk levels using validated screening approaches? Please answer YES/NO directly and do not enter anything else.\n\nHere is the model response: {response}"
                }
            ],
            "state": "success",
            "llm_output": "NO",
            "score": false
        }
    ],
    "conditional": [
        {
            "id": 5,
            "desc": "必须使用get_lawfirm_info查看律师事务所的负责人，不要直接使用案件的\"被告律师事务所\"",
            "other_info": {
                "available_packages": [
                    "math",
                    "time",
                    "stat",
                    "queue",
                    "itertools",
                    "statistics",
                    "random",
                    "collections",
                    "unicodedata"
                ]
            },
            "dimension": "conditional",
            "type": "resource",
            "is_meta": false,
            "evaluation": [
                {
                    "type": "llm_conditional_check",
                    "exec": "response是否提及律师事务所。直接回答 YES 或 NO。\n\nresponse: {response}"
                },
                {
                    "type": "code",
                    "exec": "\ndef check_following(response: str) -> bool:\n    if \"get_lawfirm_info\" in response:\n        return True\n    else:\n        return False\n"
                }
            ],
            "score": false
        },
        {
            "id": 2,
            "desc": "在写代码时你需要综合考虑最终目标<final_goal></final_goal>和本轮的任务<task></task>。",
            "other_info": {
                "from": "system_para_1",
                "first_task": "将9000万元转换为亿元，并保留两位小数。"
            },
            "is_meta": true,
            "type": "semantic",
            "dimension": "conditional",
            "dependency": [],
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "检查 response 是否严格按 task 要求执行核心操作（允许 print 辅助输出），没有执行任何其他额外操作，直接回答 YES 或 NO。\n\ntask: 将9000万元转换为亿元，并保留两位小数。\n\nresponse:{response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": 2,
            "desc": "在写代码时你需要综合考虑最终目标<final_goal></final_goal>和本轮的任务<task></task>。",
            "other_info": {
                "from": "system_para_1",
                "first_task": "根据查询结果，唐山港的全称是“唐山港集团股份有限公司”。"
            },
            "is_meta": true,
            "type": "semantic",
            "dimension": "conditional",
            "dependency": [],
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "检查 response 是否严格按 task 要求执行核心操作（允许 print 辅助输出），没有执行任何其他额外操作，直接回答 YES 或 NO。\n\ntask: 根据查询结果，唐山港的全称是“唐山港集团股份有限公司”。\n\nresponse:{response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": 2,
            "desc": "在写代码时你需要综合考虑最终目标<final_goal></final_goal>和本轮的任务<task></task>。",
            "other_info": {
                "from": "system_para_1",
                "first_task": "使用get_company_info函数来查询上海家化的全称"
            },
            "is_meta": true,
            "type": "semantic",
            "dimension": "conditional",
            "dependency": [],
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "检查 response 是否严格按 task 要求执行核心操作（允许 print 辅助输出），没有执行任何其他额外操作，直接回答 YES 或 NO。\n\ntask: 使用get_company_info函数来查询上海家化的全称\n\nresponse:{response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": 5,
            "desc": "必须使用get_lawfirm_info查看律师事务所的负责人，不要直接使用案件的\"被告律师事务所\"",
            "other_info": {
                "available_packages": [
                    "math",
                    "time",
                    "stat",
                    "queue",
                    "itertools",
                    "statistics",
                    "random",
                    "collections",
                    "unicodedata"
                ]
            },
            "dimension": "conditional",
            "type": "resource",
            "is_meta": false,
            "evaluation": [
                {
                    "type": "llm_conditional_check",
                    "exec": "response是否提及律师事务所。直接回答 YES 或 NO。\n\nresponse: {response}"
                },
                {
                    "type": "code",
                    "exec": "\ndef check_following(response: str) -> bool:\n    if \"get_lawfirm_info\" in response:\n        return True\n    else:\n        return False\n"
                }
            ],
            "score": false
        },
        {
            "id": 2,
            "desc": "在写代码时你需要综合考虑最终目标<final_goal></final_goal>和本轮的任务<task></task>。",
            "other_info": {
                "from": "system_para_1",
                "first_task": "使用get_company_info来获取甘肃陇神戎发药业股份有限公司的公司全称"
            },
            "is_meta": true,
            "type": "semantic",
            "dimension": "conditional",
            "dependency": [],
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "检查 response 是否严格按 task 要求执行核心操作（允许 print 辅助输出），没有执行任何其他额外操作，直接回答 YES 或 NO。\n\ntask: 使用get_company_info来获取甘肃陇神戎发药业股份有限公司的公司全称\n\nresponse:{response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": 2,
            "desc": "在写代码时你需要综合考虑最终目标<final_goal></final_goal>和本轮的任务<task></task>。",
            "other_info": {
                "from": "system_para_1",
                "first_task": "我需要使用get_sub_company_info函数来查询安徽中能电源有限公司的母公司信息"
            },
            "is_meta": true,
            "type": "semantic",
            "dimension": "conditional",
            "dependency": [],
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "检查 response 是否严格按 task 要求执行核心操作（允许 print 辅助输出），没有执行任何其他额外操作，直接回答 YES 或 NO。\n\ntask: 我需要使用get_sub_company_info函数来查询安徽中能电源有限公司的母公司信息\n\nresponse:{response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": 2,
            "desc": "在写代码时你需要综合考虑最终目标<final_goal></final_goal>和本轮的任务<task></task>。",
            "other_info": {
                "from": "system_para_1",
                "first_task": "使用 get_company_info 函数获取金宏气体股份有限公司的代码、法人代表以及董秘的详细信息"
            },
            "is_meta": true,
            "type": "semantic",
            "dimension": "conditional",
            "dependency": [],
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "检查 response 是否严格按 task 要求执行核心操作（允许 print 辅助输出），没有执行任何其他额外操作，直接回答 YES 或 NO。\n\ntask: 使用 get_company_info 函数获取金宏气体股份有限公司的代码、法人代表以及董秘的详细信息\n\nresponse:{response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": 2,
            "desc": "在写代码时你需要综合考虑最终目标<final_goal></final_goal>和本轮的任务<task></task>。",
            "other_info": {
                "from": "system_para_1",
                "first_task": "调用get_company_register找到该公司的'注册资本'"
            },
            "is_meta": true,
            "type": "semantic",
            "dimension": "conditional",
            "dependency": [],
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "检查 response 是否严格按 task 要求执行核心操作（允许 print 辅助输出），没有执行任何其他额外操作，直接回答 YES 或 NO。\n\ntask: 调用get_company_register找到该公司的'注册资本'\n\nresponse:{response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": 5,
            "desc": "必须使用get_lawfirm_info查看律师事务所的负责人，不要直接使用案件的\"被告律师事务所\"",
            "other_info": {
                "available_packages": [
                    "math",
                    "time",
                    "stat",
                    "queue",
                    "itertools",
                    "statistics",
                    "random",
                    "collections",
                    "unicodedata"
                ]
            },
            "dimension": "conditional",
            "type": "resource",
            "is_meta": false,
            "evaluation": [
                {
                    "type": "llm_conditional_check",
                    "exec": "response是否提及律师事务所。直接回答 YES 或 NO。\n\nresponse: {response}"
                },
                {
                    "type": "code",
                    "exec": "\ndef check_following(response: str) -> bool:\n    if \"get_lawfirm_info\" in response:\n        return True\n    else:\n        return False\n"
                }
            ],
            "score": false
        },
        {
            "id": 2,
            "desc": "在写代码时你需要综合考虑最终目标<final_goal></final_goal>和本轮的任务<task></task>。",
            "other_info": {
                "from": "system_para_1",
                "first_task": "我将使用get_legal_document函数获取案件编号为（2019）京0105民初9223号的法律文书内容"
            },
            "is_meta": true,
            "type": "semantic",
            "dimension": "conditional",
            "dependency": [],
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "检查 response 是否严格按 task 要求执行核心操作（允许 print 辅助输出），没有执行任何其他额外操作，直接回答 YES 或 NO。\n\ntask: 我将使用get_legal_document函数获取案件编号为（2019）京0105民初9223号的法律文书内容\n\nresponse:{response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": 2,
            "desc": "在写代码时你需要综合考虑最终目标<final_goal></final_goal>和本轮的任务<task></task>。",
            "other_info": {
                "from": "system_para_1",
                "first_task": "我需要使用get_legal_document来获取（2020）苏0492民初2530号案件的文档内容。"
            },
            "is_meta": true,
            "type": "semantic",
            "dimension": "conditional",
            "dependency": [],
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "检查 response 是否严格按 task 要求执行核心操作（允许 print 辅助输出），没有执行任何其他额外操作，直接回答 YES 或 NO。\n\ntask: 我需要使用get_legal_document来获取（2020）苏0492民初2530号案件的文档内容。\n\nresponse:{response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": 2,
            "desc": "在写代码时你需要综合考虑最终目标<final_goal></final_goal>和本轮的任务<task></task>。",
            "other_info": {
                "from": "system_para_1",
                "first_task": "我需要使用get_sub_company_info来获取包钢天彩靖江科技有限公司的母公司信息。"
            },
            "is_meta": true,
            "type": "semantic",
            "dimension": "conditional",
            "dependency": [],
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "检查 response 是否严格按 task 要求执行核心操作（允许 print 辅助输出），没有执行任何其他额外操作，直接回答 YES 或 NO。\n\ntask: 我需要使用get_sub_company_info来获取包钢天彩靖江科技有限公司的母公司信息。\n\nresponse:{response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": 2,
            "desc": "在写代码时你需要综合考虑最终目标<final_goal></final_goal>和本轮的任务<task></task>。",
            "other_info": {
                "from": "system_para_1",
                "first_task": "将1800万元转换为亿元，并精确到小数点后两位"
            },
            "is_meta": true,
            "type": "semantic",
            "dimension": "conditional",
            "dependency": [],
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "检查 response 是否严格按 task 要求执行核心操作（允许 print 辅助输出），没有执行任何其他额外操作，直接回答 YES 或 NO。\n\ntask: 将1800万元转换为亿元，并精确到小数点后两位\n\nresponse:{response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": 2,
            "desc": "在写代码时你需要综合考虑最终目标<final_goal></final_goal>和本轮的任务<task></task>。",
            "other_info": {
                "from": "system_para_1",
                "first_task": "根据查询结果，上海家化的全称是“上海家化联合股份有限公司”。"
            },
            "is_meta": true,
            "type": "semantic",
            "dimension": "conditional",
            "dependency": [],
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "检查 response 是否严格按 task 要求执行核心操作（允许 print 辅助输出），没有执行任何其他额外操作，直接回答 YES 或 NO。\n\ntask: 根据查询结果，上海家化的全称是“上海家化联合股份有限公司”。\n\nresponse:{response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": 5,
            "desc": "必须使用get_lawfirm_info查看律师事务所的负责人，不要直接使用案件的\"被告律师事务所\"",
            "other_info": {
                "available_packages": [
                    "math",
                    "time",
                    "stat",
                    "queue",
                    "itertools",
                    "statistics",
                    "random",
                    "collections",
                    "unicodedata"
                ]
            },
            "dimension": "conditional",
            "type": "resource",
            "is_meta": false,
            "evaluation": [
                {
                    "type": "llm_conditional_check",
                    "exec": "response是否提及律师事务所。直接回答 YES 或 NO。\n\nresponse: {response}"
                },
                {
                    "type": "code",
                    "exec": "\ndef check_following(response: str) -> bool:\n    if \"get_lawfirm_info\" in response:\n        return True\n    else:\n        return False\n"
                }
            ],
            "score": false
        },
        {
            "id": 5,
            "desc": "必须用get_court_code获取法院的区划代码",
            "other_info": {
                "available_packages": [
                    "math",
                    "time",
                    "stat",
                    "queue",
                    "itertools",
                    "statistics",
                    "random",
                    "collections",
                    "unicodedata"
                ]
            },
            "dimension": "conditional",
            "type": "resource",
            "is_meta": false,
            "evaluation": [
                {
                    "type": "llm_conditional_check",
                    "exec": "response是否提及【法院的区划代码】相关问题。直接回答 YES 或 NO。\n\nresponse: {response}"
                },
                {
                    "type": "code",
                    "exec": "\ndef check_following(response: str) -> bool:\n    if \"get_court_code\" in response:\n        return True\n    else:\n        return False\n"
                }
            ],
            "score": false
        },
        {
            "id": 2,
            "desc": "在写代码时你需要综合考虑最终目标<final_goal></final_goal>和本轮的任务<task></task>。",
            "other_info": {
                "from": "system_para_1",
                "first_task": "根据查询结果，安徽中能电源有限公司的母公司是天能电池集团股份有限公司。"
            },
            "is_meta": true,
            "type": "semantic",
            "dimension": "conditional",
            "dependency": [],
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "检查 response 是否严格按 task 要求执行核心操作（允许 print 辅助输出），没有执行任何其他额外操作，直接回答 YES 或 NO。\n\ntask: 根据查询结果，安徽中能电源有限公司的母公司是天能电池集团股份有限公司。\n\nresponse:{response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": 2,
            "desc": "在写代码时你需要综合考虑最终目标<final_goal></final_goal>和本轮的任务<task></task>。",
            "other_info": {
                "from": "system_para_1",
                "first_task": "将投资金额1.31亿转换为亿元并保留2位小数"
            },
            "is_meta": true,
            "type": "semantic",
            "dimension": "conditional",
            "dependency": [],
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "检查 response 是否严格按 task 要求执行核心操作（允许 print 辅助输出），没有执行任何其他额外操作，直接回答 YES 或 NO。\n\ntask: 将投资金额1.31亿转换为亿元并保留2位小数\n\nresponse:{response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": 2,
            "desc": "在写代码时你需要综合考虑最终目标<final_goal></final_goal>和本轮的任务<task></task>。",
            "other_info": {
                "from": "system_para_1",
                "first_task": "我将使用这些信息来回答用户的问题"
            },
            "is_meta": true,
            "type": "semantic",
            "dimension": "conditional",
            "dependency": [],
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "检查 response 是否严格按 task 要求执行核心操作（允许 print 辅助输出），没有执行任何其他额外操作，直接回答 YES 或 NO。\n\ntask: 我将使用这些信息来回答用户的问题\n\nresponse:{response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": 9,
            "desc": "finish函数将在用于返回用户的final_goal的结果时结束推理流程",
            "other_info": {},
            "dimension": "conditional",
            "type": "formatting",
            "is_meta": false,
            "evaluation": [
                {
                    "type": "code",
                    "exec": "\ndef check_following(response: str) -> bool:\n    if \"finish()\" in response:\n        return True\n    else:\n        return False\n"
                }
            ],
            "score": false
        },
        {
            "id": 2,
            "desc": "在写代码时你需要综合考虑最终目标<final_goal></final_goal>和本轮的任务<task></task>。",
            "other_info": {
                "from": "system_para_1",
                "first_task": "根据获取的信息，金宏气体股份有限公司的公司代码是688106，法人代表是金向华，董秘是陈莹。"
            },
            "is_meta": true,
            "type": "semantic",
            "dimension": "conditional",
            "dependency": [],
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "检查 response 是否严格按 task 要求执行核心操作（允许 print 辅助输出），没有执行任何其他额外操作，直接回答 YES 或 NO。\n\ntask: 根据获取的信息，金宏气体股份有限公司的公司代码是688106，法人代表是金向华，董秘是陈莹。\n\nresponse:{response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": 2,
            "desc": "在写代码时你需要综合考虑最终目标<final_goal></final_goal>和本轮的任务<task></task>。",
            "other_info": {
                "from": "system_para_1",
                "first_task": "根据查询结果，我已经获得了江苏易智供应链管理有限公司的统一社会信用代码"
            },
            "is_meta": true,
            "type": "semantic",
            "dimension": "conditional",
            "dependency": [],
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "检查 response 是否严格按 task 要求执行核心操作（允许 print 辅助输出），没有执行任何其他额外操作，直接回答 YES 或 NO。\n\ntask: 根据查询结果，我已经获得了江苏易智供应链管理有限公司的统一社会信用代码\n\nresponse:{response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": 2,
            "desc": "在写代码时你需要综合考虑最终目标<final_goal></final_goal>和本轮的任务<task></task>。",
            "other_info": {
                "from": "system_para_1",
                "first_task": "筛选出涉案金额大于10000元的案件"
            },
            "is_meta": true,
            "type": "semantic",
            "dimension": "conditional",
            "dependency": [],
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "检查 response 是否严格按 task 要求执行核心操作（允许 print 辅助输出），没有执行任何其他额外操作，直接回答 YES 或 NO。\n\ntask: 筛选出涉案金额大于10000元的案件\n\nresponse:{response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": 2,
            "desc": "在写代码时你需要综合考虑最终目标<final_goal></final_goal>和本轮的任务<task></task>。",
            "other_info": {
                "from": "system_para_1",
                "first_task": "筛选出投资金额超过12.5亿元的公司"
            },
            "is_meta": true,
            "type": "semantic",
            "dimension": "conditional",
            "dependency": [],
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "检查 response 是否严格按 task 要求执行核心操作（允许 print 辅助输出），没有执行任何其他额外操作，直接回答 YES 或 NO。\n\ntask: 筛选出投资金额超过12.5亿元的公司\n\nresponse:{response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": 2,
            "desc": "在写代码时你需要综合考虑最终目标<final_goal></final_goal>和本轮的任务<task></task>。",
            "other_info": {
                "from": "system_para_1",
                "first_task": "我需要获取金诚信矿业管理股份有限公司投资的子公司信息。"
            },
            "is_meta": true,
            "type": "semantic",
            "dimension": "conditional",
            "dependency": [],
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "检查 response 是否严格按 task 要求执行核心操作（允许 print 辅助输出），没有执行任何其他额外操作，直接回答 YES 或 NO。\n\ntask: 我需要获取金诚信矿业管理股份有限公司投资的子公司信息。\n\nresponse:{response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": 2,
            "desc": "在写代码时你需要综合考虑最终目标<final_goal></final_goal>和本轮的任务<task></task>。",
            "other_info": {
                "from": "system_para_1",
                "first_task": "根据获取的信息，华灿光电股份有限公司的股票代码是300323。"
            },
            "is_meta": true,
            "type": "semantic",
            "dimension": "conditional",
            "dependency": [],
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "检查 response 是否严格按 task 要求执行核心操作（允许 print 辅助输出），没有执行任何其他额外操作，直接回答 YES 或 NO。\n\ntask: 根据获取的信息，华灿光电股份有限公司的股票代码是300323。\n\nresponse:{response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": 2,
            "desc": "在写代码时你需要综合考虑最终目标<final_goal></final_goal>和本轮的任务<task></task>。",
            "other_info": {
                "from": "system_para_1",
                "first_task": "根据查询结果，案件编号为（2019）京0105民初9223号的原告是北京市桃李食品有限公司，被告是北京全时叁陆伍连锁便利店有限公司。"
            },
            "is_meta": true,
            "type": "semantic",
            "dimension": "conditional",
            "dependency": [],
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "检查 response 是否严格按 task 要求执行核心操作（允许 print 辅助输出），没有执行任何其他额外操作，直接回答 YES 或 NO。\n\ntask: 根据查询结果，案件编号为（2019）京0105民初9223号的原告是北京市桃李食品有限公司，被告是北京全时叁陆伍连锁便利店有限公司。\n\nresponse:{response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": 2,
            "desc": "在写代码时你需要综合考虑最终目标<final_goal></final_goal>和本轮的任务<task></task>。",
            "other_info": {
                "from": "system_para_1",
                "first_task": "筛选出参股比例为'100.0'且投资金额超过12.5亿元的子公司信息"
            },
            "is_meta": true,
            "type": "semantic",
            "dimension": "conditional",
            "dependency": [],
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "检查 response 是否严格按 task 要求执行核心操作（允许 print 辅助输出），没有执行任何其他额外操作，直接回答 YES 或 NO。\n\ntask: 筛选出参股比例为'100.0'且投资金额超过12.5亿元的子公司信息\n\nresponse:{response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": 3,
            "desc": "If using LaTeX, use double $$ as a delimiter instead of single $.",
            "other_info": {
                "from": "system_para_0",
                "type_explanation": "This constraint controls the structure and presentation format of the output, specifically the syntax norms of LaTeX by requiring double $$ delimiters rather than single $. This aligns directly with formatting requirements.",
                "meta_expalnation": "The given constraint directly governs the output format in terms of how LaTeX is delimited. It specifies a content-related output rule ('use double $$ instead of single $') based on a condition ('if using LaTeX'). Therefore, it is not a high-level rule managing other constraints, and does not qualify as a Meta Constraint.",
                "evaluation_type": {
                    "constraint_type": "code",
                    "explanation": "The constraint pertains to a specific formatting rule (using double $$ instead of single $), which can be validated through simple pattern matching and logic in code."
                },
                "evaluation_generation_success": true
            },
            "dimension": "conditional",
            "is_meta": false,
            "type": [
                "formatting"
            ],
            "evaluation": [
                {
                    "type": "code",
                    "exec": "\nimport re\n\ndef check_following(response: str) -> bool:\n    if re.search(r'(?<!\\$)\\$(?!\\$)', response):\n        return False\n    count = response.count('$$')\n    if count % 2 != 0:\n        return False\n    return True\n"
                }
            ],
            "state": "modified",
            "score": false
        },
        {
            "id": 2,
            "desc": "在写代码时你需要综合考虑最终目标<final_goal></final_goal>和本轮的任务<task></task>。",
            "other_info": {
                "from": "system_para_1",
                "first_task": "根据获取的案件信息，原告是中国建设银行股份有限公司常州经济开发区支行，被告包括常州市互联涂料有限公司、江苏新互盛电缆有限公司、常州格林电力机械制造有限公司、周某某、任某某、常州市神猴焊丝有限公司。"
            },
            "is_meta": true,
            "type": "semantic",
            "dimension": "conditional",
            "dependency": [],
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "检查 response 是否严格按 task 要求执行核心操作（允许 print 辅助输出），没有执行任何其他额外操作，直接回答 YES 或 NO。\n\ntask: 根据获取的案件信息，原告是中国建设银行股份有限公司常州经济开发区支行，被告包括常州市互联涂料有限公司、江苏新互盛电缆有限公司、常州格林电力机械制造有限公司、周某某、任某某、常州市神猴焊丝有限公司。\n\nresponse:{response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": 44,
            "desc": "Use <lov-success> to confirm successful operations.",
            "other_info": {
                "from": "system_para_0",
                "condition_desc": "If the model is confirming successful operations, then wrap them in <lov-success>.",
                "complete_instruction_para": [],
                "evaluation_type": {
                    "constraint_type": "code",
                    "explanation": "The constraint specifies the use of a specific tag (<lov-success>) to confirm successful operations, which can be directly validated by checking the presence and correct usage of the tag in the response."
                },
                "evaluation_generation_success": true
            },
            "dimension": "conditional",
            "is_meta": false,
            "type": [
                "semantic"
            ],
            "evaluation": [
                {
                    "type": "llm_conditional_check",
                    "exec": "Is the model confirming successful operations? Please answer YES/NO directly and do not enter anything else.\n\nHere is the model response: {response}"
                },
                {
                    "type": "code",
                    "exec": "import re\n\ndef check_following(response: str) -> bool:\n    return bool(re.search(r\"<lov-success>.*?</lov-success>\", response, re.DOTALL))\n"
                }
            ],
            "score": false
        },
        {
            "id": 74,
            "desc": "If you added new files, remember that you need to implement them fully.",
            "other_info": {
                "from": "system_para_138",
                "condition_desc": "If you added new files, then remember that you need to implement them fully.",
                "complete_instruction_para": [],
                "evaluation_type": {
                    "constraint_type": "llm",
                    "explanation": "The constraint requires checking whether new files are fully implemented, which can be validated directly by code by ensuring all necessary content is present in the files."
                },
                "evaluation_generation_success": true
            },
            "dimension": "conditional",
            "is_meta": false,
            "type": [
                "semantic"
            ],
            "evaluation": [
                {
                    "type": "llm_conditional_check",
                    "exec": "Did the model add any new files? Please answer YES/NO directly and do not enter anything else.\n\nHere is the model response: {response}"
                },
                {
                    "type": "llm",
                    "exec": "Does the model implement new files fully? Please answer YES/NO directly and do not enter anything else.\n\nHere is the model response: {response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": 99,
            "desc": "Clearly communicate to the user that some specific features were not implemented.",
            "other_info": {
                "from": "system_para_207",
                "condition_desc": "If a user asks for many features at once, then you do not have to implement them all as long as you clearly communicate to the user that you didn't implement some specific features.",
                "complete_instruction_para": [],
                "evaluation_type": {
                    "constraint_type": "llm",
                    "explanation": "The constraint requires a semantic understanding of whether the response clearly communicates to the user which features were not implemented. This involves subjective assessment of clarity and completeness, which cannot be directly validated by code."
                },
                "evaluation_generation_success": true
            },
            "dimension": "conditional",
            "is_meta": false,
            "type": [
                "semantic"
            ],
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "Does the response clearly communicate to the user which specific features were not implemented, if not all features were implemented? Please answer YES/NO directly and do not enter anything else.\n\nHere is the model response: {response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": 114,
            "desc": "Make sure to close all tags when writing files, with a line break before the closing tag.",
            "other_info": {
                "from": "system_para_214",
                "condition_desc": "If you do a <lov-write> operation, then make sure to close all tags when writing files, with a line break before the closing tag.",
                "complete_instruction_para": [
                    "system_para_213"
                ],
                "evaluation_type": {
                    "constraint_type": "code",
                    "explanation": "The constraint involves checking for the presence of a line break before closing tags, which can be directly validated using code by analyzing the file's syntax and formatting."
                },
                "evaluation_generation_success": true
            },
            "dimension": "conditional",
            "is_meta": false,
            "type": [
                "formatting"
            ],
            "evaluation": [
                {
                    "type": "llm_conditional_check",
                    "exec": "Is the model performing a <lov-write> operation? Please answer YES/NO directly and do not enter anything else.\n\nHere is the model response: {response}"
                },
                {
                    "type": "code",
                    "exec": "\nimport re\nfrom typing import List\n\ndef check_following(response: str) -> bool:\n    # Find all opening and closing tags\n    tag_pattern = r'<([^>]+)>'\n    tags = re.findall(tag_pattern, response)\n    \n    # Stack to keep track of opening tags\n    stack: List[str] = []\n    \n    for tag in tags:\n        # Skip self-closing tags\n        if tag.endswith('/'):\n            continue\n            \n        # Handle closing tags\n        if tag.startswith('/'):\n            # If stack is empty, we found a closing tag without matching opening tag\n            if not stack:\n                return False\n                \n            # Get the expected opening tag name\n            expected_tag = stack[-1]\n            # Remove the '/' from the closing tag\n            actual_tag = tag[1:]\n            \n            # Check if tags match\n            if expected_tag != actual_tag:\n                return False\n                \n            # Remove the matched opening tag from stack\n            stack.pop()\n        else:\n            # Add opening tag to stack\n            stack.append(tag)\n    \n    # If stack is not empty, we have unclosed tags\n    return len(stack) == 0\n"
                }
            ],
            "score": false
        },
        {
            "id": 44,
            "desc": "Use <lov-success> to confirm successful operations.",
            "other_info": {
                "from": "system_para_0",
                "condition_desc": "If the model is confirming successful operations, then wrap them in <lov-success>.",
                "complete_instruction_para": [],
                "evaluation_type": {
                    "constraint_type": "code",
                    "explanation": "The constraint specifies the use of a specific tag (<lov-success>) to confirm successful operations, which can be directly validated by checking the presence and correct usage of the tag in the response."
                },
                "evaluation_generation_success": true
            },
            "dimension": "conditional",
            "is_meta": false,
            "type": [
                "semantic"
            ],
            "evaluation": [
                {
                    "type": "llm_conditional_check",
                    "exec": "Is the model confirming successful operations? Please answer YES/NO directly and do not enter anything else.\n\nHere is the model response: {response}"
                },
                {
                    "type": "code",
                    "exec": "import re\n\ndef check_following(response: str) -> bool:\n    return bool(re.search(r\"<lov-success>.*?</lov-success>\", response, re.DOTALL))\n"
                }
            ],
            "score": false
        },
        {
            "id": 67,
            "desc": "At the start of the <lov-code> block, outline step-by-step which files need to be edited or created to implement the user's request, and mention any dependencies that need to be installed.",
            "other_info": {
                "from": "system_para_138",
                "condition_desc": "If new code needs to be written (i.e., the requested feature does not exist), then at the start of the <lov-code> block, outline step-by-step which files need to be edited or created to implement the user's request, and mention any dependencies that need to be installed.",
                "complete_instruction_para": [
                    "system_para_137"
                ],
                "evaluation_type": {
                    "constraint_type": "llm",
                    "explanation": "The constraint involves checking for the presence of a step-by-step outline at the start of the <lov-code> block, which can be directly validated by parsing and analyzing the structure of the content in the block."
                },
                "evaluation_generation_success": true
            },
            "dimension": "conditional",
            "is_meta": false,
            "type": [
                "semantic"
            ],
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "Does the response at the start of the <lov-code> block outline step-by-step which files need to be edited or created to implement the user's request, and mention any dependencies that need to be installed? Please answer YES/NO directly and do not enter anything else.\n\nHere is the model response: {response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": 99,
            "desc": "Clearly communicate to the user that some specific features were not implemented.",
            "other_info": {
                "from": "system_para_207",
                "condition_desc": "If a user asks for many features at once, then you do not have to implement them all as long as you clearly communicate to the user that you didn't implement some specific features.",
                "complete_instruction_para": [],
                "evaluation_type": {
                    "constraint_type": "llm",
                    "explanation": "The constraint requires a semantic understanding of whether the response clearly communicates to the user which features were not implemented. This involves subjective assessment of clarity and completeness, which cannot be directly validated by code."
                },
                "evaluation_generation_success": true
            },
            "dimension": "conditional",
            "is_meta": false,
            "type": [
                "semantic"
            ],
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "Does the response clearly communicate to the user which specific features were not implemented, if not all features were implemented? Please answer YES/NO directly and do not enter anything else.\n\nHere is the model response: {response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": 108,
            "desc": "When files get too large, ask the user if they want you to refactor them",
            "other_info": {
                "from": "system_para_212",
                "condition_desc": "If files get too large, then ask the user if they want you to refactor them",
                "complete_instruction_para": [],
                "evaluation_type": {
                    "constraint_type": "llm",
                    "explanation": "This constraint requires understanding the context and intent of the user interaction, which involves semantic assessment and cannot be directly validated by code or through extraction and logic."
                },
                "evaluation_generation_success": true
            },
            "dimension": "conditional",
            "is_meta": false,
            "type": [
                "semantic"
            ],
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "Does the model response explicitly ask the user if they want the files to be refactored when they are getting too large? Please answer YES/NO directly and do not enter anything else.\n\nHere is the model response: {response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": 114,
            "desc": "Make sure to close all tags when writing files, with a line break before the closing tag.",
            "other_info": {
                "from": "system_para_214",
                "condition_desc": "If you do a <lov-write> operation, then make sure to close all tags when writing files, with a line break before the closing tag.",
                "complete_instruction_para": [
                    "system_para_213"
                ],
                "evaluation_type": {
                    "constraint_type": "code",
                    "explanation": "The constraint involves checking for the presence of a line break before closing tags, which can be directly validated using code by analyzing the file's syntax and formatting."
                },
                "evaluation_generation_success": true
            },
            "dimension": "conditional",
            "is_meta": false,
            "type": [
                "formatting"
            ],
            "evaluation": [
                {
                    "type": "llm_conditional_check",
                    "exec": "Is the model performing a <lov-write> operation? Please answer YES/NO directly and do not enter anything else.\n\nHere is the model response: {response}"
                },
                {
                    "type": "code",
                    "exec": "\nimport re\nfrom typing import List\n\ndef check_following(response: str) -> bool:\n    # Find all opening and closing tags\n    tag_pattern = r'<([^>]+)>'\n    tags = re.findall(tag_pattern, response)\n    \n    # Stack to keep track of opening tags\n    stack: List[str] = []\n    \n    for tag in tags:\n        # Skip self-closing tags\n        if tag.endswith('/'):\n            continue\n            \n        # Handle closing tags\n        if tag.startswith('/'):\n            # If stack is empty, we found a closing tag without matching opening tag\n            if not stack:\n                return False\n                \n            # Get the expected opening tag name\n            expected_tag = stack[-1]\n            # Remove the '/' from the closing tag\n            actual_tag = tag[1:]\n            \n            # Check if tags match\n            if expected_tag != actual_tag:\n                return False\n                \n            # Remove the matched opening tag from stack\n            stack.pop()\n        else:\n            # Add opening tag to stack\n            stack.append(tag)\n    \n    # If stack is not empty, we have unclosed tags\n    return len(stack) == 0\n"
                }
            ],
            "score": false
        },
        {
            "id": 44,
            "desc": "Use <lov-success> to confirm successful operations.",
            "other_info": {
                "from": "system_para_0",
                "condition_desc": "If the model is confirming successful operations, then wrap them in <lov-success>.",
                "complete_instruction_para": [],
                "evaluation_type": {
                    "constraint_type": "code",
                    "explanation": "The constraint specifies the use of a specific tag (<lov-success>) to confirm successful operations, which can be directly validated by checking the presence and correct usage of the tag in the response."
                },
                "evaluation_generation_success": true
            },
            "dimension": "conditional",
            "is_meta": false,
            "type": [
                "semantic"
            ],
            "evaluation": [
                {
                    "type": "llm_conditional_check",
                    "exec": "Is the model confirming successful operations? Please answer YES/NO directly and do not enter anything else.\n\nHere is the model response: {response}"
                },
                {
                    "type": "code",
                    "exec": "import re\n\ndef check_following(response: str) -> bool:\n    return bool(re.search(r\"<lov-success>.*?</lov-success>\", response, re.DOTALL))\n"
                }
            ],
            "score": false
        },
        {
            "id": 74,
            "desc": "If you added new files, remember that you need to implement them fully.",
            "other_info": {
                "from": "system_para_138",
                "condition_desc": "If you added new files, then remember that you need to implement them fully.",
                "complete_instruction_para": [],
                "evaluation_type": {
                    "constraint_type": "llm",
                    "explanation": "The constraint requires checking whether new files are fully implemented, which can be validated directly by code by ensuring all necessary content is present in the files."
                },
                "evaluation_generation_success": true
            },
            "dimension": "conditional",
            "is_meta": false,
            "type": [
                "semantic"
            ],
            "evaluation": [
                {
                    "type": "llm_conditional_check",
                    "exec": "Did the model add any new files? Please answer YES/NO directly and do not enter anything else.\n\nHere is the model response: {response}"
                },
                {
                    "type": "llm",
                    "exec": "Does the model implement new files fully? Please answer YES/NO directly and do not enter anything else.\n\nHere is the model response: {response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": 99,
            "desc": "Clearly communicate to the user that some specific features were not implemented.",
            "other_info": {
                "from": "system_para_207",
                "condition_desc": "If a user asks for many features at once, then you do not have to implement them all as long as you clearly communicate to the user that you didn't implement some specific features.",
                "complete_instruction_para": [],
                "evaluation_type": {
                    "constraint_type": "llm",
                    "explanation": "The constraint requires a semantic understanding of whether the response clearly communicates to the user which features were not implemented. This involves subjective assessment of clarity and completeness, which cannot be directly validated by code."
                },
                "evaluation_generation_success": true
            },
            "dimension": "conditional",
            "is_meta": false,
            "type": [
                "semantic"
            ],
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "Does the response clearly communicate to the user which specific features were not implemented, if not all features were implemented? Please answer YES/NO directly and do not enter anything else.\n\nHere is the model response: {response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": 114,
            "desc": "Make sure to close all tags when writing files, with a line break before the closing tag.",
            "other_info": {
                "from": "system_para_214",
                "condition_desc": "If you do a <lov-write> operation, then make sure to close all tags when writing files, with a line break before the closing tag.",
                "complete_instruction_para": [
                    "system_para_213"
                ],
                "evaluation_type": {
                    "constraint_type": "code",
                    "explanation": "The constraint involves checking for the presence of a line break before closing tags, which can be directly validated using code by analyzing the file's syntax and formatting."
                },
                "evaluation_generation_success": true
            },
            "dimension": "conditional",
            "is_meta": false,
            "type": [
                "formatting"
            ],
            "evaluation": [
                {
                    "type": "llm_conditional_check",
                    "exec": "Is the model performing a <lov-write> operation? Please answer YES/NO directly and do not enter anything else.\n\nHere is the model response: {response}"
                },
                {
                    "type": "code",
                    "exec": "\nimport re\nfrom typing import List\n\ndef check_following(response: str) -> bool:\n    # Find all opening and closing tags\n    tag_pattern = r'<([^>]+)>'\n    tags = re.findall(tag_pattern, response)\n    \n    # Stack to keep track of opening tags\n    stack: List[str] = []\n    \n    for tag in tags:\n        # Skip self-closing tags\n        if tag.endswith('/'):\n            continue\n            \n        # Handle closing tags\n        if tag.startswith('/'):\n            # If stack is empty, we found a closing tag without matching opening tag\n            if not stack:\n                return False\n                \n            # Get the expected opening tag name\n            expected_tag = stack[-1]\n            # Remove the '/' from the closing tag\n            actual_tag = tag[1:]\n            \n            # Check if tags match\n            if expected_tag != actual_tag:\n                return False\n                \n            # Remove the matched opening tag from stack\n            stack.pop()\n        else:\n            # Add opening tag to stack\n            stack.append(tag)\n    \n    # If stack is not empty, we have unclosed tags\n    return len(stack) == 0\n"
                }
            ],
            "score": false
        },
        {
            "id": 131,
            "desc": "If the user asks for a very specific design, you MUST follow it to the letter.",
            "other_info": {
                "from": "system_para_218",
                "condition_desc": "If the user asks for a very specific design, then you MUST follow it to the letter.",
                "complete_instruction_para": [],
                "evaluation_type": {
                    "constraint_type": "llm",
                    "explanation": "Determining whether the user's specific design has been followed 'to the letter' requires a semantic and subjective understanding of the design details and their implementation, which can only be assessed by an LLM."
                },
                "evaluation_generation_success": true
            },
            "dimension": "conditional",
            "is_meta": false,
            "type": [
                "semantic"
            ],
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "Does the model's response strictly adhere to the user's specific design instructions without deviating from the details they provided? Please answer YES/NO directly and do not enter anything else.\n\nHere is the model response: {response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": 44,
            "desc": "Use <lov-success> to confirm successful operations.",
            "other_info": {
                "from": "system_para_0",
                "condition_desc": "If the model is confirming successful operations, then wrap them in <lov-success>.",
                "complete_instruction_para": [],
                "evaluation_type": {
                    "constraint_type": "code",
                    "explanation": "The constraint specifies the use of a specific tag (<lov-success>) to confirm successful operations, which can be directly validated by checking the presence and correct usage of the tag in the response."
                },
                "evaluation_generation_success": true
            },
            "dimension": "conditional",
            "is_meta": false,
            "type": [
                "semantic"
            ],
            "evaluation": [
                {
                    "type": "llm_conditional_check",
                    "exec": "Is the model confirming successful operations? Please answer YES/NO directly and do not enter anything else.\n\nHere is the model response: {response}"
                },
                {
                    "type": "code",
                    "exec": "import re\n\ndef check_following(response: str) -> bool:\n    return bool(re.search(r\"<lov-success>.*?</lov-success>\", response, re.DOTALL))\n"
                }
            ],
            "score": false
        },
        {
            "id": 58,
            "desc": "Proceed with code edits only if the user explicitly requests changes or new features that have not already been implemented.",
            "other_info": {
                "from": "system_para_135",
                "condition_desc": "If the user explicitly requests changes or new features that have not already been implemented, then proceed with code edits.",
                "complete_instruction_para": [
                    ""
                ],
                "evaluation_type": {
                    "constraint_type": "llm",
                    "explanation": "Determining whether to proceed with code edits requires semantic understanding of the user's intent, which involves assessing whether their request explicitly indicates a desire for changes or new features. This is subjective and requires open-ended interpretation of the user's language."
                },
                "evaluation_generation_success": true
            },
            "dimension": "conditional",
            "is_meta": true,
            "type": [
                "semantic"
            ],
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "Does the model response explicitly proceed with code edits as instructed? Please answer YES/NO directly and do not enter anything else.\n\nHere is the model response: {response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": 74,
            "desc": "If you added new files, remember that you need to implement them fully.",
            "other_info": {
                "from": "system_para_138",
                "condition_desc": "If you added new files, then remember that you need to implement them fully.",
                "complete_instruction_para": [],
                "evaluation_type": {
                    "constraint_type": "llm",
                    "explanation": "The constraint requires checking whether new files are fully implemented, which can be validated directly by code by ensuring all necessary content is present in the files."
                },
                "evaluation_generation_success": true
            },
            "dimension": "conditional",
            "is_meta": false,
            "type": [
                "semantic"
            ],
            "evaluation": [
                {
                    "type": "llm_conditional_check",
                    "exec": "Did the model add any new files? Please answer YES/NO directly and do not enter anything else.\n\nHere is the model response: {response}"
                },
                {
                    "type": "llm",
                    "exec": "Does the model implement new files fully? Please answer YES/NO directly and do not enter anything else.\n\nHere is the model response: {response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": 114,
            "desc": "Make sure to close all tags when writing files, with a line break before the closing tag.",
            "other_info": {
                "from": "system_para_214",
                "condition_desc": "If you do a <lov-write> operation, then make sure to close all tags when writing files, with a line break before the closing tag.",
                "complete_instruction_para": [
                    "system_para_213"
                ],
                "evaluation_type": {
                    "constraint_type": "code",
                    "explanation": "The constraint involves checking for the presence of a line break before closing tags, which can be directly validated using code by analyzing the file's syntax and formatting."
                },
                "evaluation_generation_success": true
            },
            "dimension": "conditional",
            "is_meta": false,
            "type": [
                "formatting"
            ],
            "evaluation": [
                {
                    "type": "llm_conditional_check",
                    "exec": "Is the model performing a <lov-write> operation? Please answer YES/NO directly and do not enter anything else.\n\nHere is the model response: {response}"
                },
                {
                    "type": "code",
                    "exec": "\nimport re\nfrom typing import List\n\ndef check_following(response: str) -> bool:\n    # Find all opening and closing tags\n    tag_pattern = r'<([^>]+)>'\n    tags = re.findall(tag_pattern, response)\n    \n    # Stack to keep track of opening tags\n    stack: List[str] = []\n    \n    for tag in tags:\n        # Skip self-closing tags\n        if tag.endswith('/'):\n            continue\n            \n        # Handle closing tags\n        if tag.startswith('/'):\n            # If stack is empty, we found a closing tag without matching opening tag\n            if not stack:\n                return False\n                \n            # Get the expected opening tag name\n            expected_tag = stack[-1]\n            # Remove the '/' from the closing tag\n            actual_tag = tag[1:]\n            \n            # Check if tags match\n            if expected_tag != actual_tag:\n                return False\n                \n            # Remove the matched opening tag from stack\n            stack.pop()\n        else:\n            # Add opening tag to stack\n            stack.append(tag)\n    \n    # If stack is not empty, we have unclosed tags\n    return len(stack) == 0\n"
                }
            ],
            "score": false
        },
        {
            "id": 131,
            "desc": "If the user asks for a very specific design, you MUST follow it to the letter.",
            "other_info": {
                "from": "system_para_218",
                "condition_desc": "If the user asks for a very specific design, then you MUST follow it to the letter.",
                "complete_instruction_para": [],
                "evaluation_type": {
                    "constraint_type": "llm",
                    "explanation": "Determining whether the user's specific design has been followed 'to the letter' requires a semantic and subjective understanding of the design details and their implementation, which can only be assessed by an LLM."
                },
                "evaluation_generation_success": true
            },
            "dimension": "conditional",
            "is_meta": false,
            "type": [
                "semantic"
            ],
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "Does the model's response strictly adhere to the user's specific design instructions without deviating from the details they provided? Please answer YES/NO directly and do not enter anything else.\n\nHere is the model response: {response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": 44,
            "desc": "Use <lov-success> to confirm successful operations.",
            "other_info": {
                "from": "system_para_0",
                "condition_desc": "If the model is confirming successful operations, then wrap them in <lov-success>.",
                "complete_instruction_para": [],
                "evaluation_type": {
                    "constraint_type": "code",
                    "explanation": "The constraint specifies the use of a specific tag (<lov-success>) to confirm successful operations, which can be directly validated by checking the presence and correct usage of the tag in the response."
                },
                "evaluation_generation_success": true
            },
            "dimension": "conditional",
            "is_meta": false,
            "type": [
                "semantic"
            ],
            "evaluation": [
                {
                    "type": "llm_conditional_check",
                    "exec": "Is the model confirming successful operations? Please answer YES/NO directly and do not enter anything else.\n\nHere is the model response: {response}"
                },
                {
                    "type": "code",
                    "exec": "import re\n\ndef check_following(response: str) -> bool:\n    return bool(re.search(r\"<lov-success>.*?</lov-success>\", response, re.DOTALL))\n"
                }
            ],
            "score": false
        },
        {
            "id": 67,
            "desc": "At the start of the <lov-code> block, outline step-by-step which files need to be edited or created to implement the user's request, and mention any dependencies that need to be installed.",
            "other_info": {
                "from": "system_para_138",
                "condition_desc": "If new code needs to be written (i.e., the requested feature does not exist), then at the start of the <lov-code> block, outline step-by-step which files need to be edited or created to implement the user's request, and mention any dependencies that need to be installed.",
                "complete_instruction_para": [
                    "system_para_137"
                ],
                "evaluation_type": {
                    "constraint_type": "llm",
                    "explanation": "The constraint involves checking for the presence of a step-by-step outline at the start of the <lov-code> block, which can be directly validated by parsing and analyzing the structure of the content in the block."
                },
                "evaluation_generation_success": true
            },
            "dimension": "conditional",
            "is_meta": false,
            "type": [
                "semantic"
            ],
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "Does the response at the start of the <lov-code> block outline step-by-step which files need to be edited or created to implement the user's request, and mention any dependencies that need to be installed? Please answer YES/NO directly and do not enter anything else.\n\nHere is the model response: {response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": 99,
            "desc": "Clearly communicate to the user that some specific features were not implemented.",
            "other_info": {
                "from": "system_para_207",
                "condition_desc": "If a user asks for many features at once, then you do not have to implement them all as long as you clearly communicate to the user that you didn't implement some specific features.",
                "complete_instruction_para": [],
                "evaluation_type": {
                    "constraint_type": "llm",
                    "explanation": "The constraint requires a semantic understanding of whether the response clearly communicates to the user which features were not implemented. This involves subjective assessment of clarity and completeness, which cannot be directly validated by code."
                },
                "evaluation_generation_success": true
            },
            "dimension": "conditional",
            "is_meta": false,
            "type": [
                "semantic"
            ],
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "Does the response clearly communicate to the user which specific features were not implemented, if not all features were implemented? Please answer YES/NO directly and do not enter anything else.\n\nHere is the model response: {response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": 114,
            "desc": "Make sure to close all tags when writing files, with a line break before the closing tag.",
            "other_info": {
                "from": "system_para_214",
                "condition_desc": "If you do a <lov-write> operation, then make sure to close all tags when writing files, with a line break before the closing tag.",
                "complete_instruction_para": [
                    "system_para_213"
                ],
                "evaluation_type": {
                    "constraint_type": "code",
                    "explanation": "The constraint involves checking for the presence of a line break before closing tags, which can be directly validated using code by analyzing the file's syntax and formatting."
                },
                "evaluation_generation_success": true
            },
            "dimension": "conditional",
            "is_meta": false,
            "type": [
                "formatting"
            ],
            "evaluation": [
                {
                    "type": "llm_conditional_check",
                    "exec": "Is the model performing a <lov-write> operation? Please answer YES/NO directly and do not enter anything else.\n\nHere is the model response: {response}"
                },
                {
                    "type": "code",
                    "exec": "\nimport re\nfrom typing import List\n\ndef check_following(response: str) -> bool:\n    # Find all opening and closing tags\n    tag_pattern = r'<([^>]+)>'\n    tags = re.findall(tag_pattern, response)\n    \n    # Stack to keep track of opening tags\n    stack: List[str] = []\n    \n    for tag in tags:\n        # Skip self-closing tags\n        if tag.endswith('/'):\n            continue\n            \n        # Handle closing tags\n        if tag.startswith('/'):\n            # If stack is empty, we found a closing tag without matching opening tag\n            if not stack:\n                return False\n                \n            # Get the expected opening tag name\n            expected_tag = stack[-1]\n            # Remove the '/' from the closing tag\n            actual_tag = tag[1:]\n            \n            # Check if tags match\n            if expected_tag != actual_tag:\n                return False\n                \n            # Remove the matched opening tag from stack\n            stack.pop()\n        else:\n            # Add opening tag to stack\n            stack.append(tag)\n    \n    # If stack is not empty, we have unclosed tags\n    return len(stack) == 0\n"
                }
            ],
            "score": false
        },
        {
            "id": 131,
            "desc": "If the user asks for a very specific design, you MUST follow it to the letter.",
            "other_info": {
                "from": "system_para_218",
                "condition_desc": "If the user asks for a very specific design, then you MUST follow it to the letter.",
                "complete_instruction_para": [],
                "evaluation_type": {
                    "constraint_type": "llm",
                    "explanation": "Determining whether the user's specific design has been followed 'to the letter' requires a semantic and subjective understanding of the design details and their implementation, which can only be assessed by an LLM."
                },
                "evaluation_generation_success": true
            },
            "dimension": "conditional",
            "is_meta": false,
            "type": [
                "semantic"
            ],
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "Does the model's response strictly adhere to the user's specific design instructions without deviating from the details they provided? Please answer YES/NO directly and do not enter anything else.\n\nHere is the model response: {response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": 12,
            "desc": "If completing the user's task requires writing or modifying files, your code and final answer should fix the problem at the root cause rather than applying surface-level patches, when possible.",
            "other_info": {
                "from": "system_para_5",
                "condition_desc": "",
                "complete_instruction_para": [],
                "evaluation_type": {
                    "constraint_type": "llm",
                    "explanation": "The constraint requires semantic understanding to assess whether the solution addresses the root cause of the problem rather than applying surface-level fixes. This involves subjective judgment and cannot be directly validated by code or extracted content."
                },
                "evaluation_generation_success": true
            },
            "dimension": "conditional",
            "is_meta": false,
            "type": [
                "semantic"
            ],
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "Does the model's response demonstrate that the proposed solution addresses the root cause of the problem rather than applying a surface-level patch? Please answer YES/NO directly and do not enter anything else.\n\nHere is the model response: {response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": 15,
            "desc": "If completing the user's task requires writing or modifying files, your code and final answer should update documentation as necessary.",
            "other_info": {
                "from": "system_para_5",
                "condition_desc": "",
                "complete_instruction_para": [],
                "evaluation_type": {
                    "constraint_type": "llm",
                    "explanation": "Determining whether documentation has been updated as necessary requires a semantic understanding of the changes made and their relevance to the documentation. This is a subjective and open-ended task that cannot be directly validated by code."
                },
                "evaluation_generation_success": true
            },
            "dimension": "conditional",
            "is_meta": false,
            "type": [
                "semantic"
            ],
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "Does the model's response include updates to the documentation where necessary to reflect the changes made in the code? Please answer YES/NO directly and do not enter anything else.\n\nHere is the model response: {response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": 17,
            "desc": "If completing the user's task requires writing or modifying files, your code and final answer should use `git log` and `git blame` to search the history of the codebase if additional context is required; internet access is disabled.",
            "other_info": {
                "from": "system_para_5",
                "condition_desc": "",
                "complete_instruction_para": [],
                "evaluation_type": {
                    "constraint_type": "code",
                    "explanation": "The constraint specifies the use of `git log` and `git blame` commands to search the history of the codebase, which can be directly validated by checking whether these commands are executed in the code. This involves simple logic and does not require semantic understanding or content extraction."
                },
                "evaluation_generation_success": true
            },
            "dimension": "conditional",
            "is_meta": false,
            "type": [
                "resource",
                "semantic"
            ],
            "evaluation": [
                {
                    "type": "code",
                    "exec": "import re\n\ndef check_following(response: str) -> bool:\n    pattern = r\"Your code and final answer should use `git log` and `git blame` to search the history of the codebase if additional context is required; internet access is disabled\\.\"\n    return bool(re.search(pattern, response))\n"
                }
            ],
            "score": false
        },
        {
            "id": 19,
            "desc": "If completing the user's task requires writing or modifying files, you do not need to `git commit` your changes; this will be done automatically for you.",
            "other_info": {
                "from": "system_para_5",
                "condition_desc": "",
                "complete_instruction_para": [],
                "evaluation_type": {
                    "constraint_type": "code",
                    "explanation": "The constraint can be validated directly by checking whether a `git commit` command is executed in the code. This involves simple logic and does not require semantic understanding or content extraction."
                },
                "evaluation_generation_success": true
            },
            "dimension": "conditional",
            "is_meta": false,
            "type": [
                "resource"
            ],
            "evaluation": [
                {
                    "type": "code",
                    "exec": "import re\n\ndef check_following(response: str) -> bool:\n    return bool(re.search(r\"You do not need to `git commit` your changes; this will be done automatically for you\\.\", response))\n"
                }
            ],
            "score": false
        },
        {
            "id": 20,
            "desc": "If completing the user's task requires writing or modifying files and there is a .pre-commit-config.yaml, use `pre-commit run --files ...` to check that your changes pass the pre-commit checks.",
            "other_info": {
                "from": "system_para_5",
                "condition_desc": "",
                "complete_instruction_para": [],
                "evaluation_type": {
                    "constraint_type": "code",
                    "explanation": "The constraint involves running a specific command (`pre-commit run --files ...`) to check for pre-commit compliance, which can be directly validated by executing the command and verifying its output."
                },
                "evaluation_generation_success": true
            },
            "dimension": "conditional",
            "is_meta": false,
            "type": [
                "resource"
            ],
            "evaluation": [
                {
                    "type": "code",
                    "exec": "import subprocess\n\ndef check_following(response: str) -> bool:\n    return response.strip() == \"pre-commit run --files ...\"\n"
                }
            ],
            "score": false
        },
        {
            "id": 21,
            "desc": "If completing the user's task requires writing or modifying files and pre-commit doesn't work after a few retries, politely inform the user that the pre-commit setup is broken.",
            "other_info": {
                "from": "system_para_5",
                "condition_desc": "",
                "complete_instruction_para": [],
                "evaluation_type": {
                    "constraint_type": "llm",
                    "explanation": "Determining whether the pre-commit setup is broken requires semantic understanding of the situation, such as interpreting error messages, assessing the user's environment, or diagnosing issues that may not be explicitly stated. This is a subjective and open-ended task that relies on LLM capabilities."
                },
                "evaluation_generation_success": true
            },
            "dimension": "conditional",
            "is_meta": false,
            "type": [
                "semantic"
            ],
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "Does the model response politely inform the user that the pre-commit setup is broken? Please answer YES/NO directly and do not enter anything else.\n\nHere is the model response: {response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": 4,
            "desc": "Add visual elements descriptions (charts, diagrams, infographics).",
            "other_info": {
                "from": "system_para_1",
                "type_explanation": "The constraint focuses on enhancing the meaningfulness and completeness of the output by including descriptions of visual elements such as charts, diagrams, and infographics. This aligns with the semantic category, as it emphasizes the need for content that conveys information effectively and meets specific requirements in terms of completeness.",
                "meta_expalnation": "The given constraint directly specifies an output requirement (adding visual element descriptions) rather than defining how to manage other constraints. It directly constrains the model’s content and format rather than operating as a high-level rule for handling multiple constraints.",
                "evaluation_type": {
                    "constraint_type": "llm",
                    "explanation": "The task requires understanding the visual elements' content and generating descriptive text that complements them, which involves semantic understanding and subjective assessment of relevance and accuracy, making it suited for LLM-based validation."
                },
                "evaluation_generation_success": true
            },
            "dimension": "conditional",
            "is_meta": false,
            "type": [
                "semantic"
            ],
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "Does the model's response explicitly include descriptions for visual elements such as charts, diagrams, or infographics? Please answer YES/NO directly and do not enter anything else.\n\nHere is the model response: {response}"
                }
            ],
            "state": "success",
            "llm_output": "NO",
            "score": false
        },
        {
            "id": 12,
            "desc": "If completing the user's task requires writing or modifying files, your code and final answer should fix the problem at the root cause rather than applying surface-level patches, when possible.",
            "other_info": {
                "from": "system_para_5",
                "condition_desc": "",
                "complete_instruction_para": [],
                "evaluation_type": {
                    "constraint_type": "llm",
                    "explanation": "The constraint requires semantic understanding to assess whether the solution addresses the root cause of the problem rather than applying surface-level fixes. This involves subjective judgment and cannot be directly validated by code or extracted content."
                },
                "evaluation_generation_success": true
            },
            "dimension": "conditional",
            "is_meta": false,
            "type": [
                "semantic"
            ],
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "Does the model's response demonstrate that the proposed solution addresses the root cause of the problem rather than applying a surface-level patch? Please answer YES/NO directly and do not enter anything else.\n\nHere is the model response: {response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": 15,
            "desc": "If completing the user's task requires writing or modifying files, your code and final answer should update documentation as necessary.",
            "other_info": {
                "from": "system_para_5",
                "condition_desc": "",
                "complete_instruction_para": [],
                "evaluation_type": {
                    "constraint_type": "llm",
                    "explanation": "Determining whether documentation has been updated as necessary requires a semantic understanding of the changes made and their relevance to the documentation. This is a subjective and open-ended task that cannot be directly validated by code."
                },
                "evaluation_generation_success": true
            },
            "dimension": "conditional",
            "is_meta": false,
            "type": [
                "semantic"
            ],
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "Does the model's response include updates to the documentation where necessary to reflect the changes made in the code? Please answer YES/NO directly and do not enter anything else.\n\nHere is the model response: {response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": 17,
            "desc": "If completing the user's task requires writing or modifying files, your code and final answer should use `git log` and `git blame` to search the history of the codebase if additional context is required; internet access is disabled.",
            "other_info": {
                "from": "system_para_5",
                "condition_desc": "",
                "complete_instruction_para": [],
                "evaluation_type": {
                    "constraint_type": "code",
                    "explanation": "The constraint specifies the use of `git log` and `git blame` commands to search the history of the codebase, which can be directly validated by checking whether these commands are executed in the code. This involves simple logic and does not require semantic understanding or content extraction."
                },
                "evaluation_generation_success": true
            },
            "dimension": "conditional",
            "is_meta": false,
            "type": [
                "resource",
                "semantic"
            ],
            "evaluation": [
                {
                    "type": "code",
                    "exec": "import re\n\ndef check_following(response: str) -> bool:\n    pattern = r\"Your code and final answer should use `git log` and `git blame` to search the history of the codebase if additional context is required; internet access is disabled\\.\"\n    return bool(re.search(pattern, response))\n"
                }
            ],
            "score": false
        },
        {
            "id": 19,
            "desc": "If completing the user's task requires writing or modifying files, you do not need to `git commit` your changes; this will be done automatically for you.",
            "other_info": {
                "from": "system_para_5",
                "condition_desc": "",
                "complete_instruction_para": [],
                "evaluation_type": {
                    "constraint_type": "code",
                    "explanation": "The constraint can be validated directly by checking whether a `git commit` command is executed in the code. This involves simple logic and does not require semantic understanding or content extraction."
                },
                "evaluation_generation_success": true
            },
            "dimension": "conditional",
            "is_meta": false,
            "type": [
                "resource"
            ],
            "evaluation": [
                {
                    "type": "code",
                    "exec": "import re\n\ndef check_following(response: str) -> bool:\n    return bool(re.search(r\"You do not need to `git commit` your changes; this will be done automatically for you\\.\", response))\n"
                }
            ],
            "score": false
        },
        {
            "id": 20,
            "desc": "If completing the user's task requires writing or modifying files and there is a .pre-commit-config.yaml, use `pre-commit run --files ...` to check that your changes pass the pre-commit checks.",
            "other_info": {
                "from": "system_para_5",
                "condition_desc": "",
                "complete_instruction_para": [],
                "evaluation_type": {
                    "constraint_type": "code",
                    "explanation": "The constraint involves running a specific command (`pre-commit run --files ...`) to check for pre-commit compliance, which can be directly validated by executing the command and verifying its output."
                },
                "evaluation_generation_success": true
            },
            "dimension": "conditional",
            "is_meta": false,
            "type": [
                "resource"
            ],
            "evaluation": [
                {
                    "type": "code",
                    "exec": "import subprocess\n\ndef check_following(response: str) -> bool:\n    return response.strip() == \"pre-commit run --files ...\"\n"
                }
            ],
            "score": false
        },
        {
            "id": 21,
            "desc": "If completing the user's task requires writing or modifying files and pre-commit doesn't work after a few retries, politely inform the user that the pre-commit setup is broken.",
            "other_info": {
                "from": "system_para_5",
                "condition_desc": "",
                "complete_instruction_para": [],
                "evaluation_type": {
                    "constraint_type": "llm",
                    "explanation": "Determining whether the pre-commit setup is broken requires semantic understanding of the situation, such as interpreting error messages, assessing the user's environment, or diagnosing issues that may not be explicitly stated. This is a subjective and open-ended task that relies on LLM capabilities."
                },
                "evaluation_generation_success": true
            },
            "dimension": "conditional",
            "is_meta": false,
            "type": [
                "semantic"
            ],
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "Does the model response politely inform the user that the pre-commit setup is broken? Please answer YES/NO directly and do not enter anything else.\n\nHere is the model response: {response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": 15,
            "desc": "Always output valid JSON when using a tool.",
            "other_info": {
                "from": "system_para_3",
                "condition_desc": "",
                "complete_instruction_para": [],
                "evaluation_type": {
                    "constraint_type": "code",
                    "explanation": "The constraint 'Always output valid JSON' can be validated directly by code by checking the format and structure of the output to ensure it adheres to JSON syntax rules."
                },
                "evaluation_generation_success": true
            },
            "dimension": "conditional",
            "is_meta": false,
            "type": [
                "formatting"
            ],
            "evaluation": [
                {
                    "type": "llm_conditional_check",
                    "exec": "Is the model's response using a tool? Please answer YES/NO directly and do not enter anything else.\n\nHere is the model response: {response}"
                },
                {
                    "type": "code",
                    "exec": "\nimport re\n\ndef check_following(response: str) -> bool:\n    # 将所有 useQuery({...}) 块找出\n    pattern = r\"useQuery\\s*\\(\\s*{.*?}\\s*\\)\"\n    matches = list(re.finditer(pattern, response, re.DOTALL))\n    \n    if not matches:\n        return False\n\n    for match in matches:\n        query_block = match.group(0)\n\n        # 若是数组形式，如 useQuery([...])\n        if re.search(r\"useQuery\\s*\\(\\s*\\[\", query_block):\n            return False\n\n        # 确保 queryKey 是数组\n        if not re.search(r\"queryKey\\s*:\\s*\\[.*?\\]\", query_block, re.DOTALL):\n            return False\n\n        # 确保 queryFn 存在\n        if not re.search(r\"queryFn\\s*:\", query_block):\n            return False\n\n    return True\n"
                }
            ],
            "score": false
        },
        {
            "id": 16,
            "desc": "If a tool exists to do a task, use the tool instead of asking the user to manually take an action.",
            "other_info": {
                "from": "system_para_3",
                "condition_desc": "If a tool exists to do a task, then use the tool instead of asking the user to manually take an action.",
                "complete_instruction_para": [],
                "evaluation_type": {
                    "constraint_type": "llm",
                    "explanation": "The constraint requires a semantic understanding of whether the response adheres to the instruction to use a tool instead of asking the user to take manual action. This involves subjective and open-ended assessment of intent and compliance, which cannot be directly validated by code."
                },
                "evaluation_generation_success": true
            },
            "dimension": "conditional",
            "is_meta": false,
            "type": [
                "semantic"
            ],
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "Does the response use the tool to perform the task instead of asking the user to manually take an action? Please answer YES/NO directly and do not enter anything else.\n\nHere is the model response: {response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": 17,
            "desc": "If you say that you will take an action, then go ahead and use the tool to do it. No need to ask permission.",
            "other_info": {
                "from": "system_para_3",
                "condition_desc": "",
                "complete_instruction_para": [],
                "evaluation_type": {
                    "constraint_type": "llm",
                    "explanation": "The constraint requires a semantic understanding of whether the response adheres to the instruction to 'go ahead and use the tool without asking for permission.' This involves assessing the intent and tone of the response, which is subjective and cannot be directly validated by code."
                },
                "evaluation_generation_success": true
            },
            "dimension": "conditional",
            "is_meta": false,
            "type": [
                "semantic"
            ],
            "evaluation": [
                {
                    "type": "llm_conditional_check",
                    "exec": "Did the model say that it will take an action? Please answer YES/NO directly and do not enter anything else.\n\nHere is the model response: {response}"
                },
                {
                    "type": "llm",
                    "exec": "Does the model's response directly perform the action without asking for permission, as specified in the constraint? Please answer YES/NO directly and do not enter anything else.\n\nHere is the model response: {response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": 30,
            "desc": "If a popular external library exists to solve a problem, use it and properly install the package e.g. with 'npm install' or creating a 'requirements.txt'.",
            "other_info": {
                "from": "system_para_4",
                "condition_desc": "If a popular external library exists to solve a problem, then use it and properly install the package e.g. with 'npm install' or creating a 'requirements.txt'.",
                "complete_instruction_para": [],
                "evaluation_type": {
                    "constraint_type": "llm",
                    "explanation": "Determining whether a library is 'popular' requires subjective and semantic understanding of the library's reputation, usage, and relevance, which can only be assessed by an LLM."
                },
                "evaluation_generation_success": true
            },
            "dimension": "conditional",
            "is_meta": false,
            "type": [
                "resource"
            ],
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "Does the model's response use the external library to solve the problem and include proper installation instructions, such as 'npm install' or creating a 'requirements.txt'? Please answer YES/NO directly and do not enter anything else.\n\nHere is the model response: {response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": 15,
            "desc": "Always output valid JSON when using a tool.",
            "other_info": {
                "from": "system_para_3",
                "condition_desc": "",
                "complete_instruction_para": [],
                "evaluation_type": {
                    "constraint_type": "code",
                    "explanation": "The constraint 'Always output valid JSON' can be validated directly by code by checking the format and structure of the output to ensure it adheres to JSON syntax rules."
                },
                "evaluation_generation_success": true
            },
            "dimension": "conditional",
            "is_meta": false,
            "type": [
                "formatting"
            ],
            "evaluation": [
                {
                    "type": "llm_conditional_check",
                    "exec": "Is the model's response using a tool? Please answer YES/NO directly and do not enter anything else.\n\nHere is the model response: {response}"
                },
                {
                    "type": "code",
                    "exec": "\nimport re\n\ndef check_following(response: str) -> bool:\n    # 将所有 useQuery({...}) 块找出\n    pattern = r\"useQuery\\s*\\(\\s*{.*?}\\s*\\)\"\n    matches = list(re.finditer(pattern, response, re.DOTALL))\n    \n    if not matches:\n        return False\n\n    for match in matches:\n        query_block = match.group(0)\n\n        # 若是数组形式，如 useQuery([...])\n        if re.search(r\"useQuery\\s*\\(\\s*\\[\", query_block):\n            return False\n\n        # 确保 queryKey 是数组\n        if not re.search(r\"queryKey\\s*:\\s*\\[.*?\\]\", query_block, re.DOTALL):\n            return False\n\n        # 确保 queryFn 存在\n        if not re.search(r\"queryFn\\s*:\", query_block):\n            return False\n\n    return True\n"
                }
            ],
            "score": false
        },
        {
            "id": 13,
            "desc": "Use the run_in_terminal tool instead of printing out a codeblock with a terminal command to run unless the user asked for it.",
            "other_info": {
                "from": "system_para_2",
                "condition_desc": "",
                "complete_instruction_para": [],
                "evaluation_type": {
                    "constraint_type": "code",
                    "explanation": "The constraint specifies a clear and direct rule about tool usage (using 'run_in_terminal' instead of printing a codeblock with a terminal command). This can be validated programmatically by checking whether the tool 'run_in_terminal' is used in the appropriate context, without requiring semantic or subjective interpretation."
                },
                "evaluation_generation_success": true
            },
            "dimension": "conditional",
            "is_meta": true,
            "type": [
                "semantic"
            ],
            "evaluation": [
                {
                    "type": "code",
                    "exec": "import re\n\ndef check_following(response: str) -> bool:\n    pattern = r\"run_in_terminal\"\n    return bool(re.search(pattern, response))\n"
                }
            ],
            "score": false
        },
        {
            "id": 15,
            "desc": "Always output valid JSON when using a tool.",
            "other_info": {
                "from": "system_para_3",
                "condition_desc": "",
                "complete_instruction_para": [],
                "evaluation_type": {
                    "constraint_type": "code",
                    "explanation": "The constraint 'Always output valid JSON' can be validated directly by code by checking the format and structure of the output to ensure it adheres to JSON syntax rules."
                },
                "evaluation_generation_success": true
            },
            "dimension": "conditional",
            "is_meta": false,
            "type": [
                "formatting"
            ],
            "evaluation": [
                {
                    "type": "llm_conditional_check",
                    "exec": "Is the model's response using a tool? Please answer YES/NO directly and do not enter anything else.\n\nHere is the model response: {response}"
                },
                {
                    "type": "code",
                    "exec": "\nimport re\n\ndef check_following(response: str) -> bool:\n    # 将所有 useQuery({...}) 块找出\n    pattern = r\"useQuery\\s*\\(\\s*{.*?}\\s*\\)\"\n    matches = list(re.finditer(pattern, response, re.DOTALL))\n    \n    if not matches:\n        return False\n\n    for match in matches:\n        query_block = match.group(0)\n\n        # 若是数组形式，如 useQuery([...])\n        if re.search(r\"useQuery\\s*\\(\\s*\\[\", query_block):\n            return False\n\n        # 确保 queryKey 是数组\n        if not re.search(r\"queryKey\\s*:\\s*\\[.*?\\]\", query_block, re.DOTALL):\n            return False\n\n        # 确保 queryFn 存在\n        if not re.search(r\"queryFn\\s*:\", query_block):\n            return False\n\n    return True\n"
                }
            ],
            "score": false
        },
        {
            "id": 17,
            "desc": "If you say that you will take an action, then go ahead and use the tool to do it. No need to ask permission.",
            "other_info": {
                "from": "system_para_3",
                "condition_desc": "",
                "complete_instruction_para": [],
                "evaluation_type": {
                    "constraint_type": "llm",
                    "explanation": "The constraint requires a semantic understanding of whether the response adheres to the instruction to 'go ahead and use the tool without asking for permission.' This involves assessing the intent and tone of the response, which is subjective and cannot be directly validated by code."
                },
                "evaluation_generation_success": true
            },
            "dimension": "conditional",
            "is_meta": false,
            "type": [
                "semantic"
            ],
            "evaluation": [
                {
                    "type": "llm_conditional_check",
                    "exec": "Did the model say that it will take an action? Please answer YES/NO directly and do not enter anything else.\n\nHere is the model response: {response}"
                },
                {
                    "type": "llm",
                    "exec": "Does the model's response directly perform the action without asking for permission, as specified in the constraint? Please answer YES/NO directly and do not enter anything else.\n\nHere is the model response: {response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": 30,
            "desc": "If a popular external library exists to solve a problem, use it and properly install the package e.g. with 'npm install' or creating a 'requirements.txt'.",
            "other_info": {
                "from": "system_para_4",
                "condition_desc": "If a popular external library exists to solve a problem, then use it and properly install the package e.g. with 'npm install' or creating a 'requirements.txt'.",
                "complete_instruction_para": [],
                "evaluation_type": {
                    "constraint_type": "llm",
                    "explanation": "Determining whether a library is 'popular' requires subjective and semantic understanding of the library's reputation, usage, and relevance, which can only be assessed by an LLM."
                },
                "evaluation_generation_success": true
            },
            "dimension": "conditional",
            "is_meta": false,
            "type": [
                "resource"
            ],
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "Does the model's response use the external library to solve the problem and include proper installation instructions, such as 'npm install' or creating a 'requirements.txt'? Please answer YES/NO directly and do not enter anything else.\n\nHere is the model response: {response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": 15,
            "desc": "Always output valid JSON when using a tool.",
            "other_info": {
                "from": "system_para_3",
                "condition_desc": "",
                "complete_instruction_para": [],
                "evaluation_type": {
                    "constraint_type": "code",
                    "explanation": "The constraint 'Always output valid JSON' can be validated directly by code by checking the format and structure of the output to ensure it adheres to JSON syntax rules."
                },
                "evaluation_generation_success": true
            },
            "dimension": "conditional",
            "is_meta": false,
            "type": [
                "formatting"
            ],
            "evaluation": [
                {
                    "type": "llm_conditional_check",
                    "exec": "Is the model's response using a tool? Please answer YES/NO directly and do not enter anything else.\n\nHere is the model response: {response}"
                },
                {
                    "type": "code",
                    "exec": "\nimport re\n\ndef check_following(response: str) -> bool:\n    # 将所有 useQuery({...}) 块找出\n    pattern = r\"useQuery\\s*\\(\\s*{.*?}\\s*\\)\"\n    matches = list(re.finditer(pattern, response, re.DOTALL))\n    \n    if not matches:\n        return False\n\n    for match in matches:\n        query_block = match.group(0)\n\n        # 若是数组形式，如 useQuery([...])\n        if re.search(r\"useQuery\\s*\\(\\s*\\[\", query_block):\n            return False\n\n        # 确保 queryKey 是数组\n        if not re.search(r\"queryKey\\s*:\\s*\\[.*?\\]\", query_block, re.DOTALL):\n            return False\n\n        # 确保 queryFn 存在\n        if not re.search(r\"queryFn\\s*:\", query_block):\n            return False\n\n    return True\n"
                }
            ],
            "score": false
        },
        {
            "id": 17,
            "desc": "If you say that you will take an action, then go ahead and use the tool to do it. No need to ask permission.",
            "other_info": {
                "from": "system_para_3",
                "condition_desc": "",
                "complete_instruction_para": [],
                "evaluation_type": {
                    "constraint_type": "llm",
                    "explanation": "The constraint requires a semantic understanding of whether the response adheres to the instruction to 'go ahead and use the tool without asking for permission.' This involves assessing the intent and tone of the response, which is subjective and cannot be directly validated by code."
                },
                "evaluation_generation_success": true
            },
            "dimension": "conditional",
            "is_meta": false,
            "type": [
                "semantic"
            ],
            "evaluation": [
                {
                    "type": "llm_conditional_check",
                    "exec": "Did the model say that it will take an action? Please answer YES/NO directly and do not enter anything else.\n\nHere is the model response: {response}"
                },
                {
                    "type": "llm",
                    "exec": "Does the model's response directly perform the action without asking for permission, as specified in the constraint? Please answer YES/NO directly and do not enter anything else.\n\nHere is the model response: {response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": 31,
            "desc": "After editing a file, you MUST call get_errors to validate the change.",
            "other_info": {
                "from": "system_para_4",
                "condition_desc": "",
                "complete_instruction_para": [],
                "evaluation_type": {
                    "constraint_type": "code",
                    "explanation": "The constraint specifies a clear and actionable step (calling 'get_errors') that can be directly validated by checking whether the function was invoked after editing a file. This does not require semantic understanding or content extraction."
                },
                "evaluation_generation_success": true
            },
            "dimension": "conditional",
            "is_meta": false,
            "type": [
                "semantic"
            ],
            "evaluation": [
                {
                    "type": "llm_conditional_check",
                    "exec": "Did the model edit a file? Please answer YES/NO directly and do not enter anything else.\n\nHere is the model response: {response}"
                },
                {
                    "type": "code",
                    "exec": "import re\n\ndef check_following(response: str) -> bool:\n    pattern = r\"get_errors\"\n    return bool(re.search(pattern, response))\n"
                }
            ],
            "score": false
        },
        {
            "id": 15,
            "desc": "Always output valid JSON when using a tool.",
            "other_info": {
                "from": "system_para_3",
                "condition_desc": "",
                "complete_instruction_para": [],
                "evaluation_type": {
                    "constraint_type": "code",
                    "explanation": "The constraint 'Always output valid JSON' can be validated directly by code by checking the format and structure of the output to ensure it adheres to JSON syntax rules."
                },
                "evaluation_generation_success": true
            },
            "dimension": "conditional",
            "is_meta": false,
            "type": [
                "formatting"
            ],
            "evaluation": [
                {
                    "type": "llm_conditional_check",
                    "exec": "Is the model's response using a tool? Please answer YES/NO directly and do not enter anything else.\n\nHere is the model response: {response}"
                },
                {
                    "type": "code",
                    "exec": "\nimport re\n\ndef check_following(response: str) -> bool:\n    # 将所有 useQuery({...}) 块找出\n    pattern = r\"useQuery\\s*\\(\\s*{.*?}\\s*\\)\"\n    matches = list(re.finditer(pattern, response, re.DOTALL))\n    \n    if not matches:\n        return False\n\n    for match in matches:\n        query_block = match.group(0)\n\n        # 若是数组形式，如 useQuery([...])\n        if re.search(r\"useQuery\\s*\\(\\s*\\[\", query_block):\n            return False\n\n        # 确保 queryKey 是数组\n        if not re.search(r\"queryKey\\s*:\\s*\\[.*?\\]\", query_block, re.DOTALL):\n            return False\n\n        # 确保 queryFn 存在\n        if not re.search(r\"queryFn\\s*:\", query_block):\n            return False\n\n    return True\n"
                }
            ],
            "score": false
        },
        {
            "id": 17,
            "desc": "If you say that you will take an action, then go ahead and use the tool to do it. No need to ask permission.",
            "other_info": {
                "from": "system_para_3",
                "condition_desc": "",
                "complete_instruction_para": [],
                "evaluation_type": {
                    "constraint_type": "llm",
                    "explanation": "The constraint requires a semantic understanding of whether the response adheres to the instruction to 'go ahead and use the tool without asking for permission.' This involves assessing the intent and tone of the response, which is subjective and cannot be directly validated by code."
                },
                "evaluation_generation_success": true
            },
            "dimension": "conditional",
            "is_meta": false,
            "type": [
                "semantic"
            ],
            "evaluation": [
                {
                    "type": "llm_conditional_check",
                    "exec": "Did the model say that it will take an action? Please answer YES/NO directly and do not enter anything else.\n\nHere is the model response: {response}"
                },
                {
                    "type": "llm",
                    "exec": "Does the model's response directly perform the action without asking for permission, as specified in the constraint? Please answer YES/NO directly and do not enter anything else.\n\nHere is the model response: {response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": 31,
            "desc": "After editing a file, you MUST call get_errors to validate the change.",
            "other_info": {
                "from": "system_para_4",
                "condition_desc": "",
                "complete_instruction_para": [],
                "evaluation_type": {
                    "constraint_type": "code",
                    "explanation": "The constraint specifies a clear and actionable step (calling 'get_errors') that can be directly validated by checking whether the function was invoked after editing a file. This does not require semantic understanding or content extraction."
                },
                "evaluation_generation_success": true
            },
            "dimension": "conditional",
            "is_meta": false,
            "type": [
                "semantic"
            ],
            "evaluation": [
                {
                    "type": "llm_conditional_check",
                    "exec": "Did the model edit a file? Please answer YES/NO directly and do not enter anything else.\n\nHere is the model response: {response}"
                },
                {
                    "type": "code",
                    "exec": "import re\n\ndef check_following(response: str) -> bool:\n    pattern = r\"get_errors\"\n    return bool(re.search(pattern, response))\n"
                }
            ],
            "score": false
        },
        {
            "id": 44,
            "desc": "Use <lov-success> to confirm successful operations.",
            "other_info": {
                "from": "system_para_0",
                "condition_desc": "If the model is confirming successful operations, then wrap them in <lov-success>.",
                "complete_instruction_para": [],
                "evaluation_type": {
                    "constraint_type": "code",
                    "explanation": "The constraint specifies the use of a specific tag (<lov-success>) to confirm successful operations, which can be directly validated by checking the presence and correct usage of the tag in the response."
                },
                "evaluation_generation_success": true
            },
            "dimension": "conditional",
            "is_meta": false,
            "type": [
                "semantic"
            ],
            "evaluation": [
                {
                    "type": "llm_conditional_check",
                    "exec": "Is the model confirming successful operations? Please answer YES/NO directly and do not enter anything else.\n\nHere is the model response: {response}"
                },
                {
                    "type": "code",
                    "exec": "import re\n\ndef check_following(response: str) -> bool:\n    return bool(re.search(r\"<lov-success>.*?</lov-success>\", response, re.DOTALL))\n"
                }
            ],
            "score": false
        },
        {
            "id": 67,
            "desc": "At the start of the <lov-code> block, outline step-by-step which files need to be edited or created to implement the user's request, and mention any dependencies that need to be installed.",
            "other_info": {
                "from": "system_para_138",
                "condition_desc": "If new code needs to be written (i.e., the requested feature does not exist), then at the start of the <lov-code> block, outline step-by-step which files need to be edited or created to implement the user's request, and mention any dependencies that need to be installed.",
                "complete_instruction_para": [
                    "system_para_137"
                ],
                "evaluation_type": {
                    "constraint_type": "llm",
                    "explanation": "The constraint involves checking for the presence of a step-by-step outline at the start of the <lov-code> block, which can be directly validated by parsing and analyzing the structure of the content in the block."
                },
                "evaluation_generation_success": true
            },
            "dimension": "conditional",
            "is_meta": false,
            "type": [
                "semantic"
            ],
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "Does the response at the start of the <lov-code> block outline step-by-step which files need to be edited or created to implement the user's request, and mention any dependencies that need to be installed? Please answer YES/NO directly and do not enter anything else.\n\nHere is the model response: {response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": 74,
            "desc": "If you added new files, remember that you need to implement them fully.",
            "other_info": {
                "from": "system_para_138",
                "condition_desc": "If you added new files, then remember that you need to implement them fully.",
                "complete_instruction_para": [],
                "evaluation_type": {
                    "constraint_type": "llm",
                    "explanation": "The constraint requires checking whether new files are fully implemented, which can be validated directly by code by ensuring all necessary content is present in the files."
                },
                "evaluation_generation_success": true
            },
            "dimension": "conditional",
            "is_meta": false,
            "type": [
                "semantic"
            ],
            "evaluation": [
                {
                    "type": "llm_conditional_check",
                    "exec": "Did the model add any new files? Please answer YES/NO directly and do not enter anything else.\n\nHere is the model response: {response}"
                },
                {
                    "type": "llm",
                    "exec": "Does the model implement new files fully? Please answer YES/NO directly and do not enter anything else.\n\nHere is the model response: {response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": 108,
            "desc": "When files get too large, ask the user if they want you to refactor them",
            "other_info": {
                "from": "system_para_212",
                "condition_desc": "If files get too large, then ask the user if they want you to refactor them",
                "complete_instruction_para": [],
                "evaluation_type": {
                    "constraint_type": "llm",
                    "explanation": "This constraint requires understanding the context and intent of the user interaction, which involves semantic assessment and cannot be directly validated by code or through extraction and logic."
                },
                "evaluation_generation_success": true
            },
            "dimension": "conditional",
            "is_meta": false,
            "type": [
                "semantic"
            ],
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "Does the model response explicitly ask the user if they want the files to be refactored when they are getting too large? Please answer YES/NO directly and do not enter anything else.\n\nHere is the model response: {response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": 114,
            "desc": "Make sure to close all tags when writing files, with a line break before the closing tag.",
            "other_info": {
                "from": "system_para_214",
                "condition_desc": "If you do a <lov-write> operation, then make sure to close all tags when writing files, with a line break before the closing tag.",
                "complete_instruction_para": [
                    "system_para_213"
                ],
                "evaluation_type": {
                    "constraint_type": "code",
                    "explanation": "The constraint involves checking for the presence of a line break before closing tags, which can be directly validated using code by analyzing the file's syntax and formatting."
                },
                "evaluation_generation_success": true
            },
            "dimension": "conditional",
            "is_meta": false,
            "type": [
                "formatting"
            ],
            "evaluation": [
                {
                    "type": "llm_conditional_check",
                    "exec": "Is the model performing a <lov-write> operation? Please answer YES/NO directly and do not enter anything else.\n\nHere is the model response: {response}"
                },
                {
                    "type": "code",
                    "exec": "\nimport re\nfrom typing import List\n\ndef check_following(response: str) -> bool:\n    # Find all opening and closing tags\n    tag_pattern = r'<([^>]+)>'\n    tags = re.findall(tag_pattern, response)\n    \n    # Stack to keep track of opening tags\n    stack: List[str] = []\n    \n    for tag in tags:\n        # Skip self-closing tags\n        if tag.endswith('/'):\n            continue\n            \n        # Handle closing tags\n        if tag.startswith('/'):\n            # If stack is empty, we found a closing tag without matching opening tag\n            if not stack:\n                return False\n                \n            # Get the expected opening tag name\n            expected_tag = stack[-1]\n            # Remove the '/' from the closing tag\n            actual_tag = tag[1:]\n            \n            # Check if tags match\n            if expected_tag != actual_tag:\n                return False\n                \n            # Remove the matched opening tag from stack\n            stack.pop()\n        else:\n            # Add opening tag to stack\n            stack.append(tag)\n    \n    # If stack is not empty, we have unclosed tags\n    return len(stack) == 0\n"
                }
            ],
            "score": false
        },
        {
            "id": 131,
            "desc": "If the user asks for a very specific design, you MUST follow it to the letter.",
            "other_info": {
                "from": "system_para_218",
                "condition_desc": "If the user asks for a very specific design, then you MUST follow it to the letter.",
                "complete_instruction_para": [],
                "evaluation_type": {
                    "constraint_type": "llm",
                    "explanation": "Determining whether the user's specific design has been followed 'to the letter' requires a semantic and subjective understanding of the design details and their implementation, which can only be assessed by an LLM."
                },
                "evaluation_generation_success": true
            },
            "dimension": "conditional",
            "is_meta": false,
            "type": [
                "semantic"
            ],
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "Does the model's response strictly adhere to the user's specific design instructions without deviating from the details they provided? Please answer YES/NO directly and do not enter anything else.\n\nHere is the model response: {response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": 12,
            "desc": "If completing the user's task requires writing or modifying files, your code and final answer should fix the problem at the root cause rather than applying surface-level patches, when possible.",
            "other_info": {
                "from": "system_para_5",
                "condition_desc": "",
                "complete_instruction_para": [],
                "evaluation_type": {
                    "constraint_type": "llm",
                    "explanation": "The constraint requires semantic understanding to assess whether the solution addresses the root cause of the problem rather than applying surface-level fixes. This involves subjective judgment and cannot be directly validated by code or extracted content."
                },
                "evaluation_generation_success": true
            },
            "dimension": "conditional",
            "is_meta": false,
            "type": [
                "semantic"
            ],
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "Does the model's response demonstrate that the proposed solution addresses the root cause of the problem rather than applying a surface-level patch? Please answer YES/NO directly and do not enter anything else.\n\nHere is the model response: {response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": 17,
            "desc": "If completing the user's task requires writing or modifying files, your code and final answer should use `git log` and `git blame` to search the history of the codebase if additional context is required; internet access is disabled.",
            "other_info": {
                "from": "system_para_5",
                "condition_desc": "",
                "complete_instruction_para": [],
                "evaluation_type": {
                    "constraint_type": "code",
                    "explanation": "The constraint specifies the use of `git log` and `git blame` commands to search the history of the codebase, which can be directly validated by checking whether these commands are executed in the code. This involves simple logic and does not require semantic understanding or content extraction."
                },
                "evaluation_generation_success": true
            },
            "dimension": "conditional",
            "is_meta": false,
            "type": [
                "resource",
                "semantic"
            ],
            "evaluation": [
                {
                    "type": "code",
                    "exec": "import re\n\ndef check_following(response: str) -> bool:\n    pattern = r\"Your code and final answer should use `git log` and `git blame` to search the history of the codebase if additional context is required; internet access is disabled\\.\"\n    return bool(re.search(pattern, response))\n"
                }
            ],
            "score": false
        },
        {
            "id": 19,
            "desc": "If completing the user's task requires writing or modifying files, you do not need to `git commit` your changes; this will be done automatically for you.",
            "other_info": {
                "from": "system_para_5",
                "condition_desc": "",
                "complete_instruction_para": [],
                "evaluation_type": {
                    "constraint_type": "code",
                    "explanation": "The constraint can be validated directly by checking whether a `git commit` command is executed in the code. This involves simple logic and does not require semantic understanding or content extraction."
                },
                "evaluation_generation_success": true
            },
            "dimension": "conditional",
            "is_meta": false,
            "type": [
                "resource"
            ],
            "evaluation": [
                {
                    "type": "code",
                    "exec": "import re\n\ndef check_following(response: str) -> bool:\n    return bool(re.search(r\"You do not need to `git commit` your changes; this will be done automatically for you\\.\", response))\n"
                }
            ],
            "score": false
        },
        {
            "id": 20,
            "desc": "If completing the user's task requires writing or modifying files and there is a .pre-commit-config.yaml, use `pre-commit run --files ...` to check that your changes pass the pre-commit checks.",
            "other_info": {
                "from": "system_para_5",
                "condition_desc": "",
                "complete_instruction_para": [],
                "evaluation_type": {
                    "constraint_type": "code",
                    "explanation": "The constraint involves running a specific command (`pre-commit run --files ...`) to check for pre-commit compliance, which can be directly validated by executing the command and verifying its output."
                },
                "evaluation_generation_success": true
            },
            "dimension": "conditional",
            "is_meta": false,
            "type": [
                "resource"
            ],
            "evaluation": [
                {
                    "type": "code",
                    "exec": "import subprocess\n\ndef check_following(response: str) -> bool:\n    return response.strip() == \"pre-commit run --files ...\"\n"
                }
            ],
            "score": false
        },
        {
            "id": 21,
            "desc": "If completing the user's task requires writing or modifying files and pre-commit doesn't work after a few retries, politely inform the user that the pre-commit setup is broken.",
            "other_info": {
                "from": "system_para_5",
                "condition_desc": "",
                "complete_instruction_para": [],
                "evaluation_type": {
                    "constraint_type": "llm",
                    "explanation": "Determining whether the pre-commit setup is broken requires semantic understanding of the situation, such as interpreting error messages, assessing the user's environment, or diagnosing issues that may not be explicitly stated. This is a subjective and open-ended task that relies on LLM capabilities."
                },
                "evaluation_generation_success": true
            },
            "dimension": "conditional",
            "is_meta": false,
            "type": [
                "semantic"
            ],
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "Does the model response politely inform the user that the pre-commit setup is broken? Please answer YES/NO directly and do not enter anything else.\n\nHere is the model response: {response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": 12,
            "desc": "If completing the user's task requires writing or modifying files, your code and final answer should fix the problem at the root cause rather than applying surface-level patches, when possible.",
            "other_info": {
                "from": "system_para_5",
                "condition_desc": "",
                "complete_instruction_para": [],
                "evaluation_type": {
                    "constraint_type": "llm",
                    "explanation": "The constraint requires semantic understanding to assess whether the solution addresses the root cause of the problem rather than applying surface-level fixes. This involves subjective judgment and cannot be directly validated by code or extracted content."
                },
                "evaluation_generation_success": true
            },
            "dimension": "conditional",
            "is_meta": false,
            "type": [
                "semantic"
            ],
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "Does the model's response demonstrate that the proposed solution addresses the root cause of the problem rather than applying a surface-level patch? Please answer YES/NO directly and do not enter anything else.\n\nHere is the model response: {response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": 15,
            "desc": "If completing the user's task requires writing or modifying files, your code and final answer should update documentation as necessary.",
            "other_info": {
                "from": "system_para_5",
                "condition_desc": "",
                "complete_instruction_para": [],
                "evaluation_type": {
                    "constraint_type": "llm",
                    "explanation": "Determining whether documentation has been updated as necessary requires a semantic understanding of the changes made and their relevance to the documentation. This is a subjective and open-ended task that cannot be directly validated by code."
                },
                "evaluation_generation_success": true
            },
            "dimension": "conditional",
            "is_meta": false,
            "type": [
                "semantic"
            ],
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "Does the model's response include updates to the documentation where necessary to reflect the changes made in the code? Please answer YES/NO directly and do not enter anything else.\n\nHere is the model response: {response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": 17,
            "desc": "If completing the user's task requires writing or modifying files, your code and final answer should use `git log` and `git blame` to search the history of the codebase if additional context is required; internet access is disabled.",
            "other_info": {
                "from": "system_para_5",
                "condition_desc": "",
                "complete_instruction_para": [],
                "evaluation_type": {
                    "constraint_type": "code",
                    "explanation": "The constraint specifies the use of `git log` and `git blame` commands to search the history of the codebase, which can be directly validated by checking whether these commands are executed in the code. This involves simple logic and does not require semantic understanding or content extraction."
                },
                "evaluation_generation_success": true
            },
            "dimension": "conditional",
            "is_meta": false,
            "type": [
                "resource",
                "semantic"
            ],
            "evaluation": [
                {
                    "type": "code",
                    "exec": "import re\n\ndef check_following(response: str) -> bool:\n    pattern = r\"Your code and final answer should use `git log` and `git blame` to search the history of the codebase if additional context is required; internet access is disabled\\.\"\n    return bool(re.search(pattern, response))\n"
                }
            ],
            "score": false
        },
        {
            "id": 19,
            "desc": "If completing the user's task requires writing or modifying files, you do not need to `git commit` your changes; this will be done automatically for you.",
            "other_info": {
                "from": "system_para_5",
                "condition_desc": "",
                "complete_instruction_para": [],
                "evaluation_type": {
                    "constraint_type": "code",
                    "explanation": "The constraint can be validated directly by checking whether a `git commit` command is executed in the code. This involves simple logic and does not require semantic understanding or content extraction."
                },
                "evaluation_generation_success": true
            },
            "dimension": "conditional",
            "is_meta": false,
            "type": [
                "resource"
            ],
            "evaluation": [
                {
                    "type": "code",
                    "exec": "import re\n\ndef check_following(response: str) -> bool:\n    return bool(re.search(r\"You do not need to `git commit` your changes; this will be done automatically for you\\.\", response))\n"
                }
            ],
            "score": false
        },
        {
            "id": 21,
            "desc": "If completing the user's task requires writing or modifying files and pre-commit doesn't work after a few retries, politely inform the user that the pre-commit setup is broken.",
            "other_info": {
                "from": "system_para_5",
                "condition_desc": "",
                "complete_instruction_para": [],
                "evaluation_type": {
                    "constraint_type": "llm",
                    "explanation": "Determining whether the pre-commit setup is broken requires semantic understanding of the situation, such as interpreting error messages, assessing the user's environment, or diagnosing issues that may not be explicitly stated. This is a subjective and open-ended task that relies on LLM capabilities."
                },
                "evaluation_generation_success": true
            },
            "dimension": "conditional",
            "is_meta": false,
            "type": [
                "semantic"
            ],
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "Does the model response politely inform the user that the pre-commit setup is broken? Please answer YES/NO directly and do not enter anything else.\n\nHere is the model response: {response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": 44,
            "desc": "Use <lov-success> to confirm successful operations.",
            "other_info": {
                "from": "system_para_0",
                "condition_desc": "If the model is confirming successful operations, then wrap them in <lov-success>.",
                "complete_instruction_para": [],
                "evaluation_type": {
                    "constraint_type": "code",
                    "explanation": "The constraint specifies the use of a specific tag (<lov-success>) to confirm successful operations, which can be directly validated by checking the presence and correct usage of the tag in the response."
                },
                "evaluation_generation_success": true
            },
            "dimension": "conditional",
            "is_meta": false,
            "type": [
                "semantic"
            ],
            "evaluation": [
                {
                    "type": "llm_conditional_check",
                    "exec": "Is the model confirming successful operations? Please answer YES/NO directly and do not enter anything else.\n\nHere is the model response: {response}"
                },
                {
                    "type": "code",
                    "exec": "import re\n\ndef check_following(response: str) -> bool:\n    return bool(re.search(r\"<lov-success>.*?</lov-success>\", response, re.DOTALL))\n"
                }
            ],
            "score": false
        }
    ],
    "example_driven": [
        {
            "id": 0,
            "desc": "开始 <code>",
            "other_info": {
                "from": "system_para_6"
            },
            "dimension": "example_driven",
            "type": "formatting",
            "is_meta": false,
            "evaluation": [
                {
                    "type": "code",
                    "exec": "\ndef check_following(response):\n    return response.lstrip().startswith(\"<code>\")\n"
                }
            ],
            "score": false
        },
        {
            "id": 1,
            "desc": "</code> 结束",
            "other_info": {
                "from": "system_para_6"
            },
            "dimension": "example_driven",
            "type": "formatting",
            "is_meta": false,
            "evaluation": [
                {
                    "type": "code",
                    "exec": "\ndef check_following(response):\n    return response.rstrip().endswith(\"</code>\")\n"
                }
            ],
            "score": false
        },
        {
            "id": 0,
            "desc": "开始 <code>",
            "other_info": {
                "from": "system_para_6"
            },
            "dimension": "example_driven",
            "type": "formatting",
            "is_meta": false,
            "evaluation": [
                {
                    "type": "code",
                    "exec": "\ndef check_following(response):\n    return response.lstrip().startswith(\"<code>\")\n"
                }
            ],
            "score": false
        },
        {
            "id": 1,
            "desc": "</code> 结束",
            "other_info": {
                "from": "system_para_6"
            },
            "dimension": "example_driven",
            "type": "formatting",
            "is_meta": false,
            "evaluation": [
                {
                    "type": "code",
                    "exec": "\ndef check_following(response):\n    return response.rstrip().endswith(\"</code>\")\n"
                }
            ],
            "score": false
        },
        {
            "id": 0,
            "desc": "开始 <code>",
            "other_info": {
                "from": "system_para_6"
            },
            "dimension": "example_driven",
            "type": "formatting",
            "is_meta": false,
            "evaluation": [
                {
                    "type": "code",
                    "exec": "\ndef check_following(response):\n    return response.lstrip().startswith(\"<code>\")\n"
                }
            ],
            "score": false
        },
        {
            "id": 1,
            "desc": "</code> 结束",
            "other_info": {
                "from": "system_para_6"
            },
            "dimension": "example_driven",
            "type": "formatting",
            "is_meta": false,
            "evaluation": [
                {
                    "type": "code",
                    "exec": "\ndef check_following(response):\n    return response.rstrip().endswith(\"</code>\")\n"
                }
            ],
            "score": false
        },
        {
            "id": 0,
            "desc": "开始 <code>",
            "other_info": {
                "from": "system_para_6"
            },
            "dimension": "example_driven",
            "type": "formatting",
            "is_meta": false,
            "evaluation": [
                {
                    "type": "code",
                    "exec": "\ndef check_following(response):\n    return response.lstrip().startswith(\"<code>\")\n"
                }
            ],
            "score": false
        },
        {
            "id": 1,
            "desc": "</code> 结束",
            "other_info": {
                "from": "system_para_6"
            },
            "dimension": "example_driven",
            "type": "formatting",
            "is_meta": false,
            "evaluation": [
                {
                    "type": "code",
                    "exec": "\ndef check_following(response):\n    return response.rstrip().endswith(\"</code>\")\n"
                }
            ],
            "score": false
        },
        {
            "id": 0,
            "desc": "开始 <code>",
            "other_info": {
                "from": "system_para_6"
            },
            "dimension": "example_driven",
            "type": "formatting",
            "is_meta": false,
            "evaluation": [
                {
                    "type": "code",
                    "exec": "\ndef check_following(response):\n    return response.lstrip().startswith(\"<code>\")\n"
                }
            ],
            "score": false
        },
        {
            "id": 1,
            "desc": "</code> 结束",
            "other_info": {
                "from": "system_para_6"
            },
            "dimension": "example_driven",
            "type": "formatting",
            "is_meta": false,
            "evaluation": [
                {
                    "type": "code",
                    "exec": "\ndef check_following(response):\n    return response.rstrip().endswith(\"</code>\")\n"
                }
            ],
            "score": false
        },
        {
            "id": 0,
            "desc": "开始 <code>",
            "other_info": {
                "from": "system_para_6"
            },
            "dimension": "example_driven",
            "type": "formatting",
            "is_meta": false,
            "evaluation": [
                {
                    "type": "code",
                    "exec": "\ndef check_following(response):\n    return response.lstrip().startswith(\"<code>\")\n"
                }
            ],
            "score": false
        },
        {
            "id": 1,
            "desc": "</code> 结束",
            "other_info": {
                "from": "system_para_6"
            },
            "dimension": "example_driven",
            "type": "formatting",
            "is_meta": false,
            "evaluation": [
                {
                    "type": "code",
                    "exec": "\ndef check_following(response):\n    return response.rstrip().endswith(\"</code>\")\n"
                }
            ],
            "score": false
        },
        {
            "id": 0,
            "desc": "开始 <code>",
            "other_info": {
                "from": "system_para_6"
            },
            "dimension": "example_driven",
            "type": "formatting",
            "is_meta": false,
            "evaluation": [
                {
                    "type": "code",
                    "exec": "\ndef check_following(response):\n    return response.lstrip().startswith(\"<code>\")\n"
                }
            ],
            "score": false
        },
        {
            "id": 1,
            "desc": "</code> 结束",
            "other_info": {
                "from": "system_para_6"
            },
            "dimension": "example_driven",
            "type": "formatting",
            "is_meta": false,
            "evaluation": [
                {
                    "type": "code",
                    "exec": "\ndef check_following(response):\n    return response.rstrip().endswith(\"</code>\")\n"
                }
            ],
            "score": false
        },
        {
            "id": 0,
            "desc": "开始 <code>",
            "other_info": {
                "from": "system_para_6"
            },
            "dimension": "example_driven",
            "type": "formatting",
            "is_meta": false,
            "evaluation": [
                {
                    "type": "code",
                    "exec": "\ndef check_following(response):\n    return response.lstrip().startswith(\"<code>\")\n"
                }
            ],
            "score": false
        },
        {
            "id": 1,
            "desc": "</code> 结束",
            "other_info": {
                "from": "system_para_6"
            },
            "dimension": "example_driven",
            "type": "formatting",
            "is_meta": false,
            "evaluation": [
                {
                    "type": "code",
                    "exec": "\ndef check_following(response):\n    return response.rstrip().endswith(\"</code>\")\n"
                }
            ],
            "score": false
        },
        {
            "id": 0,
            "desc": "开始 <code>",
            "other_info": {
                "from": "system_para_6"
            },
            "dimension": "example_driven",
            "type": "formatting",
            "is_meta": false,
            "evaluation": [
                {
                    "type": "code",
                    "exec": "\ndef check_following(response):\n    return response.lstrip().startswith(\"<code>\")\n"
                }
            ],
            "score": false
        },
        {
            "id": 1,
            "desc": "</code> 结束",
            "other_info": {
                "from": "system_para_6"
            },
            "dimension": "example_driven",
            "type": "formatting",
            "is_meta": false,
            "evaluation": [
                {
                    "type": "code",
                    "exec": "\ndef check_following(response):\n    return response.rstrip().endswith(\"</code>\")\n"
                }
            ],
            "score": false
        },
        {
            "id": 0,
            "desc": "开始 <code>",
            "other_info": {
                "from": "system_para_6"
            },
            "dimension": "example_driven",
            "type": "formatting",
            "is_meta": false,
            "evaluation": [
                {
                    "type": "code",
                    "exec": "\ndef check_following(response):\n    return response.lstrip().startswith(\"<code>\")\n"
                }
            ],
            "score": false
        },
        {
            "id": 1,
            "desc": "</code> 结束",
            "other_info": {
                "from": "system_para_6"
            },
            "dimension": "example_driven",
            "type": "formatting",
            "is_meta": false,
            "evaluation": [
                {
                    "type": "code",
                    "exec": "\ndef check_following(response):\n    return response.rstrip().endswith(\"</code>\")\n"
                }
            ],
            "score": false
        },
        {
            "id": 0,
            "desc": "开始 <code>",
            "other_info": {
                "from": "system_para_6"
            },
            "dimension": "example_driven",
            "type": "formatting",
            "is_meta": false,
            "evaluation": [
                {
                    "type": "code",
                    "exec": "\ndef check_following(response):\n    return response.lstrip().startswith(\"<code>\")\n"
                }
            ],
            "score": false
        },
        {
            "id": 1,
            "desc": "</code> 结束",
            "other_info": {
                "from": "system_para_6"
            },
            "dimension": "example_driven",
            "type": "formatting",
            "is_meta": false,
            "evaluation": [
                {
                    "type": "code",
                    "exec": "\ndef check_following(response):\n    return response.rstrip().endswith(\"</code>\")\n"
                }
            ],
            "score": false
        },
        {
            "id": 0,
            "desc": "开始 <code>",
            "other_info": {
                "from": "system_para_6"
            },
            "dimension": "example_driven",
            "type": "formatting",
            "is_meta": false,
            "evaluation": [
                {
                    "type": "code",
                    "exec": "\ndef check_following(response):\n    return response.lstrip().startswith(\"<code>\")\n"
                }
            ],
            "score": false
        },
        {
            "id": 1,
            "desc": "</code> 结束",
            "other_info": {
                "from": "system_para_6"
            },
            "dimension": "example_driven",
            "type": "formatting",
            "is_meta": false,
            "evaluation": [
                {
                    "type": "code",
                    "exec": "\ndef check_following(response):\n    return response.rstrip().endswith(\"</code>\")\n"
                }
            ],
            "score": false
        },
        {
            "id": 0,
            "desc": "开始 <code>",
            "other_info": {
                "from": "system_para_6"
            },
            "dimension": "example_driven",
            "type": "formatting",
            "is_meta": false,
            "evaluation": [
                {
                    "type": "code",
                    "exec": "\ndef check_following(response):\n    return response.lstrip().startswith(\"<code>\")\n"
                }
            ],
            "score": false
        },
        {
            "id": 1,
            "desc": "</code> 结束",
            "other_info": {
                "from": "system_para_6"
            },
            "dimension": "example_driven",
            "type": "formatting",
            "is_meta": false,
            "evaluation": [
                {
                    "type": "code",
                    "exec": "\ndef check_following(response):\n    return response.rstrip().endswith(\"</code>\")\n"
                }
            ],
            "score": false
        },
        {
            "id": 0,
            "desc": "开始 <code>",
            "other_info": {
                "from": "system_para_6"
            },
            "dimension": "example_driven",
            "type": "formatting",
            "is_meta": false,
            "evaluation": [
                {
                    "type": "code",
                    "exec": "\ndef check_following(response):\n    return response.lstrip().startswith(\"<code>\")\n"
                }
            ],
            "score": false
        },
        {
            "id": 1,
            "desc": "</code> 结束",
            "other_info": {
                "from": "system_para_6"
            },
            "dimension": "example_driven",
            "type": "formatting",
            "is_meta": false,
            "evaluation": [
                {
                    "type": "code",
                    "exec": "\ndef check_following(response):\n    return response.rstrip().endswith(\"</code>\")\n"
                }
            ],
            "score": false
        },
        {
            "id": 0,
            "desc": "开始 <code>",
            "other_info": {
                "from": "system_para_6"
            },
            "dimension": "example_driven",
            "type": "formatting",
            "is_meta": false,
            "evaluation": [
                {
                    "type": "code",
                    "exec": "\ndef check_following(response):\n    return response.lstrip().startswith(\"<code>\")\n"
                }
            ],
            "score": false
        },
        {
            "id": 1,
            "desc": "</code> 结束",
            "other_info": {
                "from": "system_para_6"
            },
            "dimension": "example_driven",
            "type": "formatting",
            "is_meta": false,
            "evaluation": [
                {
                    "type": "code",
                    "exec": "\ndef check_following(response):\n    return response.rstrip().endswith(\"</code>\")\n"
                }
            ],
            "score": false
        },
        {
            "id": 0,
            "desc": "开始 <code>",
            "other_info": {
                "from": "system_para_6"
            },
            "dimension": "example_driven",
            "type": "formatting",
            "is_meta": false,
            "evaluation": [
                {
                    "type": "code",
                    "exec": "\ndef check_following(response):\n    return response.lstrip().startswith(\"<code>\")\n"
                }
            ],
            "score": false
        },
        {
            "id": 1,
            "desc": "</code> 结束",
            "other_info": {
                "from": "system_para_6"
            },
            "dimension": "example_driven",
            "type": "formatting",
            "is_meta": false,
            "evaluation": [
                {
                    "type": "code",
                    "exec": "\ndef check_following(response):\n    return response.rstrip().endswith(\"</code>\")\n"
                }
            ],
            "score": false
        },
        {
            "id": 0,
            "desc": "开始 <code>",
            "other_info": {
                "from": "system_para_6"
            },
            "dimension": "example_driven",
            "type": "formatting",
            "is_meta": false,
            "evaluation": [
                {
                    "type": "code",
                    "exec": "\ndef check_following(response):\n    return response.lstrip().startswith(\"<code>\")\n"
                }
            ],
            "score": false
        },
        {
            "id": 1,
            "desc": "</code> 结束",
            "other_info": {
                "from": "system_para_6"
            },
            "dimension": "example_driven",
            "type": "formatting",
            "is_meta": false,
            "evaluation": [
                {
                    "type": "code",
                    "exec": "\ndef check_following(response):\n    return response.rstrip().endswith(\"</code>\")\n"
                }
            ],
            "score": false
        },
        {
            "id": 0,
            "desc": "开始 <code>",
            "other_info": {
                "from": "system_para_6"
            },
            "dimension": "example_driven",
            "type": "formatting",
            "is_meta": false,
            "evaluation": [
                {
                    "type": "code",
                    "exec": "\ndef check_following(response):\n    return response.lstrip().startswith(\"<code>\")\n"
                }
            ],
            "score": false
        },
        {
            "id": 1,
            "desc": "</code> 结束",
            "other_info": {
                "from": "system_para_6"
            },
            "dimension": "example_driven",
            "type": "formatting",
            "is_meta": false,
            "evaluation": [
                {
                    "type": "code",
                    "exec": "\ndef check_following(response):\n    return response.rstrip().endswith(\"</code>\")\n"
                }
            ],
            "score": false
        },
        {
            "id": -1,
            "desc": "In case (2), write this sub-question with an [END] mark as a leaf node.",
            "other_info": {
                "from": "system_para_0",
                "condition_desc": "If case (2), write this sub-question with an [END] mark as a leaf node.",
                "complete_instruction_para": []
            },
            "dimension": "example_driven",
            "is_meta": false,
            "type": [
                "formatting"
            ],
            "evaluation": [
                {
                    "type": "llm_conditional_check",
                    "exec": "Does there exist sub-questions that satisfy case (2), which can be 'directly answered by analyzing the answers of at least two previously answered questions, such as comparing, judging, intersecting, or counting'? Please answer YES/NO directly and do not enter anything else.\n\nHere is model response: {response}"
                },
                {
                    "type": "llm",
                    "exec": "In the model response, are all sub-questions that satisfy case (2) written with an [END] mark as leaf nodes? Please answer YES/NO directly and do not enter anything else.\n\nHere is model response: {response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": -1,
            "desc": "use escape quotes \"\" to enclose work titles and function parameters",
            "other_info": {
                "from": "system_para_0",
                "condition_desc": "",
                "complete_instruction_para": []
            },
            "dimension": "example_driven",
            "is_meta": false,
            "type": [
                "formatting"
            ],
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "Did the model use escape quotes \"\" to enclose all work titles and function parameters? Please answer YES/NO directly and do not enter anything else.\n\nHere is model response: {response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": -1,
            "desc": "In case (2), write this sub-question with an [END] mark as a leaf node.",
            "other_info": {
                "from": "system_para_0",
                "condition_desc": "If case (2), write this sub-question with an [END] mark as a leaf node.",
                "complete_instruction_para": []
            },
            "dimension": "example_driven",
            "is_meta": false,
            "type": [
                "formatting"
            ],
            "evaluation": [
                {
                    "type": "llm_conditional_check",
                    "exec": "Does there exist sub-questions that satisfy case (2), which can be 'directly answered by analyzing the answers of at least two previously answered questions, such as comparing, judging, intersecting, or counting'? Please answer YES/NO directly and do not enter anything else.\n\nHere is model response: {response}"
                },
                {
                    "type": "llm",
                    "exec": "In the model response, are all sub-questions that satisfy case (2) written with an [END] mark as leaf nodes? Please answer YES/NO directly and do not enter anything else.\n\nHere is model response: {response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": -1,
            "desc": "use escape quotes \"\" to enclose work titles and function parameters",
            "other_info": {
                "from": "system_para_0",
                "condition_desc": "",
                "complete_instruction_para": []
            },
            "dimension": "example_driven",
            "is_meta": false,
            "type": [
                "formatting"
            ],
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "Did the model use escape quotes \"\" to enclose all work titles and function parameters? Please answer YES/NO directly and do not enter anything else.\n\nHere is model response: {response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": -1,
            "desc": "In case (2), write this sub-question with an [END] mark as a leaf node.",
            "other_info": {
                "from": "system_para_0",
                "condition_desc": "If case (2), write this sub-question with an [END] mark as a leaf node.",
                "complete_instruction_para": []
            },
            "dimension": "example_driven",
            "is_meta": false,
            "type": [
                "formatting"
            ],
            "evaluation": [
                {
                    "type": "llm_conditional_check",
                    "exec": "Does there exist sub-questions that satisfy case (2), which can be 'directly answered by analyzing the answers of at least two previously answered questions, such as comparing, judging, intersecting, or counting'? Please answer YES/NO directly and do not enter anything else.\n\nHere is model response: {response}"
                },
                {
                    "type": "llm",
                    "exec": "In the model response, are all sub-questions that satisfy case (2) written with an [END] mark as leaf nodes? Please answer YES/NO directly and do not enter anything else.\n\nHere is model response: {response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": -1,
            "desc": "use double quotes to enclose sub-questions and functions",
            "other_info": {
                "from": "system_para_0",
                "condition_desc": "",
                "complete_instruction_para": []
            },
            "dimension": "example_driven",
            "is_meta": false,
            "type": [
                "formatting"
            ],
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "Did the model use double quotes to enclose all sub-questions and functions? Please answer YES/NO directly and do not enter anything else.\n\nHere is model response: {response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": -1,
            "desc": "use escape quotes \"\" to enclose work titles and function parameters",
            "other_info": {
                "from": "system_para_0",
                "condition_desc": "",
                "complete_instruction_para": []
            },
            "dimension": "example_driven",
            "is_meta": false,
            "type": [
                "formatting"
            ],
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "Did the model use escape quotes \"\" to enclose all work titles and function parameters? Please answer YES/NO directly and do not enter anything else.\n\nHere is model response: {response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": -1,
            "desc": "In case (2), write this sub-question with an [END] mark as a leaf node.",
            "other_info": {
                "from": "system_para_0",
                "condition_desc": "If case (2), write this sub-question with an [END] mark as a leaf node.",
                "complete_instruction_para": []
            },
            "dimension": "example_driven",
            "is_meta": false,
            "type": [
                "formatting"
            ],
            "evaluation": [
                {
                    "type": "llm_conditional_check",
                    "exec": "Does there exist sub-questions that satisfy case (2), which can be 'directly answered by analyzing the answers of at least two previously answered questions, such as comparing, judging, intersecting, or counting'? Please answer YES/NO directly and do not enter anything else.\n\nHere is model response: {response}"
                },
                {
                    "type": "llm",
                    "exec": "In the model response, are all sub-questions that satisfy case (2) written with an [END] mark as leaf nodes? Please answer YES/NO directly and do not enter anything else.\n\nHere is model response: {response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": -1,
            "desc": "use escape quotes \"\" to enclose work titles and function parameters",
            "other_info": {
                "from": "system_para_0",
                "condition_desc": "",
                "complete_instruction_para": []
            },
            "dimension": "example_driven",
            "is_meta": false,
            "type": [
                "formatting"
            ],
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "Did the model use escape quotes \"\" to enclose all work titles and function parameters? Please answer YES/NO directly and do not enter anything else.\n\nHere is model response: {response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": -1,
            "desc": "In case (2), write this sub-question with an [END] mark as a leaf node.",
            "other_info": {
                "from": "system_para_0",
                "condition_desc": "If case (2), write this sub-question with an [END] mark as a leaf node.",
                "complete_instruction_para": []
            },
            "dimension": "example_driven",
            "is_meta": false,
            "type": [
                "formatting"
            ],
            "evaluation": [
                {
                    "type": "llm_conditional_check",
                    "exec": "Does there exist sub-questions that satisfy case (2), which can be 'directly answered by analyzing the answers of at least two previously answered questions, such as comparing, judging, intersecting, or counting'? Please answer YES/NO directly and do not enter anything else.\n\nHere is model response: {response}"
                },
                {
                    "type": "llm",
                    "exec": "In the model response, are all sub-questions that satisfy case (2) written with an [END] mark as leaf nodes? Please answer YES/NO directly and do not enter anything else.\n\nHere is model response: {response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": -1,
            "desc": "use escape quotes \"\" to enclose work titles and function parameters",
            "other_info": {
                "from": "system_para_0",
                "condition_desc": "",
                "complete_instruction_para": []
            },
            "dimension": "example_driven",
            "is_meta": false,
            "type": [
                "formatting"
            ],
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "Did the model use escape quotes \"\" to enclose all work titles and function parameters? Please answer YES/NO directly and do not enter anything else.\n\nHere is model response: {response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": -1,
            "desc": "use escape quotes \"\" to enclose work titles and function parameters",
            "other_info": {
                "from": "system_para_0",
                "condition_desc": "",
                "complete_instruction_para": []
            },
            "dimension": "example_driven",
            "is_meta": false,
            "type": [
                "formatting"
            ],
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "Did the model use escape quotes \"\" to enclose all work titles and function parameters? Please answer YES/NO directly and do not enter anything else.\n\nHere is model response: {response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": -1,
            "desc": "use escape quotes \"\" to enclose work titles and function parameters",
            "other_info": {
                "from": "system_para_0",
                "condition_desc": "",
                "complete_instruction_para": []
            },
            "dimension": "example_driven",
            "is_meta": false,
            "type": [
                "formatting"
            ],
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "Did the model use escape quotes \"\" to enclose all work titles and function parameters? Please answer YES/NO directly and do not enter anything else.\n\nHere is model response: {response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": -1,
            "desc": "In case (2), write this sub-question with an [END] mark as a leaf node.",
            "other_info": {
                "from": "system_para_0",
                "condition_desc": "If case (2), write this sub-question with an [END] mark as a leaf node.",
                "complete_instruction_para": []
            },
            "dimension": "example_driven",
            "is_meta": false,
            "type": [
                "formatting"
            ],
            "evaluation": [
                {
                    "type": "llm_conditional_check",
                    "exec": "Does there exist sub-questions that satisfy case (2), which can be 'directly answered by analyzing the answers of at least two previously answered questions, such as comparing, judging, intersecting, or counting'? Please answer YES/NO directly and do not enter anything else.\n\nHere is model response: {response}"
                },
                {
                    "type": "llm",
                    "exec": "In the model response, are all sub-questions that satisfy case (2) written with an [END] mark as leaf nodes? Please answer YES/NO directly and do not enter anything else.\n\nHere is model response: {response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": -1,
            "desc": "use escape quotes \"\" to enclose work titles and function parameters",
            "other_info": {
                "from": "system_para_0",
                "condition_desc": "",
                "complete_instruction_para": []
            },
            "dimension": "example_driven",
            "is_meta": false,
            "type": [
                "formatting"
            ],
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "Did the model use escape quotes \"\" to enclose all work titles and function parameters? Please answer YES/NO directly and do not enter anything else.\n\nHere is model response: {response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": -1,
            "desc": "use escape quotes \"\" to enclose work titles and function parameters",
            "other_info": {
                "from": "system_para_0",
                "condition_desc": "",
                "complete_instruction_para": []
            },
            "dimension": "example_driven",
            "is_meta": false,
            "type": [
                "formatting"
            ],
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "Did the model use escape quotes \"\" to enclose all work titles and function parameters? Please answer YES/NO directly and do not enter anything else.\n\nHere is model response: {response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": -1,
            "desc": "In case (2), write this sub-question with an [END] mark as a leaf node.",
            "other_info": {
                "from": "system_para_0",
                "condition_desc": "If case (2), write this sub-question with an [END] mark as a leaf node.",
                "complete_instruction_para": []
            },
            "dimension": "example_driven",
            "is_meta": false,
            "type": [
                "formatting"
            ],
            "evaluation": [
                {
                    "type": "llm_conditional_check",
                    "exec": "Does there exist sub-questions that satisfy case (2), which can be 'directly answered by analyzing the answers of at least two previously answered questions, such as comparing, judging, intersecting, or counting'? Please answer YES/NO directly and do not enter anything else.\n\nHere is model response: {response}"
                },
                {
                    "type": "llm",
                    "exec": "In the model response, are all sub-questions that satisfy case (2) written with an [END] mark as leaf nodes? Please answer YES/NO directly and do not enter anything else.\n\nHere is model response: {response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": -1,
            "desc": "In case (2), write this sub-question with an [END] mark as a leaf node.",
            "other_info": {
                "from": "system_para_0",
                "condition_desc": "If case (2), write this sub-question with an [END] mark as a leaf node.",
                "complete_instruction_para": []
            },
            "dimension": "example_driven",
            "is_meta": false,
            "type": [
                "formatting"
            ],
            "evaluation": [
                {
                    "type": "llm_conditional_check",
                    "exec": "Does there exist sub-questions that satisfy case (2), which can be 'directly answered by analyzing the answers of at least two previously answered questions, such as comparing, judging, intersecting, or counting'? Please answer YES/NO directly and do not enter anything else.\n\nHere is model response: {response}"
                },
                {
                    "type": "llm",
                    "exec": "In the model response, are all sub-questions that satisfy case (2) written with an [END] mark as leaf nodes? Please answer YES/NO directly and do not enter anything else.\n\nHere is model response: {response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": -1,
            "desc": "use escape quotes \"\" to enclose work titles and function parameters",
            "other_info": {
                "from": "system_para_0",
                "condition_desc": "",
                "complete_instruction_para": []
            },
            "dimension": "example_driven",
            "is_meta": false,
            "type": [
                "formatting"
            ],
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "Did the model use escape quotes \"\" to enclose all work titles and function parameters? Please answer YES/NO directly and do not enter anything else.\n\nHere is model response: {response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": -1,
            "desc": "For [END] leaf questions, format your question as 'Given answers of [q_idx_1] and [q_idx_2], ...', where [q_idx_1] and [q_idx_2] are question indices of the previously answered questions required to answer this [END] question.",
            "other_info": {
                "from": "system_para_0",
                "condition_desc": "If [END] leaf questions, format your question as 'Given answers of [q_idx_1] and [q_idx_2], ...', where [q_idx_1] and [q_idx_2] are question indices of the previously answered questions required to answer this [END] question..",
                "complete_instruction_para": []
            },
            "dimension": "example_driven",
            "is_meta": false,
            "type": [
                "formatting"
            ],
            "evaluation": [
                {
                    "type": "llm_conditional_check",
                    "exec": "Are there [END] leaf questions in the tree? Please answer YES/NO directly and do not enter anything else.\n\nHere is model response: {response}"
                },
                {
                    "type": "llm",
                    "exec": "Are all [END] leaf questions formatted as 'Given answers of [q_idx_1] and [q_idx_2], ...', where [q_idx_1] and [q_idx_2] are question indices of previously answered questions required to answer this [END] question? Please answer YES/NO directly and do not enter anything else.\n\nHere is model response: {response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": -1,
            "desc": "use escape quotes \"\" to enclose work titles and function parameters",
            "other_info": {
                "from": "system_para_0",
                "condition_desc": "",
                "complete_instruction_para": []
            },
            "dimension": "example_driven",
            "is_meta": false,
            "type": [
                "formatting"
            ],
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "Did the model use escape quotes \"\" to enclose all work titles and function parameters? Please answer YES/NO directly and do not enter anything else.\n\nHere is model response: {response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": -1,
            "desc": "For function leaf nodes, do not write nested functions such as Filter(Search(...))",
            "other_info": {
                "from": "system_para_0",
                "condition_desc": "If function leaf nodes, do not write nested functions such as Filter(Search(...)).",
                "complete_instruction_para": []
            },
            "dimension": "example_driven",
            "is_meta": false,
            "type": [
                "formatting"
            ],
            "evaluation": [
                {
                    "type": "llm_conditional_check",
                    "exec": "Are there function leaf nodes in the question decomposition tree? Please answer YES/NO directly and do not enter anything else.\n\nHere is model response: {response}"
                },
                {
                    "type": "llm",
                    "exec": "For all function leaf nodes in the model response, did the model avoid from writing any nested functions such as Filter(Search(...))? Please answer YES/NO directly and do not enter anything else.\n\nHere is model response: {response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": -1,
            "desc": "use escape quotes \"\" to enclose work titles and function parameters",
            "other_info": {
                "from": "system_para_0",
                "condition_desc": "",
                "complete_instruction_para": []
            },
            "dimension": "example_driven",
            "is_meta": false,
            "type": [
                "formatting"
            ],
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "Did the model use escape quotes \"\" to enclose all work titles and function parameters? Please answer YES/NO directly and do not enter anything else.\n\nHere is model response: {response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": -1,
            "desc": "In case (2), write this sub-question with an [END] mark as a leaf node.",
            "other_info": {
                "from": "system_para_0",
                "condition_desc": "If case (2), write this sub-question with an [END] mark as a leaf node.",
                "complete_instruction_para": []
            },
            "dimension": "example_driven",
            "is_meta": false,
            "type": [
                "formatting"
            ],
            "evaluation": [
                {
                    "type": "llm_conditional_check",
                    "exec": "Does there exist sub-questions that satisfy case (2), which can be 'directly answered by analyzing the answers of at least two previously answered questions, such as comparing, judging, intersecting, or counting'? Please answer YES/NO directly and do not enter anything else.\n\nHere is model response: {response}"
                },
                {
                    "type": "llm",
                    "exec": "In the model response, are all sub-questions that satisfy case (2) written with an [END] mark as leaf nodes? Please answer YES/NO directly and do not enter anything else.\n\nHere is model response: {response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": -1,
            "desc": "use escape quotes \"\" to enclose work titles and function parameters",
            "other_info": {
                "from": "system_para_0",
                "condition_desc": "",
                "complete_instruction_para": []
            },
            "dimension": "example_driven",
            "is_meta": false,
            "type": [
                "formatting"
            ],
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "Did the model use escape quotes \"\" to enclose all work titles and function parameters? Please answer YES/NO directly and do not enter anything else.\n\nHere is model response: {response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": -1,
            "desc": "In case (2), write this sub-question with an [END] mark as a leaf node.",
            "other_info": {
                "from": "system_para_0",
                "condition_desc": "If case (2), write this sub-question with an [END] mark as a leaf node.",
                "complete_instruction_para": []
            },
            "dimension": "example_driven",
            "is_meta": false,
            "type": [
                "formatting"
            ],
            "evaluation": [
                {
                    "type": "llm_conditional_check",
                    "exec": "Does there exist sub-questions that satisfy case (2), which can be 'directly answered by analyzing the answers of at least two previously answered questions, such as comparing, judging, intersecting, or counting'? Please answer YES/NO directly and do not enter anything else.\n\nHere is model response: {response}"
                },
                {
                    "type": "llm",
                    "exec": "In the model response, are all sub-questions that satisfy case (2) written with an [END] mark as leaf nodes? Please answer YES/NO directly and do not enter anything else.\n\nHere is model response: {response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": -1,
            "desc": "use escape quotes \"\" to enclose work titles and function parameters",
            "other_info": {
                "from": "system_para_0",
                "condition_desc": "",
                "complete_instruction_para": []
            },
            "dimension": "example_driven",
            "is_meta": false,
            "type": [
                "formatting"
            ],
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "Did the model use escape quotes \"\" to enclose all work titles and function parameters? Please answer YES/NO directly and do not enter anything else.\n\nHere is model response: {response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": -1,
            "desc": "In case (2), write this sub-question with an [END] mark as a leaf node.",
            "other_info": {
                "from": "system_para_0",
                "condition_desc": "If case (2), write this sub-question with an [END] mark as a leaf node.",
                "complete_instruction_para": []
            },
            "dimension": "example_driven",
            "is_meta": false,
            "type": [
                "formatting"
            ],
            "evaluation": [
                {
                    "type": "llm_conditional_check",
                    "exec": "Does there exist sub-questions that satisfy case (2), which can be 'directly answered by analyzing the answers of at least two previously answered questions, such as comparing, judging, intersecting, or counting'? Please answer YES/NO directly and do not enter anything else.\n\nHere is model response: {response}"
                },
                {
                    "type": "llm",
                    "exec": "In the model response, are all sub-questions that satisfy case (2) written with an [END] mark as leaf nodes? Please answer YES/NO directly and do not enter anything else.\n\nHere is model response: {response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": -1,
            "desc": "use escape quotes \"\" to enclose work titles and function parameters",
            "other_info": {
                "from": "system_para_0",
                "condition_desc": "",
                "complete_instruction_para": []
            },
            "dimension": "example_driven",
            "is_meta": false,
            "type": [
                "formatting"
            ],
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "Did the model use escape quotes \"\" to enclose all work titles and function parameters? Please answer YES/NO directly and do not enter anything else.\n\nHere is model response: {response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": -1,
            "desc": "In case (2), write this sub-question with an [END] mark as a leaf node.",
            "other_info": {
                "from": "system_para_0",
                "condition_desc": "If case (2), write this sub-question with an [END] mark as a leaf node.",
                "complete_instruction_para": []
            },
            "dimension": "example_driven",
            "is_meta": false,
            "type": [
                "formatting"
            ],
            "evaluation": [
                {
                    "type": "llm_conditional_check",
                    "exec": "Does there exist sub-questions that satisfy case (2), which can be 'directly answered by analyzing the answers of at least two previously answered questions, such as comparing, judging, intersecting, or counting'? Please answer YES/NO directly and do not enter anything else.\n\nHere is model response: {response}"
                },
                {
                    "type": "llm",
                    "exec": "In the model response, are all sub-questions that satisfy case (2) written with an [END] mark as leaf nodes? Please answer YES/NO directly and do not enter anything else.\n\nHere is model response: {response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": -1,
            "desc": "use escape quotes \"\" to enclose work titles and function parameters",
            "other_info": {
                "from": "system_para_0",
                "condition_desc": "",
                "complete_instruction_para": []
            },
            "dimension": "example_driven",
            "is_meta": false,
            "type": [
                "formatting"
            ],
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "Did the model use escape quotes \"\" to enclose all work titles and function parameters? Please answer YES/NO directly and do not enter anything else.\n\nHere is model response: {response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": -1,
            "desc": "In case (2), write this sub-question with an [END] mark as a leaf node.",
            "other_info": {
                "from": "system_para_0",
                "condition_desc": "If case (2), write this sub-question with an [END] mark as a leaf node.",
                "complete_instruction_para": []
            },
            "dimension": "example_driven",
            "is_meta": false,
            "type": [
                "formatting"
            ],
            "evaluation": [
                {
                    "type": "llm_conditional_check",
                    "exec": "Does there exist sub-questions that satisfy case (2), which can be 'directly answered by analyzing the answers of at least two previously answered questions, such as comparing, judging, intersecting, or counting'? Please answer YES/NO directly and do not enter anything else.\n\nHere is model response: {response}"
                },
                {
                    "type": "llm",
                    "exec": "In the model response, are all sub-questions that satisfy case (2) written with an [END] mark as leaf nodes? Please answer YES/NO directly and do not enter anything else.\n\nHere is model response: {response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": -1,
            "desc": "use escape quotes \"\" to enclose work titles and function parameters",
            "other_info": {
                "from": "system_para_0",
                "condition_desc": "",
                "complete_instruction_para": []
            },
            "dimension": "example_driven",
            "is_meta": false,
            "type": [
                "formatting"
            ],
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "Did the model use escape quotes \"\" to enclose all work titles and function parameters? Please answer YES/NO directly and do not enter anything else.\n\nHere is model response: {response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": -1,
            "desc": "In case (2), write this sub-question with an [END] mark as a leaf node.",
            "other_info": {
                "from": "system_para_0",
                "condition_desc": "If case (2), write this sub-question with an [END] mark as a leaf node.",
                "complete_instruction_para": []
            },
            "dimension": "example_driven",
            "is_meta": false,
            "type": [
                "formatting"
            ],
            "evaluation": [
                {
                    "type": "llm_conditional_check",
                    "exec": "Does there exist sub-questions that satisfy case (2), which can be 'directly answered by analyzing the answers of at least two previously answered questions, such as comparing, judging, intersecting, or counting'? Please answer YES/NO directly and do not enter anything else.\n\nHere is model response: {response}"
                },
                {
                    "type": "llm",
                    "exec": "In the model response, are all sub-questions that satisfy case (2) written with an [END] mark as leaf nodes? Please answer YES/NO directly and do not enter anything else.\n\nHere is model response: {response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": -1,
            "desc": "use escape quotes \"\" to enclose work titles and function parameters",
            "other_info": {
                "from": "system_para_0",
                "condition_desc": "",
                "complete_instruction_para": []
            },
            "dimension": "example_driven",
            "is_meta": false,
            "type": [
                "formatting"
            ],
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "Did the model use escape quotes \"\" to enclose all work titles and function parameters? Please answer YES/NO directly and do not enter anything else.\n\nHere is model response: {response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": -1,
            "desc": "In case (2), write this sub-question with an [END] mark as a leaf node.",
            "other_info": {
                "from": "system_para_0",
                "condition_desc": "If case (2), write this sub-question with an [END] mark as a leaf node.",
                "complete_instruction_para": []
            },
            "dimension": "example_driven",
            "is_meta": false,
            "type": [
                "formatting"
            ],
            "evaluation": [
                {
                    "type": "llm_conditional_check",
                    "exec": "Does there exist sub-questions that satisfy case (2), which can be 'directly answered by analyzing the answers of at least two previously answered questions, such as comparing, judging, intersecting, or counting'? Please answer YES/NO directly and do not enter anything else.\n\nHere is model response: {response}"
                },
                {
                    "type": "llm",
                    "exec": "In the model response, are all sub-questions that satisfy case (2) written with an [END] mark as leaf nodes? Please answer YES/NO directly and do not enter anything else.\n\nHere is model response: {response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": -1,
            "desc": "use escape quotes \"\" to enclose work titles and function parameters",
            "other_info": {
                "from": "system_para_0",
                "condition_desc": "",
                "complete_instruction_para": []
            },
            "dimension": "example_driven",
            "is_meta": false,
            "type": [
                "formatting"
            ],
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "Did the model use escape quotes \"\" to enclose all work titles and function parameters? Please answer YES/NO directly and do not enter anything else.\n\nHere is model response: {response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": -1,
            "desc": "In case (2), write this sub-question with an [END] mark as a leaf node.",
            "other_info": {
                "from": "system_para_0",
                "condition_desc": "If case (2), write this sub-question with an [END] mark as a leaf node.",
                "complete_instruction_para": []
            },
            "dimension": "example_driven",
            "is_meta": false,
            "type": [
                "formatting"
            ],
            "evaluation": [
                {
                    "type": "llm_conditional_check",
                    "exec": "Does there exist sub-questions that satisfy case (2), which can be 'directly answered by analyzing the answers of at least two previously answered questions, such as comparing, judging, intersecting, or counting'? Please answer YES/NO directly and do not enter anything else.\n\nHere is model response: {response}"
                },
                {
                    "type": "llm",
                    "exec": "In the model response, are all sub-questions that satisfy case (2) written with an [END] mark as leaf nodes? Please answer YES/NO directly and do not enter anything else.\n\nHere is model response: {response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": -1,
            "desc": "In case (2), write this sub-question with an [END] mark as a leaf node.",
            "other_info": {
                "from": "system_para_0",
                "condition_desc": "If case (2), write this sub-question with an [END] mark as a leaf node.",
                "complete_instruction_para": []
            },
            "dimension": "example_driven",
            "is_meta": false,
            "type": [
                "formatting"
            ],
            "evaluation": [
                {
                    "type": "llm_conditional_check",
                    "exec": "Does there exist sub-questions that satisfy case (2), which can be 'directly answered by analyzing the answers of at least two previously answered questions, such as comparing, judging, intersecting, or counting'? Please answer YES/NO directly and do not enter anything else.\n\nHere is model response: {response}"
                },
                {
                    "type": "llm",
                    "exec": "In the model response, are all sub-questions that satisfy case (2) written with an [END] mark as leaf nodes? Please answer YES/NO directly and do not enter anything else.\n\nHere is model response: {response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": -1,
            "desc": "use escape quotes \"\" to enclose work titles and function parameters",
            "other_info": {
                "from": "system_para_0",
                "condition_desc": "",
                "complete_instruction_para": []
            },
            "dimension": "example_driven",
            "is_meta": false,
            "type": [
                "formatting"
            ],
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "Did the model use escape quotes \"\" to enclose all work titles and function parameters? Please answer YES/NO directly and do not enter anything else.\n\nHere is model response: {response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": -1,
            "desc": "In case (2), write this sub-question with an [END] mark as a leaf node.",
            "other_info": {
                "from": "system_para_0",
                "condition_desc": "If case (2), write this sub-question with an [END] mark as a leaf node.",
                "complete_instruction_para": []
            },
            "dimension": "example_driven",
            "is_meta": false,
            "type": [
                "formatting"
            ],
            "evaluation": [
                {
                    "type": "llm_conditional_check",
                    "exec": "Does there exist sub-questions that satisfy case (2), which can be 'directly answered by analyzing the answers of at least two previously answered questions, such as comparing, judging, intersecting, or counting'? Please answer YES/NO directly and do not enter anything else.\n\nHere is model response: {response}"
                },
                {
                    "type": "llm",
                    "exec": "In the model response, are all sub-questions that satisfy case (2) written with an [END] mark as leaf nodes? Please answer YES/NO directly and do not enter anything else.\n\nHere is model response: {response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": -1,
            "desc": "use escape quotes \"\" to enclose work titles and function parameters",
            "other_info": {
                "from": "system_para_0",
                "condition_desc": "",
                "complete_instruction_para": []
            },
            "dimension": "example_driven",
            "is_meta": false,
            "type": [
                "formatting"
            ],
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "Did the model use escape quotes \"\" to enclose all work titles and function parameters? Please answer YES/NO directly and do not enter anything else.\n\nHere is model response: {response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": -1,
            "desc": "In case (2), write this sub-question with an [END] mark as a leaf node.",
            "other_info": {
                "from": "system_para_0",
                "condition_desc": "If case (2), write this sub-question with an [END] mark as a leaf node.",
                "complete_instruction_para": []
            },
            "dimension": "example_driven",
            "is_meta": false,
            "type": [
                "formatting"
            ],
            "evaluation": [
                {
                    "type": "llm_conditional_check",
                    "exec": "Does there exist sub-questions that satisfy case (2), which can be 'directly answered by analyzing the answers of at least two previously answered questions, such as comparing, judging, intersecting, or counting'? Please answer YES/NO directly and do not enter anything else.\n\nHere is model response: {response}"
                },
                {
                    "type": "llm",
                    "exec": "In the model response, are all sub-questions that satisfy case (2) written with an [END] mark as leaf nodes? Please answer YES/NO directly and do not enter anything else.\n\nHere is model response: {response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": -1,
            "desc": "use escape quotes \"\" to enclose work titles and function parameters",
            "other_info": {
                "from": "system_para_0",
                "condition_desc": "",
                "complete_instruction_para": []
            },
            "dimension": "example_driven",
            "is_meta": false,
            "type": [
                "formatting"
            ],
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "Did the model use escape quotes \"\" to enclose all work titles and function parameters? Please answer YES/NO directly and do not enter anything else.\n\nHere is model response: {response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": -1,
            "desc": "In case (2), write this sub-question with an [END] mark as a leaf node.",
            "other_info": {
                "from": "system_para_0",
                "condition_desc": "If case (2), write this sub-question with an [END] mark as a leaf node.",
                "complete_instruction_para": []
            },
            "dimension": "example_driven",
            "is_meta": false,
            "type": [
                "formatting"
            ],
            "evaluation": [
                {
                    "type": "llm_conditional_check",
                    "exec": "Does there exist sub-questions that satisfy case (2), which can be 'directly answered by analyzing the answers of at least two previously answered questions, such as comparing, judging, intersecting, or counting'? Please answer YES/NO directly and do not enter anything else.\n\nHere is model response: {response}"
                },
                {
                    "type": "llm",
                    "exec": "In the model response, are all sub-questions that satisfy case (2) written with an [END] mark as leaf nodes? Please answer YES/NO directly and do not enter anything else.\n\nHere is model response: {response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": -1,
            "desc": "use escape quotes \"\" to enclose work titles and function parameters",
            "other_info": {
                "from": "system_para_0",
                "condition_desc": "",
                "complete_instruction_para": []
            },
            "dimension": "example_driven",
            "is_meta": false,
            "type": [
                "formatting"
            ],
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "Did the model use escape quotes \"\" to enclose all work titles and function parameters? Please answer YES/NO directly and do not enter anything else.\n\nHere is model response: {response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": -1,
            "desc": "use escape quotes \"\" to enclose work titles and function parameters",
            "other_info": {
                "from": "system_para_0",
                "condition_desc": "",
                "complete_instruction_para": []
            },
            "dimension": "example_driven",
            "is_meta": false,
            "type": [
                "formatting"
            ],
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "Did the model use escape quotes \"\" to enclose all work titles and function parameters? Please answer YES/NO directly and do not enter anything else.\n\nHere is model response: {response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": -1,
            "desc": "use escape quotes \"\" to enclose work titles and function parameters",
            "other_info": {
                "from": "system_para_0",
                "condition_desc": "",
                "complete_instruction_para": []
            },
            "dimension": "example_driven",
            "is_meta": false,
            "type": [
                "formatting"
            ],
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "Did the model use escape quotes \"\" to enclose all work titles and function parameters? Please answer YES/NO directly and do not enter anything else.\n\nHere is model response: {response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": -1,
            "desc": "In case (2), write this sub-question with an [END] mark as a leaf node.",
            "other_info": {
                "from": "system_para_0",
                "condition_desc": "If case (2), write this sub-question with an [END] mark as a leaf node.",
                "complete_instruction_para": []
            },
            "dimension": "example_driven",
            "is_meta": false,
            "type": [
                "formatting"
            ],
            "evaluation": [
                {
                    "type": "llm_conditional_check",
                    "exec": "Does there exist sub-questions that satisfy case (2), which can be 'directly answered by analyzing the answers of at least two previously answered questions, such as comparing, judging, intersecting, or counting'? Please answer YES/NO directly and do not enter anything else.\n\nHere is model response: {response}"
                },
                {
                    "type": "llm",
                    "exec": "In the model response, are all sub-questions that satisfy case (2) written with an [END] mark as leaf nodes? Please answer YES/NO directly and do not enter anything else.\n\nHere is model response: {response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": -1,
            "desc": "use escape quotes \"\" to enclose work titles and function parameters",
            "other_info": {
                "from": "system_para_0",
                "condition_desc": "",
                "complete_instruction_para": []
            },
            "dimension": "example_driven",
            "is_meta": false,
            "type": [
                "formatting"
            ],
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "Did the model use escape quotes \"\" to enclose all work titles and function parameters? Please answer YES/NO directly and do not enter anything else.\n\nHere is model response: {response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": -1,
            "desc": "In case (2), write this sub-question with an [END] mark as a leaf node.",
            "other_info": {
                "from": "system_para_0",
                "condition_desc": "If case (2), write this sub-question with an [END] mark as a leaf node.",
                "complete_instruction_para": []
            },
            "dimension": "example_driven",
            "is_meta": false,
            "type": [
                "formatting"
            ],
            "evaluation": [
                {
                    "type": "llm_conditional_check",
                    "exec": "Does there exist sub-questions that satisfy case (2), which can be 'directly answered by analyzing the answers of at least two previously answered questions, such as comparing, judging, intersecting, or counting'? Please answer YES/NO directly and do not enter anything else.\n\nHere is model response: {response}"
                },
                {
                    "type": "llm",
                    "exec": "In the model response, are all sub-questions that satisfy case (2) written with an [END] mark as leaf nodes? Please answer YES/NO directly and do not enter anything else.\n\nHere is model response: {response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": -1,
            "desc": "In case (2), write this sub-question with an [END] mark as a leaf node.",
            "other_info": {
                "from": "system_para_0",
                "condition_desc": "If case (2), write this sub-question with an [END] mark as a leaf node.",
                "complete_instruction_para": []
            },
            "dimension": "example_driven",
            "is_meta": false,
            "type": [
                "formatting"
            ],
            "evaluation": [
                {
                    "type": "llm_conditional_check",
                    "exec": "Does there exist sub-questions that satisfy case (2), which can be 'directly answered by analyzing the answers of at least two previously answered questions, such as comparing, judging, intersecting, or counting'? Please answer YES/NO directly and do not enter anything else.\n\nHere is model response: {response}"
                },
                {
                    "type": "llm",
                    "exec": "In the model response, are all sub-questions that satisfy case (2) written with an [END] mark as leaf nodes? Please answer YES/NO directly and do not enter anything else.\n\nHere is model response: {response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": -1,
            "desc": "use escape quotes \"\" to enclose work titles and function parameters",
            "other_info": {
                "from": "system_para_0",
                "condition_desc": "",
                "complete_instruction_para": []
            },
            "dimension": "example_driven",
            "is_meta": false,
            "type": [
                "formatting"
            ],
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "Did the model use escape quotes \"\" to enclose all work titles and function parameters? Please answer YES/NO directly and do not enter anything else.\n\nHere is model response: {response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": -1,
            "desc": "use escape quotes \"\" to enclose work titles and function parameters",
            "other_info": {
                "from": "system_para_0",
                "condition_desc": "",
                "complete_instruction_para": []
            },
            "dimension": "example_driven",
            "is_meta": false,
            "type": [
                "formatting"
            ],
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "Did the model use escape quotes \"\" to enclose all work titles and function parameters? Please answer YES/NO directly and do not enter anything else.\n\nHere is model response: {response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": -1,
            "desc": "In case (2), write this sub-question with an [END] mark as a leaf node.",
            "other_info": {
                "from": "system_para_0",
                "condition_desc": "If case (2), write this sub-question with an [END] mark as a leaf node.",
                "complete_instruction_para": []
            },
            "dimension": "example_driven",
            "is_meta": false,
            "type": [
                "formatting"
            ],
            "evaluation": [
                {
                    "type": "llm_conditional_check",
                    "exec": "Does there exist sub-questions that satisfy case (2), which can be 'directly answered by analyzing the answers of at least two previously answered questions, such as comparing, judging, intersecting, or counting'? Please answer YES/NO directly and do not enter anything else.\n\nHere is model response: {response}"
                },
                {
                    "type": "llm",
                    "exec": "In the model response, are all sub-questions that satisfy case (2) written with an [END] mark as leaf nodes? Please answer YES/NO directly and do not enter anything else.\n\nHere is model response: {response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": -1,
            "desc": "use escape quotes \"\" to enclose work titles and function parameters",
            "other_info": {
                "from": "system_para_0",
                "condition_desc": "",
                "complete_instruction_para": []
            },
            "dimension": "example_driven",
            "is_meta": false,
            "type": [
                "formatting"
            ],
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "Did the model use escape quotes \"\" to enclose all work titles and function parameters? Please answer YES/NO directly and do not enter anything else.\n\nHere is model response: {response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": -1,
            "desc": "In case (2), write this sub-question with an [END] mark as a leaf node.",
            "other_info": {
                "from": "system_para_0",
                "condition_desc": "If case (2), write this sub-question with an [END] mark as a leaf node.",
                "complete_instruction_para": []
            },
            "dimension": "example_driven",
            "is_meta": false,
            "type": [
                "formatting"
            ],
            "evaluation": [
                {
                    "type": "llm_conditional_check",
                    "exec": "Does there exist sub-questions that satisfy case (2), which can be 'directly answered by analyzing the answers of at least two previously answered questions, such as comparing, judging, intersecting, or counting'? Please answer YES/NO directly and do not enter anything else.\n\nHere is model response: {response}"
                },
                {
                    "type": "llm",
                    "exec": "In the model response, are all sub-questions that satisfy case (2) written with an [END] mark as leaf nodes? Please answer YES/NO directly and do not enter anything else.\n\nHere is model response: {response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": -1,
            "desc": "use escape quotes \"\" to enclose work titles and function parameters",
            "other_info": {
                "from": "system_para_0",
                "condition_desc": "",
                "complete_instruction_para": []
            },
            "dimension": "example_driven",
            "is_meta": false,
            "type": [
                "formatting"
            ],
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "Did the model use escape quotes \"\" to enclose all work titles and function parameters? Please answer YES/NO directly and do not enter anything else.\n\nHere is model response: {response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": -1,
            "desc": "In case (2), write this sub-question with an [END] mark as a leaf node.",
            "other_info": {
                "from": "system_para_0",
                "condition_desc": "If case (2), write this sub-question with an [END] mark as a leaf node.",
                "complete_instruction_para": []
            },
            "dimension": "example_driven",
            "is_meta": false,
            "type": [
                "formatting"
            ],
            "evaluation": [
                {
                    "type": "llm_conditional_check",
                    "exec": "Does there exist sub-questions that satisfy case (2), which can be 'directly answered by analyzing the answers of at least two previously answered questions, such as comparing, judging, intersecting, or counting'? Please answer YES/NO directly and do not enter anything else.\n\nHere is model response: {response}"
                },
                {
                    "type": "llm",
                    "exec": "In the model response, are all sub-questions that satisfy case (2) written with an [END] mark as leaf nodes? Please answer YES/NO directly and do not enter anything else.\n\nHere is model response: {response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": -1,
            "desc": "use escape quotes \"\" to enclose work titles and function parameters",
            "other_info": {
                "from": "system_para_0",
                "condition_desc": "",
                "complete_instruction_para": []
            },
            "dimension": "example_driven",
            "is_meta": false,
            "type": [
                "formatting"
            ],
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "Did the model use escape quotes \"\" to enclose all work titles and function parameters? Please answer YES/NO directly and do not enter anything else.\n\nHere is model response: {response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": -1,
            "desc": "In case (2), write this sub-question with an [END] mark as a leaf node.",
            "other_info": {
                "from": "system_para_0",
                "condition_desc": "If case (2), write this sub-question with an [END] mark as a leaf node.",
                "complete_instruction_para": []
            },
            "dimension": "example_driven",
            "is_meta": false,
            "type": [
                "formatting"
            ],
            "evaluation": [
                {
                    "type": "llm_conditional_check",
                    "exec": "Does there exist sub-questions that satisfy case (2), which can be 'directly answered by analyzing the answers of at least two previously answered questions, such as comparing, judging, intersecting, or counting'? Please answer YES/NO directly and do not enter anything else.\n\nHere is model response: {response}"
                },
                {
                    "type": "llm",
                    "exec": "In the model response, are all sub-questions that satisfy case (2) written with an [END] mark as leaf nodes? Please answer YES/NO directly and do not enter anything else.\n\nHere is model response: {response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": -1,
            "desc": "For [END] leaf questions, format your question as 'Given answers of [q_idx_1] and [q_idx_2], ...', where [q_idx_1] and [q_idx_2] are question indices of the previously answered questions required to answer this [END] question.",
            "other_info": {
                "from": "system_para_0",
                "condition_desc": "If [END] leaf questions, format your question as 'Given answers of [q_idx_1] and [q_idx_2], ...', where [q_idx_1] and [q_idx_2] are question indices of the previously answered questions required to answer this [END] question..",
                "complete_instruction_para": []
            },
            "dimension": "example_driven",
            "is_meta": false,
            "type": [
                "formatting"
            ],
            "evaluation": [
                {
                    "type": "llm_conditional_check",
                    "exec": "Are there [END] leaf questions in the tree? Please answer YES/NO directly and do not enter anything else.\n\nHere is model response: {response}"
                },
                {
                    "type": "llm",
                    "exec": "Are all [END] leaf questions formatted as 'Given answers of [q_idx_1] and [q_idx_2], ...', where [q_idx_1] and [q_idx_2] are question indices of previously answered questions required to answer this [END] question? Please answer YES/NO directly and do not enter anything else.\n\nHere is model response: {response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": -1,
            "desc": "In case (2), write this sub-question with an [END] mark as a leaf node.",
            "other_info": {
                "from": "system_para_0",
                "condition_desc": "If case (2), write this sub-question with an [END] mark as a leaf node.",
                "complete_instruction_para": []
            },
            "dimension": "example_driven",
            "is_meta": false,
            "type": [
                "formatting"
            ],
            "evaluation": [
                {
                    "type": "llm_conditional_check",
                    "exec": "Does there exist sub-questions that satisfy case (2), which can be 'directly answered by analyzing the answers of at least two previously answered questions, such as comparing, judging, intersecting, or counting'? Please answer YES/NO directly and do not enter anything else.\n\nHere is model response: {response}"
                },
                {
                    "type": "llm",
                    "exec": "In the model response, are all sub-questions that satisfy case (2) written with an [END] mark as leaf nodes? Please answer YES/NO directly and do not enter anything else.\n\nHere is model response: {response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": -1,
            "desc": "In case (2), write this sub-question with an [END] mark as a leaf node.",
            "other_info": {
                "from": "system_para_0",
                "condition_desc": "If case (2), write this sub-question with an [END] mark as a leaf node.",
                "complete_instruction_para": []
            },
            "dimension": "example_driven",
            "is_meta": false,
            "type": [
                "formatting"
            ],
            "evaluation": [
                {
                    "type": "llm_conditional_check",
                    "exec": "Does there exist sub-questions that satisfy case (2), which can be 'directly answered by analyzing the answers of at least two previously answered questions, such as comparing, judging, intersecting, or counting'? Please answer YES/NO directly and do not enter anything else.\n\nHere is model response: {response}"
                },
                {
                    "type": "llm",
                    "exec": "In the model response, are all sub-questions that satisfy case (2) written with an [END] mark as leaf nodes? Please answer YES/NO directly and do not enter anything else.\n\nHere is model response: {response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        }
    ]
}