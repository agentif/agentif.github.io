{
    "unconditional": [
        {
            "id": 0,
            "desc": "您可以使用一系列functions。",
            "other_info": {
                "function": [
                    "get_company_info",
                    "get_sub_company_info",
                    "get_company_register",
                    "get_sub_company_info_list",
                    "get_company_register_name",
                    "finish"
                ],
                "from": "system_para_1"
            },
            "dimension": "unconditional",
            "type": "resource",
            "is_meta": false,
            "evaluation": [
                {
                    "type": "llm",
                    "required_keys": [
                        "response"
                    ],
                    "exec": "请帮我提取出下面这段话中**当前步**使用的函数名称，不关注未来要做的事。\n请仅回答函数名称，并以list形式直接返回给我，不要回答其他的内容。\n\n{response}\n"
                },
                {
                    "type": "code",
                    "exec": "\ndef check_following(response):\n    import re\n\n    def extract_tool(text):\n        pattern = r'\\[([^]]+)\\]'\n        matches = re.findall(pattern, text)\n        if not matches:\n            return []\n        try:\n            tools = eval(f\"[{matches[-1]}]\")\n            return tools\n        except Exception as e:\n            # print(\"error in extract_tool:\", e)\n            return []\n\n    try:\n        tools = ['get_company_info', 'get_sub_company_info', 'get_company_register', 'get_sub_company_info_list', 'get_company_register_name', 'finish']\n        functions = extract_tool(response)\n        if isinstance(functions, list):\n            for item in functions:\n                if item not in tools:\n                    return False\n        return True\n    except Exception as e:\n        print(\"error in resource checker:\", e)\n        return False\n"
                }
            ],
            "score": false
        },
        {
            "id": 1,
            "desc": "注意不要在一次thought中连续使用多个functions。",
            "other_info": {
                "function": [
                    "get_company_info",
                    "get_sub_company_info",
                    "get_company_register",
                    "get_sub_company_info_list",
                    "get_company_register_name",
                    "finish"
                ],
                "from": "system_para_3"
            },
            "dimension": "unconditional",
            "type": "resource",
            "is_meta": false,
            "evaluation": [
                {
                    "type": "llm",
                    "required_keys": [
                        "response"
                    ],
                    "exec": "请帮我提取出下面这段话中**当前步**使用的函数名称，不关注未来要做的事。\n请仅回答函数名称，并以list形式直接返回给我，不要回答其他的内容。\n\n{response}\n"
                },
                {
                    "type": "code",
                    "exec": "\ndef check_following(response):\n    import re\n\n    def extract_tool(text):\n        pattern = r'\\[([^]]+)\\]'\n        matches = re.findall(pattern, text)\n        if not matches:\n            return []\n        try:\n            tools = eval(f\"[{matches[-1]}]\")\n            return tools\n        except Exception as e:\n            # print(\"error in extract_tool:\", e)\n            return []\n\n    try:\n        tools = ['get_company_info', 'get_sub_company_info', 'get_company_register', 'get_sub_company_info_list', 'get_company_register_name', 'finish']\n        functions = extract_tool(response)\n        if len(functions) <= 1:\n            return True\n        return False\n    except Exception as e:\n        print(\"error in resource checker count:\", e)\n        return False\n"
                }
            ],
            "score": false
        },
        {
            "id": 3,
            "desc": "</thought> 结束",
            "other_info": {
                "from": "system_para_1"
            },
            "dimension": "unconditional",
            "type": "formatting",
            "is_meta": false,
            "evaluation": [
                {
                    "type": "code",
                    "exec": "\ndef check_following(response):\n    return response.rstrip().endswith(\"</thought>\")\n"
                }
            ],
            "score": false
        },
        {
            "id": 5,
            "desc": "您可以在thought中使用python packages,但只能从以下packages列表中选择：",
            "other_info": {
                "available_packages": [
                    "math",
                    "time",
                    "stat",
                    "queue",
                    "itertools",
                    "statistics",
                    "random",
                    "collections",
                    "unicodedata"
                ]
            },
            "dimension": "unconditional",
            "type": "resource",
            "is_meta": false,
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "提取下面文本中的所有的调用package，直接返回list格式结果，不要任何解释、前缀或赋值语句。\n输出格式要求：[\"package1\", \"package2\", ...]:\n\nHere is model response: {response}"
                },
                {
                    "type": "code",
                    "exec": "\nimport ast\n\ndef check_following(response: str) -> bool:\n    available_packages = ['math', 'time', 'stat', 'queue', 'itertools', 'statistics', 'random', 'collections', 'unicodedata']\n    try:\n        items = ast.literal_eval(response)\n        return isinstance(items, list) and all(item in available_packages for item in items)\n    except:\n        return False\n"
                }
            ],
            "score": false
        },
        {
            "id": 4,
            "desc": "除了第一轮外，对上一轮的<observation></observation>进行总结。",
            "other_info": {
                "from": "system_para_2"
            },
            "dimension": "unconditional",
            "type": "semantic",
            "is_meta": false,
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "response是否在一开始就总结了上一轮的<observation></observation>？直接回答 YES 或 NO。\n\nresponse: {response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": 3,
            "desc": "</thought> 结束",
            "other_info": {
                "from": "system_para_1"
            },
            "dimension": "unconditional",
            "type": "formatting",
            "is_meta": false,
            "evaluation": [
                {
                    "type": "code",
                    "exec": "\ndef check_following(response):\n    return response.rstrip().endswith(\"</thought>\")\n"
                }
            ],
            "score": false
        },
        {
            "id": 5,
            "desc": "您可以在thought中使用python packages,但只能从以下packages列表中选择：",
            "other_info": {
                "available_packages": [
                    "math",
                    "time",
                    "stat",
                    "queue",
                    "itertools",
                    "statistics",
                    "random",
                    "collections",
                    "unicodedata"
                ]
            },
            "dimension": "unconditional",
            "type": "resource",
            "is_meta": false,
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "提取下面文本中的所有的调用package，直接返回list格式结果，不要任何解释、前缀或赋值语句。\n输出格式要求：[\"package1\", \"package2\", ...]:\n\nHere is model response: {response}"
                },
                {
                    "type": "code",
                    "exec": "\nimport ast\n\ndef check_following(response: str) -> bool:\n    available_packages = ['math', 'time', 'stat', 'queue', 'itertools', 'statistics', 'random', 'collections', 'unicodedata']\n    try:\n        items = ast.literal_eval(response)\n        return isinstance(items, list) and all(item in available_packages for item in items)\n    except:\n        return False\n"
                }
            ],
            "score": false
        },
        {
            "id": 0,
            "desc": "您可以使用一系列functions。",
            "other_info": {
                "function": [
                    "get_company_register_name",
                    "get_company_register",
                    "get_company_info",
                    "get_sub_company_info",
                    "get_sub_company_info_list",
                    "finish"
                ],
                "from": "system_para_1"
            },
            "dimension": "unconditional",
            "type": "resource",
            "is_meta": false,
            "evaluation": [
                {
                    "type": "llm",
                    "required_keys": [
                        "response"
                    ],
                    "exec": "请帮我提取出下面这段话中**当前步**使用的函数名称，不关注未来要做的事。\n请仅回答函数名称，并以list形式直接返回给我，不要回答其他的内容。\n\n{response}\n"
                },
                {
                    "type": "code",
                    "exec": "\ndef check_following(response):\n    import re\n\n    def extract_tool(text):\n        pattern = r'\\[([^]]+)\\]'\n        matches = re.findall(pattern, text)\n        if not matches:\n            return []\n        try:\n            tools = eval(f\"[{matches[-1]}]\")\n            return tools\n        except Exception as e:\n            # print(\"error in extract_tool:\", e)\n            return []\n\n    try:\n        tools = ['get_company_register_name', 'get_company_register', 'get_company_info', 'get_sub_company_info', 'get_sub_company_info_list', 'finish']\n        functions = extract_tool(response)\n        if isinstance(functions, list):\n            for item in functions:\n                if item not in tools:\n                    return False\n        return True\n    except Exception as e:\n        print(\"error in resource checker:\", e)\n        return False\n"
                }
            ],
            "score": false
        },
        {
            "id": 1,
            "desc": "注意不要在一次thought中连续使用多个functions。",
            "other_info": {
                "function": [
                    "get_company_register_name",
                    "get_company_register",
                    "get_company_info",
                    "get_sub_company_info",
                    "get_sub_company_info_list",
                    "finish"
                ],
                "from": "system_para_3"
            },
            "dimension": "unconditional",
            "type": "resource",
            "is_meta": false,
            "evaluation": [
                {
                    "type": "llm",
                    "required_keys": [
                        "response"
                    ],
                    "exec": "请帮我提取出下面这段话中**当前步**使用的函数名称，不关注未来要做的事。\n请仅回答函数名称，并以list形式直接返回给我，不要回答其他的内容。\n\n{response}\n"
                },
                {
                    "type": "code",
                    "exec": "\ndef check_following(response):\n    import re\n\n    def extract_tool(text):\n        pattern = r'\\[([^]]+)\\]'\n        matches = re.findall(pattern, text)\n        if not matches:\n            return []\n        try:\n            tools = eval(f\"[{matches[-1]}]\")\n            return tools\n        except Exception as e:\n            # print(\"error in extract_tool:\", e)\n            return []\n\n    try:\n        tools = ['get_company_register_name', 'get_company_register', 'get_company_info', 'get_sub_company_info', 'get_sub_company_info_list', 'finish']\n        functions = extract_tool(response)\n        if len(functions) <= 1:\n            return True\n        return False\n    except Exception as e:\n        print(\"error in resource checker count:\", e)\n        return False\n"
                }
            ],
            "score": false
        },
        {
            "id": 3,
            "desc": "</thought> 结束",
            "other_info": {
                "from": "system_para_1"
            },
            "dimension": "unconditional",
            "type": "formatting",
            "is_meta": false,
            "evaluation": [
                {
                    "type": "code",
                    "exec": "\ndef check_following(response):\n    return response.rstrip().endswith(\"</thought>\")\n"
                }
            ],
            "score": false
        },
        {
            "id": 5,
            "desc": "您可以在thought中使用python packages,但只能从以下packages列表中选择：",
            "other_info": {
                "available_packages": [
                    "math",
                    "time",
                    "stat",
                    "queue",
                    "itertools",
                    "statistics",
                    "random",
                    "collections",
                    "unicodedata"
                ]
            },
            "dimension": "unconditional",
            "type": "resource",
            "is_meta": false,
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "提取下面文本中的所有的调用package，直接返回list格式结果，不要任何解释、前缀或赋值语句。\n输出格式要求：[\"package1\", \"package2\", ...]:\n\nHere is model response: {response}"
                },
                {
                    "type": "code",
                    "exec": "\nimport ast\n\ndef check_following(response: str) -> bool:\n    available_packages = ['math', 'time', 'stat', 'queue', 'itertools', 'statistics', 'random', 'collections', 'unicodedata']\n    try:\n        items = ast.literal_eval(response)\n        return isinstance(items, list) and all(item in available_packages for item in items)\n    except:\n        return False\n"
                }
            ],
            "score": false
        },
        {
            "id": 4,
            "desc": "除了第一轮外，对上一轮的<observation></observation>进行总结。",
            "other_info": {
                "from": "system_para_2"
            },
            "dimension": "unconditional",
            "type": "semantic",
            "is_meta": false,
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "response是否在一开始就总结了上一轮的<observation></observation>？直接回答 YES 或 NO。\n\nresponse: {response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": 3,
            "desc": "</thought> 结束",
            "other_info": {
                "from": "system_para_1"
            },
            "dimension": "unconditional",
            "type": "formatting",
            "is_meta": false,
            "evaluation": [
                {
                    "type": "code",
                    "exec": "\ndef check_following(response):\n    return response.rstrip().endswith(\"</thought>\")\n"
                }
            ],
            "score": false
        },
        {
            "id": 5,
            "desc": "您可以在thought中使用python packages,但只能从以下packages列表中选择：",
            "other_info": {
                "available_packages": [
                    "math",
                    "time",
                    "stat",
                    "queue",
                    "itertools",
                    "statistics",
                    "random",
                    "collections",
                    "unicodedata"
                ]
            },
            "dimension": "unconditional",
            "type": "resource",
            "is_meta": false,
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "提取下面文本中的所有的调用package，直接返回list格式结果，不要任何解释、前缀或赋值语句。\n输出格式要求：[\"package1\", \"package2\", ...]:\n\nHere is model response: {response}"
                },
                {
                    "type": "code",
                    "exec": "\nimport ast\n\ndef check_following(response: str) -> bool:\n    available_packages = ['math', 'time', 'stat', 'queue', 'itertools', 'statistics', 'random', 'collections', 'unicodedata']\n    try:\n        items = ast.literal_eval(response)\n        return isinstance(items, list) and all(item in available_packages for item in items)\n    except:\n        return False\n"
                }
            ],
            "score": false
        },
        {
            "id": 3,
            "desc": "</thought> 结束",
            "other_info": {
                "from": "system_para_1"
            },
            "dimension": "unconditional",
            "type": "formatting",
            "is_meta": false,
            "evaluation": [
                {
                    "type": "code",
                    "exec": "\ndef check_following(response):\n    return response.rstrip().endswith(\"</thought>\")\n"
                }
            ],
            "score": false
        },
        {
            "id": 5,
            "desc": "您可以在thought中使用python packages,但只能从以下packages列表中选择：",
            "other_info": {
                "available_packages": [
                    "math",
                    "time",
                    "stat",
                    "queue",
                    "itertools",
                    "statistics",
                    "random",
                    "collections",
                    "unicodedata"
                ]
            },
            "dimension": "unconditional",
            "type": "resource",
            "is_meta": false,
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "提取下面文本中的所有的调用package，直接返回list格式结果，不要任何解释、前缀或赋值语句。\n输出格式要求：[\"package1\", \"package2\", ...]:\n\nHere is model response: {response}"
                },
                {
                    "type": "code",
                    "exec": "\nimport ast\n\ndef check_following(response: str) -> bool:\n    available_packages = ['math', 'time', 'stat', 'queue', 'itertools', 'statistics', 'random', 'collections', 'unicodedata']\n    try:\n        items = ast.literal_eval(response)\n        return isinstance(items, list) and all(item in available_packages for item in items)\n    except:\n        return False\n"
                }
            ],
            "score": false
        },
        {
            "id": 4,
            "desc": "除了第一轮外，对上一轮的<observation></observation>进行总结。",
            "other_info": {
                "from": "system_para_2"
            },
            "dimension": "unconditional",
            "type": "semantic",
            "is_meta": false,
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "response是否在一开始就总结了上一轮的<observation></observation>？直接回答 YES 或 NO。\n\nresponse: {response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": 3,
            "desc": "</thought> 结束",
            "other_info": {
                "from": "system_para_1"
            },
            "dimension": "unconditional",
            "type": "formatting",
            "is_meta": false,
            "evaluation": [
                {
                    "type": "code",
                    "exec": "\ndef check_following(response):\n    return response.rstrip().endswith(\"</thought>\")\n"
                }
            ],
            "score": false
        },
        {
            "id": 5,
            "desc": "您可以在thought中使用python packages,但只能从以下packages列表中选择：",
            "other_info": {
                "available_packages": [
                    "math",
                    "time",
                    "stat",
                    "queue",
                    "itertools",
                    "statistics",
                    "random",
                    "collections",
                    "unicodedata"
                ]
            },
            "dimension": "unconditional",
            "type": "resource",
            "is_meta": false,
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "提取下面文本中的所有的调用package，直接返回list格式结果，不要任何解释、前缀或赋值语句。\n输出格式要求：[\"package1\", \"package2\", ...]:\n\nHere is model response: {response}"
                },
                {
                    "type": "code",
                    "exec": "\nimport ast\n\ndef check_following(response: str) -> bool:\n    available_packages = ['math', 'time', 'stat', 'queue', 'itertools', 'statistics', 'random', 'collections', 'unicodedata']\n    try:\n        items = ast.literal_eval(response)\n        return isinstance(items, list) and all(item in available_packages for item in items)\n    except:\n        return False\n"
                }
            ],
            "score": false
        },
        {
            "id": 3,
            "desc": "</thought> 结束",
            "other_info": {
                "from": "system_para_1"
            },
            "dimension": "unconditional",
            "type": "formatting",
            "is_meta": false,
            "evaluation": [
                {
                    "type": "code",
                    "exec": "\ndef check_following(response):\n    return response.rstrip().endswith(\"</thought>\")\n"
                }
            ],
            "score": false
        },
        {
            "id": 5,
            "desc": "您可以在thought中使用python packages,但只能从以下packages列表中选择：",
            "other_info": {
                "available_packages": [
                    "math",
                    "time",
                    "stat",
                    "queue",
                    "itertools",
                    "statistics",
                    "random",
                    "collections",
                    "unicodedata"
                ]
            },
            "dimension": "unconditional",
            "type": "resource",
            "is_meta": false,
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "提取下面文本中的所有的调用package，直接返回list格式结果，不要任何解释、前缀或赋值语句。\n输出格式要求：[\"package1\", \"package2\", ...]:\n\nHere is model response: {response}"
                },
                {
                    "type": "code",
                    "exec": "\nimport ast\n\ndef check_following(response: str) -> bool:\n    available_packages = ['math', 'time', 'stat', 'queue', 'itertools', 'statistics', 'random', 'collections', 'unicodedata']\n    try:\n        items = ast.literal_eval(response)\n        return isinstance(items, list) and all(item in available_packages for item in items)\n    except:\n        return False\n"
                }
            ],
            "score": false
        },
        {
            "id": 3,
            "desc": "</thought> 结束",
            "other_info": {
                "from": "system_para_1"
            },
            "dimension": "unconditional",
            "type": "formatting",
            "is_meta": false,
            "evaluation": [
                {
                    "type": "code",
                    "exec": "\ndef check_following(response):\n    return response.rstrip().endswith(\"</thought>\")\n"
                }
            ],
            "score": false
        },
        {
            "id": 5,
            "desc": "您可以在thought中使用python packages,但只能从以下packages列表中选择：",
            "other_info": {
                "available_packages": [
                    "math",
                    "time",
                    "stat",
                    "queue",
                    "itertools",
                    "statistics",
                    "random",
                    "collections",
                    "unicodedata"
                ]
            },
            "dimension": "unconditional",
            "type": "resource",
            "is_meta": false,
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "提取下面文本中的所有的调用package，直接返回list格式结果，不要任何解释、前缀或赋值语句。\n输出格式要求：[\"package1\", \"package2\", ...]:\n\nHere is model response: {response}"
                },
                {
                    "type": "code",
                    "exec": "\nimport ast\n\ndef check_following(response: str) -> bool:\n    available_packages = ['math', 'time', 'stat', 'queue', 'itertools', 'statistics', 'random', 'collections', 'unicodedata']\n    try:\n        items = ast.literal_eval(response)\n        return isinstance(items, list) and all(item in available_packages for item in items)\n    except:\n        return False\n"
                }
            ],
            "score": false
        },
        {
            "id": 4,
            "desc": "除了第一轮外，对上一轮的<observation></observation>进行总结。",
            "other_info": {
                "from": "system_para_2"
            },
            "dimension": "unconditional",
            "type": "semantic",
            "is_meta": false,
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "response是否在一开始就总结了上一轮的<observation></observation>？直接回答 YES 或 NO。\n\nresponse: {response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": 3,
            "desc": "</thought> 结束",
            "other_info": {
                "from": "system_para_1"
            },
            "dimension": "unconditional",
            "type": "formatting",
            "is_meta": false,
            "evaluation": [
                {
                    "type": "code",
                    "exec": "\ndef check_following(response):\n    return response.rstrip().endswith(\"</thought>\")\n"
                }
            ],
            "score": false
        },
        {
            "id": 5,
            "desc": "您可以在thought中使用python packages,但只能从以下packages列表中选择：",
            "other_info": {
                "available_packages": [
                    "math",
                    "time",
                    "stat",
                    "queue",
                    "itertools",
                    "statistics",
                    "random",
                    "collections",
                    "unicodedata"
                ]
            },
            "dimension": "unconditional",
            "type": "resource",
            "is_meta": false,
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "提取下面文本中的所有的调用package，直接返回list格式结果，不要任何解释、前缀或赋值语句。\n输出格式要求：[\"package1\", \"package2\", ...]:\n\nHere is model response: {response}"
                },
                {
                    "type": "code",
                    "exec": "\nimport ast\n\ndef check_following(response: str) -> bool:\n    available_packages = ['math', 'time', 'stat', 'queue', 'itertools', 'statistics', 'random', 'collections', 'unicodedata']\n    try:\n        items = ast.literal_eval(response)\n        return isinstance(items, list) and all(item in available_packages for item in items)\n    except:\n        return False\n"
                }
            ],
            "score": false
        },
        {
            "id": 0,
            "desc": "您可以使用一系列functions。",
            "other_info": {
                "function": [
                    "get_sub_company_info",
                    "get_sub_company_info_list",
                    "get_company_info",
                    "get_address_info",
                    "get_company_register",
                    "finish"
                ],
                "from": "system_para_1"
            },
            "dimension": "unconditional",
            "type": "resource",
            "is_meta": false,
            "evaluation": [
                {
                    "type": "llm",
                    "required_keys": [
                        "response"
                    ],
                    "exec": "请帮我提取出下面这段话中**当前步**使用的函数名称，不关注未来要做的事。\n请仅回答函数名称，并以list形式直接返回给我，不要回答其他的内容。\n\n{response}\n"
                },
                {
                    "type": "code",
                    "exec": "\ndef check_following(response):\n    import re\n\n    def extract_tool(text):\n        pattern = r'\\[([^]]+)\\]'\n        matches = re.findall(pattern, text)\n        if not matches:\n            return []\n        try:\n            tools = eval(f\"[{matches[-1]}]\")\n            return tools\n        except Exception as e:\n            # print(\"error in extract_tool:\", e)\n            return []\n\n    try:\n        tools = ['get_sub_company_info', 'get_sub_company_info_list', 'get_company_info', 'get_address_info', 'get_company_register', 'finish']\n        functions = extract_tool(response)\n        if isinstance(functions, list):\n            for item in functions:\n                if item not in tools:\n                    return False\n        return True\n    except Exception as e:\n        print(\"error in resource checker:\", e)\n        return False\n"
                }
            ],
            "score": false
        },
        {
            "id": 1,
            "desc": "注意不要在一次thought中连续使用多个functions。",
            "other_info": {
                "function": [
                    "get_sub_company_info",
                    "get_sub_company_info_list",
                    "get_company_info",
                    "get_address_info",
                    "get_company_register",
                    "finish"
                ],
                "from": "system_para_3"
            },
            "dimension": "unconditional",
            "type": "resource",
            "is_meta": false,
            "evaluation": [
                {
                    "type": "llm",
                    "required_keys": [
                        "response"
                    ],
                    "exec": "请帮我提取出下面这段话中**当前步**使用的函数名称，不关注未来要做的事。\n请仅回答函数名称，并以list形式直接返回给我，不要回答其他的内容。\n\n{response}\n"
                },
                {
                    "type": "code",
                    "exec": "\ndef check_following(response):\n    import re\n\n    def extract_tool(text):\n        pattern = r'\\[([^]]+)\\]'\n        matches = re.findall(pattern, text)\n        if not matches:\n            return []\n        try:\n            tools = eval(f\"[{matches[-1]}]\")\n            return tools\n        except Exception as e:\n            # print(\"error in extract_tool:\", e)\n            return []\n\n    try:\n        tools = ['get_sub_company_info', 'get_sub_company_info_list', 'get_company_info', 'get_address_info', 'get_company_register', 'finish']\n        functions = extract_tool(response)\n        if len(functions) <= 1:\n            return True\n        return False\n    except Exception as e:\n        print(\"error in resource checker count:\", e)\n        return False\n"
                }
            ],
            "score": false
        },
        {
            "id": 3,
            "desc": "</thought> 结束",
            "other_info": {
                "from": "system_para_1"
            },
            "dimension": "unconditional",
            "type": "formatting",
            "is_meta": false,
            "evaluation": [
                {
                    "type": "code",
                    "exec": "\ndef check_following(response):\n    return response.rstrip().endswith(\"</thought>\")\n"
                }
            ],
            "score": false
        },
        {
            "id": 5,
            "desc": "您可以在thought中使用python packages,但只能从以下packages列表中选择：",
            "other_info": {
                "available_packages": [
                    "math",
                    "time",
                    "stat",
                    "queue",
                    "itertools",
                    "statistics",
                    "random",
                    "collections",
                    "unicodedata"
                ]
            },
            "dimension": "unconditional",
            "type": "resource",
            "is_meta": false,
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "提取下面文本中的所有的调用package，直接返回list格式结果，不要任何解释、前缀或赋值语句。\n输出格式要求：[\"package1\", \"package2\", ...]:\n\nHere is model response: {response}"
                },
                {
                    "type": "code",
                    "exec": "\nimport ast\n\ndef check_following(response: str) -> bool:\n    available_packages = ['math', 'time', 'stat', 'queue', 'itertools', 'statistics', 'random', 'collections', 'unicodedata']\n    try:\n        items = ast.literal_eval(response)\n        return isinstance(items, list) and all(item in available_packages for item in items)\n    except:\n        return False\n"
                }
            ],
            "score": false
        },
        {
            "id": 4,
            "desc": "除了第一轮外，对上一轮的<observation></observation>进行总结。",
            "other_info": {
                "from": "system_para_2"
            },
            "dimension": "unconditional",
            "type": "semantic",
            "is_meta": false,
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "response是否在一开始就总结了上一轮的<observation></observation>？直接回答 YES 或 NO。\n\nresponse: {response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": 3,
            "desc": "</thought> 结束",
            "other_info": {
                "from": "system_para_1"
            },
            "dimension": "unconditional",
            "type": "formatting",
            "is_meta": false,
            "evaluation": [
                {
                    "type": "code",
                    "exec": "\ndef check_following(response):\n    return response.rstrip().endswith(\"</thought>\")\n"
                }
            ],
            "score": false
        },
        {
            "id": 5,
            "desc": "您可以在thought中使用python packages,但只能从以下packages列表中选择：",
            "other_info": {
                "available_packages": [
                    "math",
                    "time",
                    "stat",
                    "queue",
                    "itertools",
                    "statistics",
                    "random",
                    "collections",
                    "unicodedata"
                ]
            },
            "dimension": "unconditional",
            "type": "resource",
            "is_meta": false,
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "提取下面文本中的所有的调用package，直接返回list格式结果，不要任何解释、前缀或赋值语句。\n输出格式要求：[\"package1\", \"package2\", ...]:\n\nHere is model response: {response}"
                },
                {
                    "type": "code",
                    "exec": "\nimport ast\n\ndef check_following(response: str) -> bool:\n    available_packages = ['math', 'time', 'stat', 'queue', 'itertools', 'statistics', 'random', 'collections', 'unicodedata']\n    try:\n        items = ast.literal_eval(response)\n        return isinstance(items, list) and all(item in available_packages for item in items)\n    except:\n        return False\n"
                }
            ],
            "score": false
        },
        {
            "id": 3,
            "desc": "</thought> 结束",
            "other_info": {
                "from": "system_para_1"
            },
            "dimension": "unconditional",
            "type": "formatting",
            "is_meta": false,
            "evaluation": [
                {
                    "type": "code",
                    "exec": "\ndef check_following(response):\n    return response.rstrip().endswith(\"</thought>\")\n"
                }
            ],
            "score": false
        },
        {
            "id": 5,
            "desc": "您可以在thought中使用python packages,但只能从以下packages列表中选择：",
            "other_info": {
                "available_packages": [
                    "math",
                    "time",
                    "stat",
                    "queue",
                    "itertools",
                    "statistics",
                    "random",
                    "collections",
                    "unicodedata"
                ]
            },
            "dimension": "unconditional",
            "type": "resource",
            "is_meta": false,
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "提取下面文本中的所有的调用package，直接返回list格式结果，不要任何解释、前缀或赋值语句。\n输出格式要求：[\"package1\", \"package2\", ...]:\n\nHere is model response: {response}"
                },
                {
                    "type": "code",
                    "exec": "\nimport ast\n\ndef check_following(response: str) -> bool:\n    available_packages = ['math', 'time', 'stat', 'queue', 'itertools', 'statistics', 'random', 'collections', 'unicodedata']\n    try:\n        items = ast.literal_eval(response)\n        return isinstance(items, list) and all(item in available_packages for item in items)\n    except:\n        return False\n"
                }
            ],
            "score": false
        },
        {
            "id": 4,
            "desc": "除了第一轮外，对上一轮的<observation></observation>进行总结。",
            "other_info": {
                "from": "system_para_2"
            },
            "dimension": "unconditional",
            "type": "semantic",
            "is_meta": false,
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "response是否在一开始就总结了上一轮的<observation></observation>？直接回答 YES 或 NO。\n\nresponse: {response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": 3,
            "desc": "</thought> 结束",
            "other_info": {
                "from": "system_para_1"
            },
            "dimension": "unconditional",
            "type": "formatting",
            "is_meta": false,
            "evaluation": [
                {
                    "type": "code",
                    "exec": "\ndef check_following(response):\n    return response.rstrip().endswith(\"</thought>\")\n"
                }
            ],
            "score": false
        },
        {
            "id": 5,
            "desc": "您可以在thought中使用python packages,但只能从以下packages列表中选择：",
            "other_info": {
                "available_packages": [
                    "math",
                    "time",
                    "stat",
                    "queue",
                    "itertools",
                    "statistics",
                    "random",
                    "collections",
                    "unicodedata"
                ]
            },
            "dimension": "unconditional",
            "type": "resource",
            "is_meta": false,
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "提取下面文本中的所有的调用package，直接返回list格式结果，不要任何解释、前缀或赋值语句。\n输出格式要求：[\"package1\", \"package2\", ...]:\n\nHere is model response: {response}"
                },
                {
                    "type": "code",
                    "exec": "\nimport ast\n\ndef check_following(response: str) -> bool:\n    available_packages = ['math', 'time', 'stat', 'queue', 'itertools', 'statistics', 'random', 'collections', 'unicodedata']\n    try:\n        items = ast.literal_eval(response)\n        return isinstance(items, list) and all(item in available_packages for item in items)\n    except:\n        return False\n"
                }
            ],
            "score": false
        },
        {
            "id": 3,
            "desc": "</thought> 结束",
            "other_info": {
                "from": "system_para_1"
            },
            "dimension": "unconditional",
            "type": "formatting",
            "is_meta": false,
            "evaluation": [
                {
                    "type": "code",
                    "exec": "\ndef check_following(response):\n    return response.rstrip().endswith(\"</thought>\")\n"
                }
            ],
            "score": false
        },
        {
            "id": 5,
            "desc": "您可以在thought中使用python packages,但只能从以下packages列表中选择：",
            "other_info": {
                "available_packages": [
                    "math",
                    "time",
                    "stat",
                    "queue",
                    "itertools",
                    "statistics",
                    "random",
                    "collections",
                    "unicodedata"
                ]
            },
            "dimension": "unconditional",
            "type": "resource",
            "is_meta": false,
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "提取下面文本中的所有的调用package，直接返回list格式结果，不要任何解释、前缀或赋值语句。\n输出格式要求：[\"package1\", \"package2\", ...]:\n\nHere is model response: {response}"
                },
                {
                    "type": "code",
                    "exec": "\nimport ast\n\ndef check_following(response: str) -> bool:\n    available_packages = ['math', 'time', 'stat', 'queue', 'itertools', 'statistics', 'random', 'collections', 'unicodedata']\n    try:\n        items = ast.literal_eval(response)\n        return isinstance(items, list) and all(item in available_packages for item in items)\n    except:\n        return False\n"
                }
            ],
            "score": false
        },
        {
            "id": 4,
            "desc": "除了第一轮外，对上一轮的<observation></observation>进行总结。",
            "other_info": {
                "from": "system_para_2"
            },
            "dimension": "unconditional",
            "type": "semantic",
            "is_meta": false,
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "response是否在一开始就总结了上一轮的<observation></observation>？直接回答 YES 或 NO。\n\nresponse: {response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": 0,
            "desc": "您可以使用一系列functions。",
            "other_info": {
                "function": [
                    "get_company_register",
                    "get_company_register_name",
                    "get_company_info",
                    "get_sub_company_info",
                    "get_legal_document_company_list",
                    "finish"
                ],
                "from": "system_para_1"
            },
            "dimension": "unconditional",
            "type": "resource",
            "is_meta": false,
            "evaluation": [
                {
                    "type": "llm",
                    "required_keys": [
                        "response"
                    ],
                    "exec": "请帮我提取出下面这段话中**当前步**使用的函数名称，不关注未来要做的事。\n请仅回答函数名称，并以list形式直接返回给我，不要回答其他的内容。\n\n{response}\n"
                },
                {
                    "type": "code",
                    "exec": "\ndef check_following(response):\n    import re\n\n    def extract_tool(text):\n        pattern = r'\\[([^]]+)\\]'\n        matches = re.findall(pattern, text)\n        if not matches:\n            return []\n        try:\n            tools = eval(f\"[{matches[-1]}]\")\n            return tools\n        except Exception as e:\n            # print(\"error in extract_tool:\", e)\n            return []\n\n    try:\n        tools = ['get_company_register', 'get_company_register_name', 'get_company_info', 'get_sub_company_info', 'get_legal_document_company_list', 'finish']\n        functions = extract_tool(response)\n        if isinstance(functions, list):\n            for item in functions:\n                if item not in tools:\n                    return False\n        return True\n    except Exception as e:\n        print(\"error in resource checker:\", e)\n        return False\n"
                }
            ],
            "score": false
        },
        {
            "id": 1,
            "desc": "注意不要在一次thought中连续使用多个functions。",
            "other_info": {
                "function": [
                    "get_company_register",
                    "get_company_register_name",
                    "get_company_info",
                    "get_sub_company_info",
                    "get_legal_document_company_list",
                    "finish"
                ],
                "from": "system_para_3"
            },
            "dimension": "unconditional",
            "type": "resource",
            "is_meta": false,
            "evaluation": [
                {
                    "type": "llm",
                    "required_keys": [
                        "response"
                    ],
                    "exec": "请帮我提取出下面这段话中**当前步**使用的函数名称，不关注未来要做的事。\n请仅回答函数名称，并以list形式直接返回给我，不要回答其他的内容。\n\n{response}\n"
                },
                {
                    "type": "code",
                    "exec": "\ndef check_following(response):\n    import re\n\n    def extract_tool(text):\n        pattern = r'\\[([^]]+)\\]'\n        matches = re.findall(pattern, text)\n        if not matches:\n            return []\n        try:\n            tools = eval(f\"[{matches[-1]}]\")\n            return tools\n        except Exception as e:\n            # print(\"error in extract_tool:\", e)\n            return []\n\n    try:\n        tools = ['get_company_register', 'get_company_register_name', 'get_company_info', 'get_sub_company_info', 'get_legal_document_company_list', 'finish']\n        functions = extract_tool(response)\n        if len(functions) <= 1:\n            return True\n        return False\n    except Exception as e:\n        print(\"error in resource checker count:\", e)\n        return False\n"
                }
            ],
            "score": false
        },
        {
            "id": 3,
            "desc": "</thought> 结束",
            "other_info": {
                "from": "system_para_1"
            },
            "dimension": "unconditional",
            "type": "formatting",
            "is_meta": false,
            "evaluation": [
                {
                    "type": "code",
                    "exec": "\ndef check_following(response):\n    return response.rstrip().endswith(\"</thought>\")\n"
                }
            ],
            "score": false
        },
        {
            "id": 4,
            "desc": "除了第一轮外，对上一轮的<observation></observation>进行总结。",
            "other_info": {
                "from": "system_para_2"
            },
            "dimension": "unconditional",
            "type": "semantic",
            "is_meta": false,
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "response是否在一开始就总结了上一轮的<observation></observation>？直接回答 YES 或 NO。\n\nresponse: {response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": 3,
            "desc": "</thought> 结束",
            "other_info": {
                "from": "system_para_1"
            },
            "dimension": "unconditional",
            "type": "formatting",
            "is_meta": false,
            "evaluation": [
                {
                    "type": "code",
                    "exec": "\ndef check_following(response):\n    return response.rstrip().endswith(\"</thought>\")\n"
                }
            ],
            "score": false
        },
        {
            "id": 5,
            "desc": "您可以在thought中使用python packages,但只能从以下packages列表中选择：",
            "other_info": {
                "available_packages": [
                    "math",
                    "time",
                    "stat",
                    "queue",
                    "itertools",
                    "statistics",
                    "random",
                    "collections",
                    "unicodedata"
                ]
            },
            "dimension": "unconditional",
            "type": "resource",
            "is_meta": false,
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "提取下面文本中的所有的调用package，直接返回list格式结果，不要任何解释、前缀或赋值语句。\n输出格式要求：[\"package1\", \"package2\", ...]:\n\nHere is model response: {response}"
                },
                {
                    "type": "code",
                    "exec": "\nimport ast\n\ndef check_following(response: str) -> bool:\n    available_packages = ['math', 'time', 'stat', 'queue', 'itertools', 'statistics', 'random', 'collections', 'unicodedata']\n    try:\n        items = ast.literal_eval(response)\n        return isinstance(items, list) and all(item in available_packages for item in items)\n    except:\n        return False\n"
                }
            ],
            "score": false
        },
        {
            "id": 0,
            "desc": "您可以使用一系列functions。",
            "other_info": {
                "function": [
                    "get_sub_company_info",
                    "get_sub_company_info_list",
                    "get_company_info",
                    "get_company_register",
                    "get_address_info",
                    "finish"
                ],
                "from": "system_para_1"
            },
            "dimension": "unconditional",
            "type": "resource",
            "is_meta": false,
            "evaluation": [
                {
                    "type": "llm",
                    "required_keys": [
                        "response"
                    ],
                    "exec": "请帮我提取出下面这段话中**当前步**使用的函数名称，不关注未来要做的事。\n请仅回答函数名称，并以list形式直接返回给我，不要回答其他的内容。\n\n{response}\n"
                },
                {
                    "type": "code",
                    "exec": "\ndef check_following(response):\n    import re\n\n    def extract_tool(text):\n        pattern = r'\\[([^]]+)\\]'\n        matches = re.findall(pattern, text)\n        if not matches:\n            return []\n        try:\n            tools = eval(f\"[{matches[-1]}]\")\n            return tools\n        except Exception as e:\n            # print(\"error in extract_tool:\", e)\n            return []\n\n    try:\n        tools = ['get_sub_company_info', 'get_sub_company_info_list', 'get_company_info', 'get_company_register', 'get_address_info', 'finish']\n        functions = extract_tool(response)\n        if isinstance(functions, list):\n            for item in functions:\n                if item not in tools:\n                    return False\n        return True\n    except Exception as e:\n        print(\"error in resource checker:\", e)\n        return False\n"
                }
            ],
            "score": false
        },
        {
            "id": 1,
            "desc": "注意不要在一次thought中连续使用多个functions。",
            "other_info": {
                "function": [
                    "get_sub_company_info",
                    "get_sub_company_info_list",
                    "get_company_info",
                    "get_company_register",
                    "get_address_info",
                    "finish"
                ],
                "from": "system_para_3"
            },
            "dimension": "unconditional",
            "type": "resource",
            "is_meta": false,
            "evaluation": [
                {
                    "type": "llm",
                    "required_keys": [
                        "response"
                    ],
                    "exec": "请帮我提取出下面这段话中**当前步**使用的函数名称，不关注未来要做的事。\n请仅回答函数名称，并以list形式直接返回给我，不要回答其他的内容。\n\n{response}\n"
                },
                {
                    "type": "code",
                    "exec": "\ndef check_following(response):\n    import re\n\n    def extract_tool(text):\n        pattern = r'\\[([^]]+)\\]'\n        matches = re.findall(pattern, text)\n        if not matches:\n            return []\n        try:\n            tools = eval(f\"[{matches[-1]}]\")\n            return tools\n        except Exception as e:\n            # print(\"error in extract_tool:\", e)\n            return []\n\n    try:\n        tools = ['get_sub_company_info', 'get_sub_company_info_list', 'get_company_info', 'get_company_register', 'get_address_info', 'finish']\n        functions = extract_tool(response)\n        if len(functions) <= 1:\n            return True\n        return False\n    except Exception as e:\n        print(\"error in resource checker count:\", e)\n        return False\n"
                }
            ],
            "score": false
        },
        {
            "id": 3,
            "desc": "</thought> 结束",
            "other_info": {
                "from": "system_para_1"
            },
            "dimension": "unconditional",
            "type": "formatting",
            "is_meta": false,
            "evaluation": [
                {
                    "type": "code",
                    "exec": "\ndef check_following(response):\n    return response.rstrip().endswith(\"</thought>\")\n"
                }
            ],
            "score": false
        },
        {
            "id": 5,
            "desc": "您可以在thought中使用python packages,但只能从以下packages列表中选择：",
            "other_info": {
                "available_packages": [
                    "math",
                    "time",
                    "stat",
                    "queue",
                    "itertools",
                    "statistics",
                    "random",
                    "collections",
                    "unicodedata"
                ]
            },
            "dimension": "unconditional",
            "type": "resource",
            "is_meta": false,
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "提取下面文本中的所有的调用package，直接返回list格式结果，不要任何解释、前缀或赋值语句。\n输出格式要求：[\"package1\", \"package2\", ...]:\n\nHere is model response: {response}"
                },
                {
                    "type": "code",
                    "exec": "\nimport ast\n\ndef check_following(response: str) -> bool:\n    available_packages = ['math', 'time', 'stat', 'queue', 'itertools', 'statistics', 'random', 'collections', 'unicodedata']\n    try:\n        items = ast.literal_eval(response)\n        return isinstance(items, list) and all(item in available_packages for item in items)\n    except:\n        return False\n"
                }
            ],
            "score": false
        },
        {
            "id": 4,
            "desc": "除了第一轮外，对上一轮的<observation></observation>进行总结。",
            "other_info": {
                "from": "system_para_2"
            },
            "dimension": "unconditional",
            "type": "semantic",
            "is_meta": false,
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "response是否在一开始就总结了上一轮的<observation></observation>？直接回答 YES 或 NO。\n\nresponse: {response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": 3,
            "desc": "</thought> 结束",
            "other_info": {
                "from": "system_para_1"
            },
            "dimension": "unconditional",
            "type": "formatting",
            "is_meta": false,
            "evaluation": [
                {
                    "type": "code",
                    "exec": "\ndef check_following(response):\n    return response.rstrip().endswith(\"</thought>\")\n"
                }
            ],
            "score": false
        },
        {
            "id": 5,
            "desc": "您可以在thought中使用python packages,但只能从以下packages列表中选择：",
            "other_info": {
                "available_packages": [
                    "math",
                    "time",
                    "stat",
                    "queue",
                    "itertools",
                    "statistics",
                    "random",
                    "collections",
                    "unicodedata"
                ]
            },
            "dimension": "unconditional",
            "type": "resource",
            "is_meta": false,
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "提取下面文本中的所有的调用package，直接返回list格式结果，不要任何解释、前缀或赋值语句。\n输出格式要求：[\"package1\", \"package2\", ...]:\n\nHere is model response: {response}"
                },
                {
                    "type": "code",
                    "exec": "\nimport ast\n\ndef check_following(response: str) -> bool:\n    available_packages = ['math', 'time', 'stat', 'queue', 'itertools', 'statistics', 'random', 'collections', 'unicodedata']\n    try:\n        items = ast.literal_eval(response)\n        return isinstance(items, list) and all(item in available_packages for item in items)\n    except:\n        return False\n"
                }
            ],
            "score": false
        },
        {
            "id": 3,
            "desc": "</thought> 结束",
            "other_info": {
                "from": "system_para_1"
            },
            "dimension": "unconditional",
            "type": "formatting",
            "is_meta": false,
            "evaluation": [
                {
                    "type": "code",
                    "exec": "\ndef check_following(response):\n    return response.rstrip().endswith(\"</thought>\")\n"
                }
            ],
            "score": false
        },
        {
            "id": 3,
            "desc": "</thought> 结束",
            "other_info": {
                "from": "system_para_1"
            },
            "dimension": "unconditional",
            "type": "formatting",
            "is_meta": false,
            "evaluation": [
                {
                    "type": "code",
                    "exec": "\ndef check_following(response):\n    return response.rstrip().endswith(\"</thought>\")\n"
                }
            ],
            "score": false
        },
        {
            "id": 5,
            "desc": "您可以在thought中使用python packages,但只能从以下packages列表中选择：",
            "other_info": {
                "available_packages": [
                    "math",
                    "time",
                    "stat",
                    "queue",
                    "itertools",
                    "statistics",
                    "random",
                    "collections",
                    "unicodedata"
                ]
            },
            "dimension": "unconditional",
            "type": "resource",
            "is_meta": false,
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "提取下面文本中的所有的调用package，直接返回list格式结果，不要任何解释、前缀或赋值语句。\n输出格式要求：[\"package1\", \"package2\", ...]:\n\nHere is model response: {response}"
                },
                {
                    "type": "code",
                    "exec": "\nimport ast\n\ndef check_following(response: str) -> bool:\n    available_packages = ['math', 'time', 'stat', 'queue', 'itertools', 'statistics', 'random', 'collections', 'unicodedata']\n    try:\n        items = ast.literal_eval(response)\n        return isinstance(items, list) and all(item in available_packages for item in items)\n    except:\n        return False\n"
                }
            ],
            "score": false
        },
        {
            "id": 3,
            "desc": "</thought> 结束",
            "other_info": {
                "from": "system_para_1"
            },
            "dimension": "unconditional",
            "type": "formatting",
            "is_meta": false,
            "evaluation": [
                {
                    "type": "code",
                    "exec": "\ndef check_following(response):\n    return response.rstrip().endswith(\"</thought>\")\n"
                }
            ],
            "score": false
        },
        {
            "id": 5,
            "desc": "您可以在thought中使用python packages,但只能从以下packages列表中选择：",
            "other_info": {
                "available_packages": [
                    "math",
                    "time",
                    "stat",
                    "queue",
                    "itertools",
                    "statistics",
                    "random",
                    "collections",
                    "unicodedata"
                ]
            },
            "dimension": "unconditional",
            "type": "resource",
            "is_meta": false,
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "提取下面文本中的所有的调用package，直接返回list格式结果，不要任何解释、前缀或赋值语句。\n输出格式要求：[\"package1\", \"package2\", ...]:\n\nHere is model response: {response}"
                },
                {
                    "type": "code",
                    "exec": "\nimport ast\n\ndef check_following(response: str) -> bool:\n    available_packages = ['math', 'time', 'stat', 'queue', 'itertools', 'statistics', 'random', 'collections', 'unicodedata']\n    try:\n        items = ast.literal_eval(response)\n        return isinstance(items, list) and all(item in available_packages for item in items)\n    except:\n        return False\n"
                }
            ],
            "score": false
        },
        {
            "id": 3,
            "desc": "</thought> 结束",
            "other_info": {
                "from": "system_para_1"
            },
            "dimension": "unconditional",
            "type": "formatting",
            "is_meta": false,
            "evaluation": [
                {
                    "type": "code",
                    "exec": "\ndef check_following(response):\n    return response.rstrip().endswith(\"</thought>\")\n"
                }
            ],
            "score": false
        },
        {
            "id": 5,
            "desc": "您可以在thought中使用python packages,但只能从以下packages列表中选择：",
            "other_info": {
                "available_packages": [
                    "math",
                    "time",
                    "stat",
                    "queue",
                    "itertools",
                    "statistics",
                    "random",
                    "collections",
                    "unicodedata"
                ]
            },
            "dimension": "unconditional",
            "type": "resource",
            "is_meta": false,
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "提取下面文本中的所有的调用package，直接返回list格式结果，不要任何解释、前缀或赋值语句。\n输出格式要求：[\"package1\", \"package2\", ...]:\n\nHere is model response: {response}"
                },
                {
                    "type": "code",
                    "exec": "\nimport ast\n\ndef check_following(response: str) -> bool:\n    available_packages = ['math', 'time', 'stat', 'queue', 'itertools', 'statistics', 'random', 'collections', 'unicodedata']\n    try:\n        items = ast.literal_eval(response)\n        return isinstance(items, list) and all(item in available_packages for item in items)\n    except:\n        return False\n"
                }
            ],
            "score": false
        },
        {
            "id": 3,
            "desc": "</thought> 结束",
            "other_info": {
                "from": "system_para_1"
            },
            "dimension": "unconditional",
            "type": "formatting",
            "is_meta": false,
            "evaluation": [
                {
                    "type": "code",
                    "exec": "\ndef check_following(response):\n    return response.rstrip().endswith(\"</thought>\")\n"
                }
            ],
            "score": false
        },
        {
            "id": 5,
            "desc": "您可以在thought中使用python packages,但只能从以下packages列表中选择：",
            "other_info": {
                "available_packages": [
                    "math",
                    "time",
                    "stat",
                    "queue",
                    "itertools",
                    "statistics",
                    "random",
                    "collections",
                    "unicodedata"
                ]
            },
            "dimension": "unconditional",
            "type": "resource",
            "is_meta": false,
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "提取下面文本中的所有的调用package，直接返回list格式结果，不要任何解释、前缀或赋值语句。\n输出格式要求：[\"package1\", \"package2\", ...]:\n\nHere is model response: {response}"
                },
                {
                    "type": "code",
                    "exec": "\nimport ast\n\ndef check_following(response: str) -> bool:\n    available_packages = ['math', 'time', 'stat', 'queue', 'itertools', 'statistics', 'random', 'collections', 'unicodedata']\n    try:\n        items = ast.literal_eval(response)\n        return isinstance(items, list) and all(item in available_packages for item in items)\n    except:\n        return False\n"
                }
            ],
            "score": false
        },
        {
            "id": 0,
            "desc": "您可以使用一系列functions。",
            "other_info": {
                "function": [
                    "get_sub_company_info_list",
                    "get_sub_company_info",
                    "get_company_info",
                    "get_company_register",
                    "get_court_info_list",
                    "finish"
                ],
                "from": "system_para_1"
            },
            "dimension": "unconditional",
            "type": "resource",
            "is_meta": false,
            "evaluation": [
                {
                    "type": "llm",
                    "required_keys": [
                        "response"
                    ],
                    "exec": "请帮我提取出下面这段话中**当前步**使用的函数名称，不关注未来要做的事。\n请仅回答函数名称，并以list形式直接返回给我，不要回答其他的内容。\n\n{response}\n"
                },
                {
                    "type": "code",
                    "exec": "\ndef check_following(response):\n    import re\n\n    def extract_tool(text):\n        pattern = r'\\[([^]]+)\\]'\n        matches = re.findall(pattern, text)\n        if not matches:\n            return []\n        try:\n            tools = eval(f\"[{matches[-1]}]\")\n            return tools\n        except Exception as e:\n            # print(\"error in extract_tool:\", e)\n            return []\n\n    try:\n        tools = ['get_sub_company_info_list', 'get_sub_company_info', 'get_company_info', 'get_company_register', 'get_court_info_list', 'finish']\n        functions = extract_tool(response)\n        if isinstance(functions, list):\n            for item in functions:\n                if item not in tools:\n                    return False\n        return True\n    except Exception as e:\n        print(\"error in resource checker:\", e)\n        return False\n"
                }
            ],
            "score": false
        },
        {
            "id": 1,
            "desc": "注意不要在一次thought中连续使用多个functions。",
            "other_info": {
                "function": [
                    "get_sub_company_info_list",
                    "get_sub_company_info",
                    "get_company_info",
                    "get_company_register",
                    "get_court_info_list",
                    "finish"
                ],
                "from": "system_para_3"
            },
            "dimension": "unconditional",
            "type": "resource",
            "is_meta": false,
            "evaluation": [
                {
                    "type": "llm",
                    "required_keys": [
                        "response"
                    ],
                    "exec": "请帮我提取出下面这段话中**当前步**使用的函数名称，不关注未来要做的事。\n请仅回答函数名称，并以list形式直接返回给我，不要回答其他的内容。\n\n{response}\n"
                },
                {
                    "type": "code",
                    "exec": "\ndef check_following(response):\n    import re\n\n    def extract_tool(text):\n        pattern = r'\\[([^]]+)\\]'\n        matches = re.findall(pattern, text)\n        if not matches:\n            return []\n        try:\n            tools = eval(f\"[{matches[-1]}]\")\n            return tools\n        except Exception as e:\n            # print(\"error in extract_tool:\", e)\n            return []\n\n    try:\n        tools = ['get_sub_company_info_list', 'get_sub_company_info', 'get_company_info', 'get_company_register', 'get_court_info_list', 'finish']\n        functions = extract_tool(response)\n        if len(functions) <= 1:\n            return True\n        return False\n    except Exception as e:\n        print(\"error in resource checker count:\", e)\n        return False\n"
                }
            ],
            "score": false
        },
        {
            "id": 3,
            "desc": "</thought> 结束",
            "other_info": {
                "from": "system_para_1"
            },
            "dimension": "unconditional",
            "type": "formatting",
            "is_meta": false,
            "evaluation": [
                {
                    "type": "code",
                    "exec": "\ndef check_following(response):\n    return response.rstrip().endswith(\"</thought>\")\n"
                }
            ],
            "score": false
        },
        {
            "id": 5,
            "desc": "您可以在thought中使用python packages,但只能从以下packages列表中选择：",
            "other_info": {
                "available_packages": [
                    "math",
                    "time",
                    "stat",
                    "queue",
                    "itertools",
                    "statistics",
                    "random",
                    "collections",
                    "unicodedata"
                ]
            },
            "dimension": "unconditional",
            "type": "resource",
            "is_meta": false,
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "提取下面文本中的所有的调用package，直接返回list格式结果，不要任何解释、前缀或赋值语句。\n输出格式要求：[\"package1\", \"package2\", ...]:\n\nHere is model response: {response}"
                },
                {
                    "type": "code",
                    "exec": "\nimport ast\n\ndef check_following(response: str) -> bool:\n    available_packages = ['math', 'time', 'stat', 'queue', 'itertools', 'statistics', 'random', 'collections', 'unicodedata']\n    try:\n        items = ast.literal_eval(response)\n        return isinstance(items, list) and all(item in available_packages for item in items)\n    except:\n        return False\n"
                }
            ],
            "score": false
        },
        {
            "id": 4,
            "desc": "除了第一轮外，对上一轮的<observation></observation>进行总结。",
            "other_info": {
                "from": "system_para_2"
            },
            "dimension": "unconditional",
            "type": "semantic",
            "is_meta": false,
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "response是否在一开始就总结了上一轮的<observation></observation>？直接回答 YES 或 NO。\n\nresponse: {response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": 0,
            "desc": "您可以使用一系列functions。",
            "other_info": {
                "function": [
                    "get_company_register",
                    "get_company_register_name",
                    "get_sub_company_info",
                    "get_company_info",
                    "get_lawfirm_info",
                    "finish"
                ],
                "from": "system_para_1"
            },
            "dimension": "unconditional",
            "type": "resource",
            "is_meta": false,
            "evaluation": [
                {
                    "type": "llm",
                    "required_keys": [
                        "response"
                    ],
                    "exec": "请帮我提取出下面这段话中**当前步**使用的函数名称，不关注未来要做的事。\n请仅回答函数名称，并以list形式直接返回给我，不要回答其他的内容。\n\n{response}\n"
                },
                {
                    "type": "code",
                    "exec": "\ndef check_following(response):\n    import re\n\n    def extract_tool(text):\n        pattern = r'\\[([^]]+)\\]'\n        matches = re.findall(pattern, text)\n        if not matches:\n            return []\n        try:\n            tools = eval(f\"[{matches[-1]}]\")\n            return tools\n        except Exception as e:\n            # print(\"error in extract_tool:\", e)\n            return []\n\n    try:\n        tools = ['get_company_register', 'get_company_register_name', 'get_sub_company_info', 'get_company_info', 'get_lawfirm_info', 'finish']\n        functions = extract_tool(response)\n        if isinstance(functions, list):\n            for item in functions:\n                if item not in tools:\n                    return False\n        return True\n    except Exception as e:\n        print(\"error in resource checker:\", e)\n        return False\n"
                }
            ],
            "score": false
        },
        {
            "id": 1,
            "desc": "注意不要在一次thought中连续使用多个functions。",
            "other_info": {
                "function": [
                    "get_company_register",
                    "get_company_register_name",
                    "get_sub_company_info",
                    "get_company_info",
                    "get_lawfirm_info",
                    "finish"
                ],
                "from": "system_para_3"
            },
            "dimension": "unconditional",
            "type": "resource",
            "is_meta": false,
            "evaluation": [
                {
                    "type": "llm",
                    "required_keys": [
                        "response"
                    ],
                    "exec": "请帮我提取出下面这段话中**当前步**使用的函数名称，不关注未来要做的事。\n请仅回答函数名称，并以list形式直接返回给我，不要回答其他的内容。\n\n{response}\n"
                },
                {
                    "type": "code",
                    "exec": "\ndef check_following(response):\n    import re\n\n    def extract_tool(text):\n        pattern = r'\\[([^]]+)\\]'\n        matches = re.findall(pattern, text)\n        if not matches:\n            return []\n        try:\n            tools = eval(f\"[{matches[-1]}]\")\n            return tools\n        except Exception as e:\n            # print(\"error in extract_tool:\", e)\n            return []\n\n    try:\n        tools = ['get_company_register', 'get_company_register_name', 'get_sub_company_info', 'get_company_info', 'get_lawfirm_info', 'finish']\n        functions = extract_tool(response)\n        if len(functions) <= 1:\n            return True\n        return False\n    except Exception as e:\n        print(\"error in resource checker count:\", e)\n        return False\n"
                }
            ],
            "score": false
        },
        {
            "id": 3,
            "desc": "</thought> 结束",
            "other_info": {
                "from": "system_para_1"
            },
            "dimension": "unconditional",
            "type": "formatting",
            "is_meta": false,
            "evaluation": [
                {
                    "type": "code",
                    "exec": "\ndef check_following(response):\n    return response.rstrip().endswith(\"</thought>\")\n"
                }
            ],
            "score": false
        },
        {
            "id": 5,
            "desc": "您可以在thought中使用python packages,但只能从以下packages列表中选择：",
            "other_info": {
                "available_packages": [
                    "math",
                    "time",
                    "stat",
                    "queue",
                    "itertools",
                    "statistics",
                    "random",
                    "collections",
                    "unicodedata"
                ]
            },
            "dimension": "unconditional",
            "type": "resource",
            "is_meta": false,
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "提取下面文本中的所有的调用package，直接返回list格式结果，不要任何解释、前缀或赋值语句。\n输出格式要求：[\"package1\", \"package2\", ...]:\n\nHere is model response: {response}"
                },
                {
                    "type": "code",
                    "exec": "\nimport ast\n\ndef check_following(response: str) -> bool:\n    available_packages = ['math', 'time', 'stat', 'queue', 'itertools', 'statistics', 'random', 'collections', 'unicodedata']\n    try:\n        items = ast.literal_eval(response)\n        return isinstance(items, list) and all(item in available_packages for item in items)\n    except:\n        return False\n"
                }
            ],
            "score": false
        },
        {
            "id": 4,
            "desc": "除了第一轮外，对上一轮的<observation></observation>进行总结。",
            "other_info": {
                "from": "system_para_2"
            },
            "dimension": "unconditional",
            "type": "semantic",
            "is_meta": false,
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "response是否在一开始就总结了上一轮的<observation></observation>？直接回答 YES 或 NO。\n\nresponse: {response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": 0,
            "desc": "您可以使用一系列functions。",
            "other_info": {
                "function": [
                    "get_sub_company_info_list",
                    "get_sub_company_info",
                    "get_company_info",
                    "get_court_info_list",
                    "get_company_register",
                    "finish"
                ],
                "from": "system_para_1"
            },
            "dimension": "unconditional",
            "type": "resource",
            "is_meta": false,
            "evaluation": [
                {
                    "type": "llm",
                    "required_keys": [
                        "response"
                    ],
                    "exec": "请帮我提取出下面这段话中**当前步**使用的函数名称，不关注未来要做的事。\n请仅回答函数名称，并以list形式直接返回给我，不要回答其他的内容。\n\n{response}\n"
                },
                {
                    "type": "code",
                    "exec": "\ndef check_following(response):\n    import re\n\n    def extract_tool(text):\n        pattern = r'\\[([^]]+)\\]'\n        matches = re.findall(pattern, text)\n        if not matches:\n            return []\n        try:\n            tools = eval(f\"[{matches[-1]}]\")\n            return tools\n        except Exception as e:\n            # print(\"error in extract_tool:\", e)\n            return []\n\n    try:\n        tools = ['get_sub_company_info_list', 'get_sub_company_info', 'get_company_info', 'get_court_info_list', 'get_company_register', 'finish']\n        functions = extract_tool(response)\n        if isinstance(functions, list):\n            for item in functions:\n                if item not in tools:\n                    return False\n        return True\n    except Exception as e:\n        print(\"error in resource checker:\", e)\n        return False\n"
                }
            ],
            "score": false
        },
        {
            "id": 1,
            "desc": "注意不要在一次thought中连续使用多个functions。",
            "other_info": {
                "function": [
                    "get_sub_company_info_list",
                    "get_sub_company_info",
                    "get_company_info",
                    "get_court_info_list",
                    "get_company_register",
                    "finish"
                ],
                "from": "system_para_3"
            },
            "dimension": "unconditional",
            "type": "resource",
            "is_meta": false,
            "evaluation": [
                {
                    "type": "llm",
                    "required_keys": [
                        "response"
                    ],
                    "exec": "请帮我提取出下面这段话中**当前步**使用的函数名称，不关注未来要做的事。\n请仅回答函数名称，并以list形式直接返回给我，不要回答其他的内容。\n\n{response}\n"
                },
                {
                    "type": "code",
                    "exec": "\ndef check_following(response):\n    import re\n\n    def extract_tool(text):\n        pattern = r'\\[([^]]+)\\]'\n        matches = re.findall(pattern, text)\n        if not matches:\n            return []\n        try:\n            tools = eval(f\"[{matches[-1]}]\")\n            return tools\n        except Exception as e:\n            # print(\"error in extract_tool:\", e)\n            return []\n\n    try:\n        tools = ['get_sub_company_info_list', 'get_sub_company_info', 'get_company_info', 'get_court_info_list', 'get_company_register', 'finish']\n        functions = extract_tool(response)\n        if len(functions) <= 1:\n            return True\n        return False\n    except Exception as e:\n        print(\"error in resource checker count:\", e)\n        return False\n"
                }
            ],
            "score": false
        },
        {
            "id": 3,
            "desc": "</thought> 结束",
            "other_info": {
                "from": "system_para_1"
            },
            "dimension": "unconditional",
            "type": "formatting",
            "is_meta": false,
            "evaluation": [
                {
                    "type": "code",
                    "exec": "\ndef check_following(response):\n    return response.rstrip().endswith(\"</thought>\")\n"
                }
            ],
            "score": false
        },
        {
            "id": 5,
            "desc": "您可以在thought中使用python packages,但只能从以下packages列表中选择：",
            "other_info": {
                "available_packages": [
                    "math",
                    "time",
                    "stat",
                    "queue",
                    "itertools",
                    "statistics",
                    "random",
                    "collections",
                    "unicodedata"
                ]
            },
            "dimension": "unconditional",
            "type": "resource",
            "is_meta": false,
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "提取下面文本中的所有的调用package，直接返回list格式结果，不要任何解释、前缀或赋值语句。\n输出格式要求：[\"package1\", \"package2\", ...]:\n\nHere is model response: {response}"
                },
                {
                    "type": "code",
                    "exec": "\nimport ast\n\ndef check_following(response: str) -> bool:\n    available_packages = ['math', 'time', 'stat', 'queue', 'itertools', 'statistics', 'random', 'collections', 'unicodedata']\n    try:\n        items = ast.literal_eval(response)\n        return isinstance(items, list) and all(item in available_packages for item in items)\n    except:\n        return False\n"
                }
            ],
            "score": false
        },
        {
            "id": 4,
            "desc": "除了第一轮外，对上一轮的<observation></observation>进行总结。",
            "other_info": {
                "from": "system_para_2"
            },
            "dimension": "unconditional",
            "type": "semantic",
            "is_meta": false,
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "response是否在一开始就总结了上一轮的<observation></observation>？直接回答 YES 或 NO。\n\nresponse: {response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": 3,
            "desc": "</thought> 结束",
            "other_info": {
                "from": "system_para_1"
            },
            "dimension": "unconditional",
            "type": "formatting",
            "is_meta": false,
            "evaluation": [
                {
                    "type": "code",
                    "exec": "\ndef check_following(response):\n    return response.rstrip().endswith(\"</thought>\")\n"
                }
            ],
            "score": false
        },
        {
            "id": 5,
            "desc": "您可以在thought中使用python packages,但只能从以下packages列表中选择：",
            "other_info": {
                "available_packages": [
                    "math",
                    "time",
                    "stat",
                    "queue",
                    "itertools",
                    "statistics",
                    "random",
                    "collections",
                    "unicodedata"
                ]
            },
            "dimension": "unconditional",
            "type": "resource",
            "is_meta": false,
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "提取下面文本中的所有的调用package，直接返回list格式结果，不要任何解释、前缀或赋值语句。\n输出格式要求：[\"package1\", \"package2\", ...]:\n\nHere is model response: {response}"
                },
                {
                    "type": "code",
                    "exec": "\nimport ast\n\ndef check_following(response: str) -> bool:\n    available_packages = ['math', 'time', 'stat', 'queue', 'itertools', 'statistics', 'random', 'collections', 'unicodedata']\n    try:\n        items = ast.literal_eval(response)\n        return isinstance(items, list) and all(item in available_packages for item in items)\n    except:\n        return False\n"
                }
            ],
            "score": false
        },
        {
            "id": 0,
            "desc": "Create a safe space by acknowledging their courage in seeking support.",
            "other_info": {
                "from": "system_para_0",
                "type_explanation": "The constraint focuses on the tone and content of the response, ensuring it is empathetic, supportive, and acknowledges the courage of the individual seeking support. This aligns with the semantic category as it pertains to style, tone, and meaningful content.",
                "meta_expalnation": "The given constraint directly deals with how the output should be structured by asking the model to acknowledge courage and create a safe space, which is a directive related to content and tone. It does not manage or govern other constraints (e.g., selecting, prioritizing, or merging them), so it is not a Meta Constraint.",
                "evaluation_type": {
                    "constraint_type": "llm",
                    "explanation": "Creating a safe space and acknowledging courage involves subjective interpretation, empathetic communication, and a nuanced understanding of tone and context, which require semantic assessment by an LLM."
                },
                "evaluation_generation_success": true
            },
            "dimension": "unconditional",
            "is_meta": false,
            "type": [
                "semantic"
            ],
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "Does the model response explicitly acknowledge the user's courage in seeking support? Please answer YES/NO directly and do not enter anything else.\n\nHere is the model response: {response}"
                }
            ],
            "state": "success",
            "llm_output": "NO",
            "score": false
        },
        {
            "id": 2,
            "desc": "Ask targeted follow-up questions to understand their full situation.",
            "other_info": {
                "from": "system_para_0",
                "type_explanation": "The constraint focuses on ensuring that meaningful and complete information is gathered by asking targeted follow-up questions. This aligns with the goal of maintaining semantic accuracy and completeness in understanding the user's situation.",
                "meta_expalnation": "This constraint directly governs the model's behavior (i.e., asking follow-up questions) and does not define strategies for managing, selecting, or prioritizing other constraints. It impacts the content and process of generating output, rather than providing a high-level rule for handling multiple constraints.",
                "evaluation_type": {
                    "constraint_type": "llm",
                    "explanation": "Asking targeted follow-up questions requires open-ended, semantic understanding of the user's input, emotional state, and overall context, which can only be assessed subjectively by an LLM rather than directly through code."
                },
                "evaluation_generation_success": true
            },
            "dimension": "unconditional",
            "is_meta": false,
            "type": [
                "semantic"
            ],
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "Does the model response include targeted follow-up questions that aim to understand the user's full situation? Please answer YES/NO directly and do not enter anything else.\n\nHere is the model response: {response}"
                }
            ],
            "state": "success",
            "llm_output": "NO",
            "score": false
        },
        {
            "id": 5,
            "desc": "Help them understand their current mental health in accessible language.",
            "other_info": {
                "from": "system_para_0",
                "type_explanation": "This constraint focuses on the style of communication ('accessible language') and the meaningfulness of the content ('help them understand their current mental health'). It emphasizes the tone and clarity, which falls under ensuring the output is appropriate, accurate, and understandable for the intended audience, aligning with the semantic category.",
                "meta_expalnation": "The given constraint directly governs the model's output by specifying content and language requirements, i.e., providing mental health explanations in accessible language. It does not define strategies for managing multiple constraints, and therefore does not qualify as a meta constraint.",
                "evaluation_type": {
                    "constraint_type": "llm",
                    "explanation": "This requires semantic understanding and the ability to explain mental health concepts in an accessible and empathetic manner, which involves open-ended, subjective assessment that only an LLM can accomplish."
                },
                "evaluation_generation_success": true
            },
            "dimension": "unconditional",
            "is_meta": false,
            "type": [
                "semantic"
            ],
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "Does the response explain the user's current mental health in accessible language that is easy to understand without using overly technical or complex terms? Please answer YES/NO directly and do not enter anything else.\n\nHere is the model response: {response}"
                }
            ],
            "state": "success",
            "llm_output": "NO",
            "score": false
        },
        {
            "id": 8,
            "desc": "Please **first** provide  a 2-3 sentence **summary** of your ideas on the assessment based on the context provided.",
            "other_info": {
                "from": "system_para_2",
                "type_explanation": "The constraint specifies the structure and presentation of the output by requiring it to be a '2-3 sentence summary,' which governs the format and length of the response rather than its content or resource limitations.",
                "meta_expalnation": "The given constraint directly governs the model's output by specifying content requirements (a 2-3 sentence summary of assessment ideas). It does not include any rules about managing or prioritizing multiple constraints, nor does it concern high-level strategies for selecting, ignoring, or combining constraints. Therefore, it is not a Meta Constraint.",
                "evaluation_type": {
                    "constraint_type": "llm",
                    "explanation": "The constraint is subjective and requires semantic interpretation of whether the assessment summary correctly addresses the context provided. This involves open-ended understanding and cannot be validated directly or through extracting structured elements via code."
                },
                "evaluation_generation_success": false
            },
            "dimension": "unconditional",
            "is_meta": false,
            "type": [
                "formatting"
            ],
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "Does the model response provide a summary firstly? Please answer YES/NO directly and do not enter anything else.\n\nHere is the model response: {response}"
                }
            ],
            "state": "modified",
            "llm_output": "NO",
            "score": false
        },
        {
            "id": 15,
            "desc": "Please first provide a **2-3 sentence summary of your ideas on the assessment based on the context provided**.",
            "other_info": {
                "from": "system_para_2",
                "type_explanation": "The constraint specifies the structure and presentation of the output by requiring it to be a '2-3 sentence summary,' which governs the format and length of the response rather than its content or resource limitations.",
                "meta_expalnation": "The given constraint directly governs the model's output by specifying content requirements (a 2-3 sentence summary of assessment ideas). It does not include any rules about managing or prioritizing multiple constraints, nor does it concern high-level strategies for selecting, ignoring, or combining constraints. Therefore, it is not a Meta Constraint.",
                "evaluation_type": {
                    "constraint_type": "llm_assisted_code",
                    "explanation": "human_modified"
                },
                "evaluation_generation_success": false
            },
            "dimension": "unconditional",
            "is_meta": false,
            "type": [
                "formatting"
            ],
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "Extract the summary where the response author presents their ideas on the assessment based on the given context. Return the extracted content verbatim from the response. If multiple segments are found, return them as a Python-style list of strings. If nothing is found, return an empty string (\"\").\n\nHere is the model response: {response}"
                },
                {
                    "type": "code",
                    "exec": "import re\n\ndef check_following(response: str) -> bool:\n    first_part = response.split('\\n\\n')[0] if '\\n\\n' in response else response\n    sentences = re.split('[.!?]', first_part)\n    sentences = [s.strip() for s in sentences if s.strip()]\n    return 2 <= len(sentences) <= 3"
                }
            ],
            "state": "modified",
            "score": false
        },
        {
            "id": 11,
            "desc": "Start your response with: '## ASSESSMENT Design'.",
            "other_info": {
                "from": "system_para_3",
                "type_explanation": "The constraint specifies the structure and presentation format of the output, requiring the response to start with the exact text '## ASSESSMENT Design'. This aligns with guidelines on syntax and layout norms, characteristic of the formatting category.",
                "meta_expalnation": "The constraint directly specifies the format in which the model's output should begin ('## ASSESSMENT Design'). It does not govern the management of multiple constraints, nor does it define selection, prioritization, disabling, deduplication, or composition rules. Therefore, it is not a Meta Constraint.",
                "evaluation_type": {
                    "constraint_type": "code",
                    "explanation": "This constraint requires checking if the response starts with the specific string '## ASSESSMENT Design'. This is a simple, exact match check and can be directly validated using straightforward string operations."
                },
                "evaluation_generation_success": true
            },
            "dimension": "unconditional",
            "is_meta": false,
            "type": [
                "formatting"
            ],
            "evaluation": [
                {
                    "type": "code",
                    "exec": "import re\n\ndef check_following(response: str) -> bool:\n    return bool(re.match(r'^## ASSESSMENT Design', response))\n"
                }
            ],
            "state": "success",
            "score": false
        },
        {
            "id": 4,
            "desc": "Assess risk levels with validated screening approaches.",
            "other_info": {
                "from": "system_para_0",
                "type_explanation": "The constraint focuses on ensuring meaningful and accurate content by requiring the use of 'validated screening approaches' for risk assessment. This directly pertains to content accuracy and adherence to established methods, which aligns with the 'semantic' category.",
                "meta_expalnation": "The given constraint directly instructs the model to assess risk levels using validated screening approaches, which is an operational rule affecting the content or execution of the task rather than managing multiple constraints. It does not involve selection, prioritization, disabling, deduplication, or composition of other constraints, so it is not a Meta Constraint.",
                "evaluation_type": {
                    "constraint_type": "llm",
                    "explanation": "This constraint involves assessing risk levels, which requires semantic understanding of clinical context, interpretation of nuanced language, and the application of validated screening approaches—a highly subjective and open-ended process. It cannot be directly encoded into logic or extracted in a structured way for validation."
                },
                "evaluation_generation_success": true
            },
            "dimension": "unconditional",
            "is_meta": false,
            "type": [
                "semantic"
            ],
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "Does the response include an assessment of risk levels using validated screening approaches? Please answer YES/NO directly and do not enter anything else.\n\nHere is the model response: {response}"
                }
            ],
            "state": "success",
            "llm_output": "NO",
            "score": false
        },
        {
            "id": 5,
            "desc": "Help them understand their current mental health in accessible language.",
            "other_info": {
                "from": "system_para_0",
                "type_explanation": "This constraint focuses on the style of communication ('accessible language') and the meaningfulness of the content ('help them understand their current mental health'). It emphasizes the tone and clarity, which falls under ensuring the output is appropriate, accurate, and understandable for the intended audience, aligning with the semantic category.",
                "meta_expalnation": "The given constraint directly governs the model's output by specifying content and language requirements, i.e., providing mental health explanations in accessible language. It does not define strategies for managing multiple constraints, and therefore does not qualify as a meta constraint.",
                "evaluation_type": {
                    "constraint_type": "llm",
                    "explanation": "This requires semantic understanding and the ability to explain mental health concepts in an accessible and empathetic manner, which involves open-ended, subjective assessment that only an LLM can accomplish."
                },
                "evaluation_generation_success": true
            },
            "dimension": "unconditional",
            "is_meta": false,
            "type": [
                "semantic"
            ],
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "Does the response explain the user's current mental health in accessible language that is easy to understand without using overly technical or complex terms? Please answer YES/NO directly and do not enter anything else.\n\nHere is the model response: {response}"
                }
            ],
            "state": "success",
            "llm_output": "NO",
            "score": false
        },
        {
            "id": 6,
            "desc": "Validate their experiences without minimizing or catastrophizing.",
            "other_info": {
                "from": "system_para_0",
                "type_explanation": "The constraint ensures that the output does not diminish or exaggerate the described experiences, requiring logical consistency, neutrality of position, and tone management. These are semantic requirements aimed at meaningful and appropriate communication.",
                "meta_expalnation": "The provided constraint directly affects the output by specifying how experiences should be validated (without minimizing or catastrophizing). It does not involve managing or interacting with other constraints, such as selecting, prioritizing, disabling, deduplicating, or combining them, which are the defining characteristics of a meta constraint.",
                "evaluation_type": {
                    "constraint_type": "llm",
                    "explanation": "Validating experiences without minimizing or catastrophizing requires subjective and semantic understanding of tone, intent, and nuance. This cannot be directly coded or reliably extracted for rule-based validation."
                },
                "evaluation_generation_success": true
            },
            "dimension": "unconditional",
            "is_meta": false,
            "type": [
                "semantic"
            ],
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "Does the response validate the user's experiences? Please answer YES/NO directly and do not enter anything else.\n\nHere is the model response: {response}"
                }
            ],
            "state": "modified",
            "llm_output": "NO",
            "score": false
        },
        {
            "id": 15,
            "desc": "Please first provide a **2-3 sentence summary of your ideas on the assessment based on the context provided**.",
            "other_info": {
                "from": "system_para_2",
                "type_explanation": "The constraint specifies the structure and presentation of the output by requiring it to be a '2-3 sentence summary,' which governs the format and length of the response rather than its content or resource limitations.",
                "meta_expalnation": "The given constraint directly governs the model's output by specifying content requirements (a 2-3 sentence summary of assessment ideas). It does not include any rules about managing or prioritizing multiple constraints, nor does it concern high-level strategies for selecting, ignoring, or combining constraints. Therefore, it is not a Meta Constraint.",
                "evaluation_type": {
                    "constraint_type": "llm_assisted_code",
                    "explanation": "human_modified"
                },
                "evaluation_generation_success": false
            },
            "dimension": "unconditional",
            "is_meta": false,
            "type": [
                "formatting"
            ],
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "Extract the summary where the response author presents their ideas on the assessment based on the given context. Return the extracted content verbatim from the response. If multiple segments are found, return them as a Python-style list of strings. If nothing is found, return an empty string (\"\").\n\nHere is the model response: {response}"
                },
                {
                    "type": "code",
                    "exec": "import re\n\ndef check_following(response: str) -> bool:\n    first_part = response.split('\\n\\n')[0] if '\\n\\n' in response else response\n    sentences = re.split('[.!?]', first_part)\n    sentences = [s.strip() for s in sentences if s.strip()]\n    return 2 <= len(sentences) <= 3"
                }
            ],
            "state": "modified",
            "score": false
        },
        {
            "id": 11,
            "desc": "Start your response with: '## ASSESSMENT Design'.",
            "other_info": {
                "from": "system_para_3",
                "type_explanation": "The constraint specifies the structure and presentation format of the output, requiring the response to start with the exact text '## ASSESSMENT Design'. This aligns with guidelines on syntax and layout norms, characteristic of the formatting category.",
                "meta_expalnation": "The constraint directly specifies the format in which the model's output should begin ('## ASSESSMENT Design'). It does not govern the management of multiple constraints, nor does it define selection, prioritization, disabling, deduplication, or composition rules. Therefore, it is not a Meta Constraint.",
                "evaluation_type": {
                    "constraint_type": "code",
                    "explanation": "This constraint requires checking if the response starts with the specific string '## ASSESSMENT Design'. This is a simple, exact match check and can be directly validated using straightforward string operations."
                },
                "evaluation_generation_success": true
            },
            "dimension": "unconditional",
            "is_meta": false,
            "type": [
                "formatting"
            ],
            "evaluation": [
                {
                    "type": "code",
                    "exec": "import re\n\ndef check_following(response: str) -> bool:\n    return bool(re.match(r'^## ASSESSMENT Design', response))\n"
                }
            ],
            "state": "success",
            "score": false
        },
        {
            "id": 3,
            "desc": "</thought> 结束",
            "other_info": {
                "from": "system_para_1"
            },
            "dimension": "unconditional",
            "type": "formatting",
            "is_meta": false,
            "evaluation": [
                {
                    "type": "code",
                    "exec": "\ndef check_following(response):\n    return response.rstrip().endswith(\"</thought>\")\n"
                }
            ],
            "score": false
        },
        {
            "id": 5,
            "desc": "您可以在thought中使用python packages,但只能从以下packages列表中选择：",
            "other_info": {
                "available_packages": [
                    "math",
                    "time",
                    "stat",
                    "queue",
                    "itertools",
                    "statistics",
                    "random",
                    "collections",
                    "unicodedata"
                ]
            },
            "dimension": "unconditional",
            "type": "resource",
            "is_meta": false,
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "提取下面文本中的所有的调用package，直接返回list格式结果，不要任何解释、前缀或赋值语句。\n输出格式要求：[\"package1\", \"package2\", ...]:\n\nHere is model response: {response}"
                },
                {
                    "type": "code",
                    "exec": "\nimport ast\n\ndef check_following(response: str) -> bool:\n    available_packages = ['math', 'time', 'stat', 'queue', 'itertools', 'statistics', 'random', 'collections', 'unicodedata']\n    try:\n        items = ast.literal_eval(response)\n        return isinstance(items, list) and all(item in available_packages for item in items)\n    except:\n        return False\n"
                }
            ],
            "score": false
        },
        {
            "id": 4,
            "desc": "除了第一轮外，对上一轮的<observation></observation>进行总结。",
            "other_info": {
                "from": "system_para_2"
            },
            "dimension": "unconditional",
            "type": "semantic",
            "is_meta": false,
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "response是否在一开始就总结了上一轮的<observation></observation>？直接回答 YES 或 NO。\n\nresponse: {response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": 0,
            "desc": "Create a safe space by acknowledging their courage in seeking support.",
            "other_info": {
                "from": "system_para_0",
                "type_explanation": "The constraint focuses on the tone and content of the response, ensuring it is empathetic, supportive, and acknowledges the courage of the individual seeking support. This aligns with the semantic category as it pertains to style, tone, and meaningful content.",
                "meta_expalnation": "The given constraint directly deals with how the output should be structured by asking the model to acknowledge courage and create a safe space, which is a directive related to content and tone. It does not manage or govern other constraints (e.g., selecting, prioritizing, or merging them), so it is not a Meta Constraint.",
                "evaluation_type": {
                    "constraint_type": "llm",
                    "explanation": "Creating a safe space and acknowledging courage involves subjective interpretation, empathetic communication, and a nuanced understanding of tone and context, which require semantic assessment by an LLM."
                },
                "evaluation_generation_success": true
            },
            "dimension": "unconditional",
            "is_meta": false,
            "type": [
                "semantic"
            ],
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "Does the model response explicitly acknowledge the user's courage in seeking support? Please answer YES/NO directly and do not enter anything else.\n\nHere is the model response: {response}"
                }
            ],
            "state": "success",
            "llm_output": "NO",
            "score": false
        },
        {
            "id": 4,
            "desc": "Assess risk levels with validated screening approaches.",
            "other_info": {
                "from": "system_para_0",
                "type_explanation": "The constraint focuses on ensuring meaningful and accurate content by requiring the use of 'validated screening approaches' for risk assessment. This directly pertains to content accuracy and adherence to established methods, which aligns with the 'semantic' category.",
                "meta_expalnation": "The given constraint directly instructs the model to assess risk levels using validated screening approaches, which is an operational rule affecting the content or execution of the task rather than managing multiple constraints. It does not involve selection, prioritization, disabling, deduplication, or composition of other constraints, so it is not a Meta Constraint.",
                "evaluation_type": {
                    "constraint_type": "llm",
                    "explanation": "This constraint involves assessing risk levels, which requires semantic understanding of clinical context, interpretation of nuanced language, and the application of validated screening approaches—a highly subjective and open-ended process. It cannot be directly encoded into logic or extracted in a structured way for validation."
                },
                "evaluation_generation_success": true
            },
            "dimension": "unconditional",
            "is_meta": false,
            "type": [
                "semantic"
            ],
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "Does the response include an assessment of risk levels using validated screening approaches? Please answer YES/NO directly and do not enter anything else.\n\nHere is the model response: {response}"
                }
            ],
            "state": "success",
            "llm_output": "NO",
            "score": false
        },
        {
            "id": 5,
            "desc": "Help them understand their current mental health in accessible language.",
            "other_info": {
                "from": "system_para_0",
                "type_explanation": "This constraint focuses on the style of communication ('accessible language') and the meaningfulness of the content ('help them understand their current mental health'). It emphasizes the tone and clarity, which falls under ensuring the output is appropriate, accurate, and understandable for the intended audience, aligning with the semantic category.",
                "meta_expalnation": "The given constraint directly governs the model's output by specifying content and language requirements, i.e., providing mental health explanations in accessible language. It does not define strategies for managing multiple constraints, and therefore does not qualify as a meta constraint.",
                "evaluation_type": {
                    "constraint_type": "llm",
                    "explanation": "This requires semantic understanding and the ability to explain mental health concepts in an accessible and empathetic manner, which involves open-ended, subjective assessment that only an LLM can accomplish."
                },
                "evaluation_generation_success": true
            },
            "dimension": "unconditional",
            "is_meta": false,
            "type": [
                "semantic"
            ],
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "Does the response explain the user's current mental health in accessible language that is easy to understand without using overly technical or complex terms? Please answer YES/NO directly and do not enter anything else.\n\nHere is the model response: {response}"
                }
            ],
            "state": "success",
            "llm_output": "NO",
            "score": false
        },
        {
            "id": 6,
            "desc": "Validate their experiences without minimizing or catastrophizing.",
            "other_info": {
                "from": "system_para_0",
                "type_explanation": "The constraint ensures that the output does not diminish or exaggerate the described experiences, requiring logical consistency, neutrality of position, and tone management. These are semantic requirements aimed at meaningful and appropriate communication.",
                "meta_expalnation": "The provided constraint directly affects the output by specifying how experiences should be validated (without minimizing or catastrophizing). It does not involve managing or interacting with other constraints, such as selecting, prioritizing, disabling, deduplicating, or combining them, which are the defining characteristics of a meta constraint.",
                "evaluation_type": {
                    "constraint_type": "llm",
                    "explanation": "Validating experiences without minimizing or catastrophizing requires subjective and semantic understanding of tone, intent, and nuance. This cannot be directly coded or reliably extracted for rule-based validation."
                },
                "evaluation_generation_success": true
            },
            "dimension": "unconditional",
            "is_meta": false,
            "type": [
                "semantic"
            ],
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "Does the response validate the user's experiences? Please answer YES/NO directly and do not enter anything else.\n\nHere is the model response: {response}"
                }
            ],
            "state": "modified",
            "llm_output": "NO",
            "score": false
        },
        {
            "id": 15,
            "desc": "Please first provide a **2-3 sentence summary of your ideas on the assessment based on the context provided**.",
            "other_info": {
                "from": "system_para_2",
                "type_explanation": "The constraint specifies the structure and presentation of the output by requiring it to be a '2-3 sentence summary,' which governs the format and length of the response rather than its content or resource limitations.",
                "meta_expalnation": "The given constraint directly governs the model's output by specifying content requirements (a 2-3 sentence summary of assessment ideas). It does not include any rules about managing or prioritizing multiple constraints, nor does it concern high-level strategies for selecting, ignoring, or combining constraints. Therefore, it is not a Meta Constraint.",
                "evaluation_type": {
                    "constraint_type": "llm_assisted_code",
                    "explanation": "human_modified"
                },
                "evaluation_generation_success": false
            },
            "dimension": "unconditional",
            "is_meta": false,
            "type": [
                "formatting"
            ],
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "Extract the summary where the response author presents their ideas on the assessment based on the given context. Return the extracted content verbatim from the response. If multiple segments are found, return them as a Python-style list of strings. If nothing is found, return an empty string (\"\").\n\nHere is the model response: {response}"
                },
                {
                    "type": "code",
                    "exec": "import re\n\ndef check_following(response: str) -> bool:\n    first_part = response.split('\\n\\n')[0] if '\\n\\n' in response else response\n    sentences = re.split('[.!?]', first_part)\n    sentences = [s.strip() for s in sentences if s.strip()]\n    return 2 <= len(sentences) <= 3"
                }
            ],
            "state": "modified",
            "score": false
        },
        {
            "id": 11,
            "desc": "Start your response with: '## ASSESSMENT Design'.",
            "other_info": {
                "from": "system_para_3",
                "type_explanation": "The constraint specifies the structure and presentation format of the output, requiring the response to start with the exact text '## ASSESSMENT Design'. This aligns with guidelines on syntax and layout norms, characteristic of the formatting category.",
                "meta_expalnation": "The constraint directly specifies the format in which the model's output should begin ('## ASSESSMENT Design'). It does not govern the management of multiple constraints, nor does it define selection, prioritization, disabling, deduplication, or composition rules. Therefore, it is not a Meta Constraint.",
                "evaluation_type": {
                    "constraint_type": "code",
                    "explanation": "This constraint requires checking if the response starts with the specific string '## ASSESSMENT Design'. This is a simple, exact match check and can be directly validated using straightforward string operations."
                },
                "evaluation_generation_success": true
            },
            "dimension": "unconditional",
            "is_meta": false,
            "type": [
                "formatting"
            ],
            "evaluation": [
                {
                    "type": "code",
                    "exec": "import re\n\ndef check_following(response: str) -> bool:\n    return bool(re.match(r'^## ASSESSMENT Design', response))\n"
                }
            ],
            "state": "success",
            "score": false
        }
    ],
    "conditional": [
        {
            "id": 5,
            "desc": "必须使用get_address_info获得企业地址具体的省份，城市，区县。",
            "other_info": {
                "available_packages": [
                    "math",
                    "time",
                    "stat",
                    "queue",
                    "itertools",
                    "statistics",
                    "random",
                    "collections",
                    "unicodedata"
                ]
            },
            "dimension": "conditional",
            "type": "resource",
            "is_meta": false,
            "evaluation": [
                {
                    "type": "llm_conditional_check",
                    "exec": "response是否希望获得企业地址具体的省份或者城市或者区县。直接回答 YES 或 NO。\n\nresponse: {response}"
                },
                {
                    "type": "code",
                    "exec": "\ndef check_following(response: str) -> bool:\n    if \"get_address_info\" in response:\n        return True\n    else:\n        return False\n"
                }
            ],
            "score": false
        },
        {
            "id": 5,
            "desc": "必须使用get_address_info获得企业地址具体的省份，城市，区县。",
            "other_info": {
                "available_packages": [
                    "math",
                    "time",
                    "stat",
                    "queue",
                    "itertools",
                    "statistics",
                    "random",
                    "collections",
                    "unicodedata"
                ]
            },
            "dimension": "conditional",
            "type": "resource",
            "is_meta": false,
            "evaluation": [
                {
                    "type": "llm_conditional_check",
                    "exec": "response是否希望获得企业地址具体的省份或者城市或者区县。直接回答 YES 或 NO。\n\nresponse: {response}"
                },
                {
                    "type": "code",
                    "exec": "\ndef check_following(response: str) -> bool:\n    if \"get_address_info\" in response:\n        return True\n    else:\n        return False\n"
                }
            ],
            "score": false
        },
        {
            "id": 5,
            "desc": "必须使用get_address_info获得企业地址具体的省份，城市，区县。",
            "other_info": {
                "available_packages": [
                    "math",
                    "time",
                    "stat",
                    "queue",
                    "itertools",
                    "statistics",
                    "random",
                    "collections",
                    "unicodedata"
                ]
            },
            "dimension": "conditional",
            "type": "resource",
            "is_meta": false,
            "evaluation": [
                {
                    "type": "llm_conditional_check",
                    "exec": "response是否希望获得企业地址具体的省份或者城市或者区县。直接回答 YES 或 NO。\n\nresponse: {response}"
                },
                {
                    "type": "code",
                    "exec": "\ndef check_following(response: str) -> bool:\n    if \"get_address_info\" in response:\n        return True\n    else:\n        return False\n"
                }
            ],
            "score": false
        },
        {
            "id": 5,
            "desc": "必须使用get_address_info获得企业地址具体的省份，城市，区县。",
            "other_info": {
                "available_packages": [
                    "math",
                    "time",
                    "stat",
                    "queue",
                    "itertools",
                    "statistics",
                    "random",
                    "collections",
                    "unicodedata"
                ]
            },
            "dimension": "conditional",
            "type": "resource",
            "is_meta": false,
            "evaluation": [
                {
                    "type": "llm_conditional_check",
                    "exec": "response是否希望获得企业地址具体的省份或者城市或者区县。直接回答 YES 或 NO。\n\nresponse: {response}"
                },
                {
                    "type": "code",
                    "exec": "\ndef check_following(response: str) -> bool:\n    if \"get_address_info\" in response:\n        return True\n    else:\n        return False\n"
                }
            ],
            "score": false
        },
        {
            "id": 5,
            "desc": "必须使用get_address_info获得企业地址具体的省份，城市，区县。",
            "other_info": {
                "available_packages": [
                    "math",
                    "time",
                    "stat",
                    "queue",
                    "itertools",
                    "statistics",
                    "random",
                    "collections",
                    "unicodedata"
                ]
            },
            "dimension": "conditional",
            "type": "resource",
            "is_meta": false,
            "evaluation": [
                {
                    "type": "llm_conditional_check",
                    "exec": "response是否希望获得企业地址具体的省份或者城市或者区县。直接回答 YES 或 NO。\n\nresponse: {response}"
                },
                {
                    "type": "code",
                    "exec": "\ndef check_following(response: str) -> bool:\n    if \"get_address_info\" in response:\n        return True\n    else:\n        return False\n"
                }
            ],
            "score": false
        },
        {
            "id": 5,
            "desc": "必须使用get_address_info获得企业地址具体的省份，城市，区县。",
            "other_info": {
                "available_packages": [
                    "math",
                    "time",
                    "stat",
                    "queue",
                    "itertools",
                    "statistics",
                    "random",
                    "collections",
                    "unicodedata"
                ]
            },
            "dimension": "conditional",
            "type": "resource",
            "is_meta": false,
            "evaluation": [
                {
                    "type": "llm_conditional_check",
                    "exec": "response是否希望获得企业地址具体的省份或者城市或者区县。直接回答 YES 或 NO。\n\nresponse: {response}"
                },
                {
                    "type": "code",
                    "exec": "\ndef check_following(response: str) -> bool:\n    if \"get_address_info\" in response:\n        return True\n    else:\n        return False\n"
                }
            ],
            "score": false
        },
        {
            "id": 5,
            "desc": "必须使用get_lawfirm_info查看律师事务所的负责人，不要直接使用案件的\"被告律师事务所\"",
            "other_info": {
                "available_packages": [
                    "math",
                    "time",
                    "stat",
                    "queue",
                    "itertools",
                    "statistics",
                    "random",
                    "collections",
                    "unicodedata"
                ]
            },
            "dimension": "conditional",
            "type": "resource",
            "is_meta": false,
            "evaluation": [
                {
                    "type": "llm_conditional_check",
                    "exec": "response是否提及律师事务所。直接回答 YES 或 NO。\n\nresponse: {response}"
                },
                {
                    "type": "code",
                    "exec": "\ndef check_following(response: str) -> bool:\n    if \"get_lawfirm_info\" in response:\n        return True\n    else:\n        return False\n"
                }
            ],
            "score": false
        },
        {
            "id": 5,
            "desc": "必须使用get_address_info获得企业地址具体的省份，城市，区县。",
            "other_info": {
                "available_packages": [
                    "math",
                    "time",
                    "stat",
                    "queue",
                    "itertools",
                    "statistics",
                    "random",
                    "collections",
                    "unicodedata"
                ]
            },
            "dimension": "conditional",
            "type": "resource",
            "is_meta": false,
            "evaluation": [
                {
                    "type": "llm_conditional_check",
                    "exec": "response是否希望获得企业地址具体的省份或者城市或者区县。直接回答 YES 或 NO。\n\nresponse: {response}"
                },
                {
                    "type": "code",
                    "exec": "\ndef check_following(response: str) -> bool:\n    if \"get_address_info\" in response:\n        return True\n    else:\n        return False\n"
                }
            ],
            "score": false
        },
        {
            "id": 5,
            "desc": "必须使用get_address_info获得企业地址具体的省份，城市，区县。",
            "other_info": {
                "available_packages": [
                    "math",
                    "time",
                    "stat",
                    "queue",
                    "itertools",
                    "statistics",
                    "random",
                    "collections",
                    "unicodedata"
                ]
            },
            "dimension": "conditional",
            "type": "resource",
            "is_meta": false,
            "evaluation": [
                {
                    "type": "llm_conditional_check",
                    "exec": "response是否希望获得企业地址具体的省份或者城市或者区县。直接回答 YES 或 NO。\n\nresponse: {response}"
                },
                {
                    "type": "code",
                    "exec": "\ndef check_following(response: str) -> bool:\n    if \"get_address_info\" in response:\n        return True\n    else:\n        return False\n"
                }
            ],
            "score": false
        },
        {
            "id": 5,
            "desc": "必须使用get_address_info获得企业地址具体的省份，城市，区县。",
            "other_info": {
                "available_packages": [
                    "math",
                    "time",
                    "stat",
                    "queue",
                    "itertools",
                    "statistics",
                    "random",
                    "collections",
                    "unicodedata"
                ]
            },
            "dimension": "conditional",
            "type": "resource",
            "is_meta": false,
            "evaluation": [
                {
                    "type": "llm_conditional_check",
                    "exec": "response是否希望获得企业地址具体的省份或者城市或者区县。直接回答 YES 或 NO。\n\nresponse: {response}"
                },
                {
                    "type": "code",
                    "exec": "\ndef check_following(response: str) -> bool:\n    if \"get_address_info\" in response:\n        return True\n    else:\n        return False\n"
                }
            ],
            "score": false
        },
        {
            "id": 5,
            "desc": "必须使用get_address_info获得企业地址具体的省份，城市，区县。",
            "other_info": {
                "available_packages": [
                    "math",
                    "time",
                    "stat",
                    "queue",
                    "itertools",
                    "statistics",
                    "random",
                    "collections",
                    "unicodedata"
                ]
            },
            "dimension": "conditional",
            "type": "resource",
            "is_meta": false,
            "evaluation": [
                {
                    "type": "llm_conditional_check",
                    "exec": "response是否希望获得企业地址具体的省份或者城市或者区县。直接回答 YES 或 NO。\n\nresponse: {response}"
                },
                {
                    "type": "code",
                    "exec": "\ndef check_following(response: str) -> bool:\n    if \"get_address_info\" in response:\n        return True\n    else:\n        return False\n"
                }
            ],
            "score": false
        },
        {
            "id": 5,
            "desc": "必须使用get_lawfirm_info查看律师事务所的负责人，不要直接使用案件的\"被告律师事务所\"",
            "other_info": {
                "available_packages": [
                    "math",
                    "time",
                    "stat",
                    "queue",
                    "itertools",
                    "statistics",
                    "random",
                    "collections",
                    "unicodedata"
                ]
            },
            "dimension": "conditional",
            "type": "resource",
            "is_meta": false,
            "evaluation": [
                {
                    "type": "llm_conditional_check",
                    "exec": "response是否提及律师事务所。直接回答 YES 或 NO。\n\nresponse: {response}"
                },
                {
                    "type": "code",
                    "exec": "\ndef check_following(response: str) -> bool:\n    if \"get_lawfirm_info\" in response:\n        return True\n    else:\n        return False\n"
                }
            ],
            "score": false
        },
        {
            "id": 0,
            "desc": "Return ONLY the database name, no other text or explanation. If you're not confident about the routing, return an empty response.",
            "other_info": {
                "from": "system_para_0",
                "type_explanation": "The constraint specifies the structure of the output by requiring that only the database name is returned, with no additional text or explanation. This aligns with formatting as it controls the presentation and ensures the result adheres to a specific format.",
                "meta_expalnation": "The constraint directly specifies the output format ('Return ONLY the database name, no other text or explanation') and does not govern how to select, prioritize, ignore, deduplicate, or combine other constraints. It is not a high-level rule for managing multiple constraints but instead explicitly restricts the model's behavior.",
                "evaluation_type": {
                    "constraint_type": "code",
                    "explanation": "The constraint directly specifies returning the database name without additional text or explanation, which can be validated by string operations and format checks in code."
                },
                "evaluation_generation_success": true
            },
            "dimension": "conditional",
            "is_meta": false,
            "type": [
                "formatting"
            ],
            "evaluation": [
                {
                    "type": "code",
                    "exec": "import re\n\ndef check_following(response: str) -> bool:\n    pattern = r'^(products|support|finance)?$'\n    return bool(re.fullmatch(pattern, response.strip()))\n"
                }
            ],
            "state": "modified",
            "score": false
        },
        {
            "id": 0,
            "desc": "Return ONLY the database name, no other text or explanation. If you're not confident about the routing, return an empty response.",
            "other_info": {
                "from": "system_para_0",
                "type_explanation": "The constraint specifies the structure of the output by requiring that only the database name is returned, with no additional text or explanation. This aligns with formatting as it controls the presentation and ensures the result adheres to a specific format.",
                "meta_expalnation": "The constraint directly specifies the output format ('Return ONLY the database name, no other text or explanation') and does not govern how to select, prioritize, ignore, deduplicate, or combine other constraints. It is not a high-level rule for managing multiple constraints but instead explicitly restricts the model's behavior.",
                "evaluation_type": {
                    "constraint_type": "code",
                    "explanation": "The constraint directly specifies returning the database name without additional text or explanation, which can be validated by string operations and format checks in code."
                },
                "evaluation_generation_success": true
            },
            "dimension": "conditional",
            "is_meta": false,
            "type": [
                "formatting"
            ],
            "evaluation": [
                {
                    "type": "code",
                    "exec": "import re\n\ndef check_following(response: str) -> bool:\n    pattern = r'^(products|support|finance)?$'\n    return bool(re.fullmatch(pattern, response.strip()))\n"
                }
            ],
            "state": "modified",
            "score": false
        },
        {
            "id": 4,
            "desc": "Add visual elements descriptions (charts, diagrams, infographics).",
            "other_info": {
                "from": "system_para_1",
                "type_explanation": "The constraint focuses on enhancing the meaningfulness and completeness of the output by including descriptions of visual elements such as charts, diagrams, and infographics. This aligns with the semantic category, as it emphasizes the need for content that conveys information effectively and meets specific requirements in terms of completeness.",
                "meta_expalnation": "The given constraint directly specifies an output requirement (adding visual element descriptions) rather than defining how to manage other constraints. It directly constrains the model’s content and format rather than operating as a high-level rule for handling multiple constraints.",
                "evaluation_type": {
                    "constraint_type": "llm",
                    "explanation": "The task requires understanding the visual elements' content and generating descriptive text that complements them, which involves semantic understanding and subjective assessment of relevance and accuracy, making it suited for LLM-based validation."
                },
                "evaluation_generation_success": true
            },
            "dimension": "conditional",
            "is_meta": false,
            "type": [
                "semantic"
            ],
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "Does the model's response explicitly include descriptions for visual elements such as charts, diagrams, or infographics? Please answer YES/NO directly and do not enter anything else.\n\nHere is the model response: {response}"
                }
            ],
            "state": "success",
            "llm_output": "NO",
            "score": false
        },
        {
            "id": 4,
            "desc": "Add visual elements descriptions (charts, diagrams, infographics).",
            "other_info": {
                "from": "system_para_1",
                "type_explanation": "The constraint focuses on enhancing the meaningfulness and completeness of the output by including descriptions of visual elements such as charts, diagrams, and infographics. This aligns with the semantic category, as it emphasizes the need for content that conveys information effectively and meets specific requirements in terms of completeness.",
                "meta_expalnation": "The given constraint directly specifies an output requirement (adding visual element descriptions) rather than defining how to manage other constraints. It directly constrains the model’s content and format rather than operating as a high-level rule for handling multiple constraints.",
                "evaluation_type": {
                    "constraint_type": "llm",
                    "explanation": "The task requires understanding the visual elements' content and generating descriptive text that complements them, which involves semantic understanding and subjective assessment of relevance and accuracy, making it suited for LLM-based validation."
                },
                "evaluation_generation_success": true
            },
            "dimension": "conditional",
            "is_meta": false,
            "type": [
                "semantic"
            ],
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "Does the model's response explicitly include descriptions for visual elements such as charts, diagrams, or infographics? Please answer YES/NO directly and do not enter anything else.\n\nHere is the model response: {response}"
                }
            ],
            "state": "success",
            "llm_output": "NO",
            "score": false
        },
        {
            "id": 4,
            "desc": "Add visual elements descriptions (charts, diagrams, infographics).",
            "other_info": {
                "from": "system_para_1",
                "type_explanation": "The constraint focuses on enhancing the meaningfulness and completeness of the output by including descriptions of visual elements such as charts, diagrams, and infographics. This aligns with the semantic category, as it emphasizes the need for content that conveys information effectively and meets specific requirements in terms of completeness.",
                "meta_expalnation": "The given constraint directly specifies an output requirement (adding visual element descriptions) rather than defining how to manage other constraints. It directly constrains the model’s content and format rather than operating as a high-level rule for handling multiple constraints.",
                "evaluation_type": {
                    "constraint_type": "llm",
                    "explanation": "The task requires understanding the visual elements' content and generating descriptive text that complements them, which involves semantic understanding and subjective assessment of relevance and accuracy, making it suited for LLM-based validation."
                },
                "evaluation_generation_success": true
            },
            "dimension": "conditional",
            "is_meta": false,
            "type": [
                "semantic"
            ],
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "Does the model's response explicitly include descriptions for visual elements such as charts, diagrams, or infographics? Please answer YES/NO directly and do not enter anything else.\n\nHere is the model response: {response}"
                }
            ],
            "state": "success",
            "llm_output": "NO",
            "score": false
        },
        {
            "id": 6,
            "desc": "Plan for multiplayer infrastructure if applicable.",
            "other_info": {
                "from": "system_para_0",
                "type_explanation": "The constraint focuses on the content's logic and completeness by requiring consideration of multiplayer infrastructure, but only if it is applicable. This relates to ensuring meaningful and accurate content based on situational conditions (conditional logic). The emphasis is on the conceptual requirement rather than formatting or resource usage.",
                "meta_expalnation": "This constraint directly guides the content or behavior of the model by specifying a conditional requirement related to multiplayer infrastructure. It does not involve high-level rules for managing other constraints, such as selecting, prioritizing, or combining them.",
                "evaluation_type": {
                    "constraint_type": "llm",
                    "explanation": "This constraint requires a semantic understanding of multiplayer infrastructure, including assessing design choices, networking models, and scalability. It cannot be verified through simple code but rather requires an LLM's broader expertise in game development concepts."
                },
                "evaluation_generation_success": true
            },
            "dimension": "conditional",
            "is_meta": false,
            "type": [
                "semantic"
            ],
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "Does the model's response include a clear and specific plan for multiplayer infrastructure, addressing components such as server architecture, networking protocols, and player synchronization? Please answer YES/NO directly and do not enter anything else.\n\nHere is the model response: {response}"
                }
            ],
            "state": "modified",
            "llm_output": "NO",
            "score": false
        },
        {
            "id": 6,
            "desc": "Plan for multiplayer infrastructure if applicable.",
            "other_info": {
                "from": "system_para_0",
                "type_explanation": "The constraint focuses on the content's logic and completeness by requiring consideration of multiplayer infrastructure, but only if it is applicable. This relates to ensuring meaningful and accurate content based on situational conditions (conditional logic). The emphasis is on the conceptual requirement rather than formatting or resource usage.",
                "meta_expalnation": "This constraint directly guides the content or behavior of the model by specifying a conditional requirement related to multiplayer infrastructure. It does not involve high-level rules for managing other constraints, such as selecting, prioritizing, or combining them.",
                "evaluation_type": {
                    "constraint_type": "llm",
                    "explanation": "This constraint requires a semantic understanding of multiplayer infrastructure, including assessing design choices, networking models, and scalability. It cannot be verified through simple code but rather requires an LLM's broader expertise in game development concepts."
                },
                "evaluation_generation_success": true
            },
            "dimension": "conditional",
            "is_meta": false,
            "type": [
                "semantic"
            ],
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "Does the model's response include a clear and specific plan for multiplayer infrastructure, addressing components such as server architecture, networking protocols, and player synchronization? Please answer YES/NO directly and do not enter anything else.\n\nHere is the model response: {response}"
                }
            ],
            "state": "modified",
            "llm_output": "NO",
            "score": false
        },
        {
            "id": 4,
            "desc": "Add visual elements descriptions (charts, diagrams, infographics).",
            "other_info": {
                "from": "system_para_1",
                "type_explanation": "The constraint focuses on enhancing the meaningfulness and completeness of the output by including descriptions of visual elements such as charts, diagrams, and infographics. This aligns with the semantic category, as it emphasizes the need for content that conveys information effectively and meets specific requirements in terms of completeness.",
                "meta_expalnation": "The given constraint directly specifies an output requirement (adding visual element descriptions) rather than defining how to manage other constraints. It directly constrains the model’s content and format rather than operating as a high-level rule for handling multiple constraints.",
                "evaluation_type": {
                    "constraint_type": "llm",
                    "explanation": "The task requires understanding the visual elements' content and generating descriptive text that complements them, which involves semantic understanding and subjective assessment of relevance and accuracy, making it suited for LLM-based validation."
                },
                "evaluation_generation_success": true
            },
            "dimension": "conditional",
            "is_meta": false,
            "type": [
                "semantic"
            ],
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "Does the model's response explicitly include descriptions for visual elements such as charts, diagrams, or infographics? Please answer YES/NO directly and do not enter anything else.\n\nHere is the model response: {response}"
                }
            ],
            "state": "success",
            "llm_output": "NO",
            "score": false
        },
        {
            "id": 9,
            "desc": "finish函数将在用于返回用户的final_goal的结果时结束推理流程",
            "other_info": {},
            "dimension": "conditional",
            "type": "formatting",
            "is_meta": false,
            "evaluation": [
                {
                    "type": "code",
                    "exec": "\ndef check_following(response: str) -> bool:\n    if \"finish()\" in response:\n        return True\n    else:\n        return False\n"
                }
            ],
            "score": false
        },
        {
            "id": 3,
            "desc": "If using LaTeX, use double $$ as a delimiter instead of single $.",
            "other_info": {
                "from": "system_para_0",
                "type_explanation": "This constraint controls the structure and presentation format of the output, specifically the syntax norms of LaTeX by requiring double $$ delimiters rather than single $. This aligns directly with formatting requirements.",
                "meta_expalnation": "The given constraint directly governs the output format in terms of how LaTeX is delimited. It specifies a content-related output rule ('use double $$ instead of single $') based on a condition ('if using LaTeX'). Therefore, it is not a high-level rule managing other constraints, and does not qualify as a Meta Constraint.",
                "evaluation_type": {
                    "constraint_type": "code",
                    "explanation": "The constraint pertains to a specific formatting rule (using double $$ instead of single $), which can be validated through simple pattern matching and logic in code."
                },
                "evaluation_generation_success": true
            },
            "dimension": "conditional",
            "is_meta": false,
            "type": [
                "formatting"
            ],
            "evaluation": [
                {
                    "type": "code",
                    "exec": "\nimport re\n\ndef check_following(response: str) -> bool:\n    if re.search(r'(?<!\\$)\\$(?!\\$)', response):\n        return False\n    count = response.count('$$')\n    if count % 2 != 0:\n        return False\n    return True\n"
                }
            ],
            "state": "modified",
            "score": false
        },
        {
            "id": 2,
            "desc": "在写代码时你需要综合考虑最终目标<final_goal></final_goal>和本轮的任务<task></task>。",
            "other_info": {
                "from": "system_para_1",
                "first_task": "我需要使用get_sub_company_info来获取包钢天彩靖江科技有限公司的母公司信息。"
            },
            "is_meta": true,
            "type": "semantic",
            "dimension": "conditional",
            "dependency": [],
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "检查 response 是否严格按 task 要求执行核心操作（允许 print 辅助输出），没有执行任何其他额外操作，直接回答 YES 或 NO。\n\ntask: 我需要使用get_sub_company_info来获取包钢天彩靖江科技有限公司的母公司信息。\n\nresponse:{response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": 2,
            "desc": "在写代码时你需要综合考虑最终目标<final_goal></final_goal>和本轮的任务<task></task>。",
            "other_info": {
                "from": "system_para_1",
                "first_task": "根据查询结果，安徽中能电源有限公司的母公司是天能电池集团股份有限公司。"
            },
            "is_meta": true,
            "type": "semantic",
            "dimension": "conditional",
            "dependency": [],
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "检查 response 是否严格按 task 要求执行核心操作（允许 print 辅助输出），没有执行任何其他额外操作，直接回答 YES 或 NO。\n\ntask: 根据查询结果，安徽中能电源有限公司的母公司是天能电池集团股份有限公司。\n\nresponse:{response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": 2,
            "desc": "在写代码时你需要综合考虑最终目标<final_goal></final_goal>和本轮的任务<task></task>。",
            "other_info": {
                "from": "system_para_1",
                "first_task": "根据查询结果，案件编号为（2019）京0105民初9223号的原告是北京市桃李食品有限公司，被告是北京全时叁陆伍连锁便利店有限公司。"
            },
            "is_meta": true,
            "type": "semantic",
            "dimension": "conditional",
            "dependency": [],
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "检查 response 是否严格按 task 要求执行核心操作（允许 print 辅助输出），没有执行任何其他额外操作，直接回答 YES 或 NO。\n\ntask: 根据查询结果，案件编号为（2019）京0105民初9223号的原告是北京市桃李食品有限公司，被告是北京全时叁陆伍连锁便利店有限公司。\n\nresponse:{response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": 2,
            "desc": "在写代码时你需要综合考虑最终目标<final_goal></final_goal>和本轮的任务<task></task>。",
            "other_info": {
                "from": "system_para_1",
                "first_task": "将投资金额1.31亿转换为亿元并保留2位小数"
            },
            "is_meta": true,
            "type": "semantic",
            "dimension": "conditional",
            "dependency": [],
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "检查 response 是否严格按 task 要求执行核心操作（允许 print 辅助输出），没有执行任何其他额外操作，直接回答 YES 或 NO。\n\ntask: 将投资金额1.31亿转换为亿元并保留2位小数\n\nresponse:{response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": 2,
            "desc": "在写代码时你需要综合考虑最终目标<final_goal></final_goal>和本轮的任务<task></task>。",
            "other_info": {
                "from": "system_para_1",
                "first_task": "使用 get_company_info 函数获取金宏气体股份有限公司的代码、法人代表以及董秘的详细信息"
            },
            "is_meta": true,
            "type": "semantic",
            "dimension": "conditional",
            "dependency": [],
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "检查 response 是否严格按 task 要求执行核心操作（允许 print 辅助输出），没有执行任何其他额外操作，直接回答 YES 或 NO。\n\ntask: 使用 get_company_info 函数获取金宏气体股份有限公司的代码、法人代表以及董秘的详细信息\n\nresponse:{response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": 2,
            "desc": "在写代码时你需要综合考虑最终目标<final_goal></final_goal>和本轮的任务<task></task>。",
            "other_info": {
                "from": "system_para_1",
                "first_task": "筛选出参股比例为'100.0'且投资金额超过12.5亿元的子公司信息"
            },
            "is_meta": true,
            "type": "semantic",
            "dimension": "conditional",
            "dependency": [],
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "检查 response 是否严格按 task 要求执行核心操作（允许 print 辅助输出），没有执行任何其他额外操作，直接回答 YES 或 NO。\n\ntask: 筛选出参股比例为'100.0'且投资金额超过12.5亿元的子公司信息\n\nresponse:{response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": 4,
            "desc": "Add visual elements descriptions (charts, diagrams, infographics).",
            "other_info": {
                "from": "system_para_1",
                "type_explanation": "The constraint focuses on enhancing the meaningfulness and completeness of the output by including descriptions of visual elements such as charts, diagrams, and infographics. This aligns with the semantic category, as it emphasizes the need for content that conveys information effectively and meets specific requirements in terms of completeness.",
                "meta_expalnation": "The given constraint directly specifies an output requirement (adding visual element descriptions) rather than defining how to manage other constraints. It directly constrains the model’s content and format rather than operating as a high-level rule for handling multiple constraints.",
                "evaluation_type": {
                    "constraint_type": "llm",
                    "explanation": "The task requires understanding the visual elements' content and generating descriptive text that complements them, which involves semantic understanding and subjective assessment of relevance and accuracy, making it suited for LLM-based validation."
                },
                "evaluation_generation_success": true
            },
            "dimension": "conditional",
            "is_meta": false,
            "type": [
                "semantic"
            ],
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "Does the model's response explicitly include descriptions for visual elements such as charts, diagrams, or infographics? Please answer YES/NO directly and do not enter anything else.\n\nHere is the model response: {response}"
                }
            ],
            "state": "success",
            "llm_output": "NO",
            "score": false
        },
        {
            "id": 4,
            "desc": "Add visual elements descriptions (charts, diagrams, infographics).",
            "other_info": {
                "from": "system_para_1",
                "type_explanation": "The constraint focuses on enhancing the meaningfulness and completeness of the output by including descriptions of visual elements such as charts, diagrams, and infographics. This aligns with the semantic category, as it emphasizes the need for content that conveys information effectively and meets specific requirements in terms of completeness.",
                "meta_expalnation": "The given constraint directly specifies an output requirement (adding visual element descriptions) rather than defining how to manage other constraints. It directly constrains the model’s content and format rather than operating as a high-level rule for handling multiple constraints.",
                "evaluation_type": {
                    "constraint_type": "llm",
                    "explanation": "The task requires understanding the visual elements' content and generating descriptive text that complements them, which involves semantic understanding and subjective assessment of relevance and accuracy, making it suited for LLM-based validation."
                },
                "evaluation_generation_success": true
            },
            "dimension": "conditional",
            "is_meta": false,
            "type": [
                "semantic"
            ],
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "Does the model's response explicitly include descriptions for visual elements such as charts, diagrams, or infographics? Please answer YES/NO directly and do not enter anything else.\n\nHere is the model response: {response}"
                }
            ],
            "state": "success",
            "llm_output": "NO",
            "score": false
        },
        {
            "id": -1,
            "desc": "When the question provides explicit entity knowledge, always write a descriptor for the Search() function based on the question's information.",
            "other_info": {
                "from": "system_para_0",
                "condition_desc": "If the question provides explicit entity knowledge, always write a descriptor for the Search() function based on the question's information.",
                "complete_instruction_para": []
            },
            "dimension": "conditional",
            "is_meta": false,
            "type": [
                "semantic"
            ],
            "evaluation": [
                {
                    "type": "llm_conditional_check",
                    "exec": "Does the question provide explicit entity knowledge? Please answer YES/NO directly and do not enter anything else.\n\nHere is model response: {response}"
                },
                {
                    "type": "llm",
                    "exec": "Did the model write a descriptor parameter for the Search() function? Please answer YES/NO directly and do not enter anything else.\n\nHere is model response: {response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": -1,
            "desc": "When the question provides explicit entity knowledge, always write a descriptor for the Search() function based on the question's information.",
            "other_info": {
                "from": "system_para_0",
                "condition_desc": "If the question provides explicit entity knowledge, always write a descriptor for the Search() function based on the question's information.",
                "complete_instruction_para": []
            },
            "dimension": "conditional",
            "is_meta": false,
            "type": [
                "semantic"
            ],
            "evaluation": [
                {
                    "type": "llm_conditional_check",
                    "exec": "Does the question provide explicit entity knowledge? Please answer YES/NO directly and do not enter anything else.\n\nHere is model response: {response}"
                },
                {
                    "type": "llm",
                    "exec": "Did the model write a descriptor parameter for the Search() function? Please answer YES/NO directly and do not enter anything else.\n\nHere is model response: {response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": 2,
            "desc": "在写代码时你需要综合考虑最终目标<final_goal></final_goal>和本轮的任务<task></task>。",
            "other_info": {
                "from": "system_para_1",
                "first_task": "根据获取的案件信息，原告是中国建设银行股份有限公司常州经济开发区支行，被告包括常州市互联涂料有限公司、江苏新互盛电缆有限公司、常州格林电力机械制造有限公司、周某某、任某某、常州市神猴焊丝有限公司。"
            },
            "is_meta": true,
            "type": "semantic",
            "dimension": "conditional",
            "dependency": [],
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "检查 response 是否严格按 task 要求执行核心操作（允许 print 辅助输出），没有执行任何其他额外操作，直接回答 YES 或 NO。\n\ntask: 根据获取的案件信息，原告是中国建设银行股份有限公司常州经济开发区支行，被告包括常州市互联涂料有限公司、江苏新互盛电缆有限公司、常州格林电力机械制造有限公司、周某某、任某某、常州市神猴焊丝有限公司。\n\nresponse:{response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": 2,
            "desc": "在写代码时你需要综合考虑最终目标<final_goal></final_goal>和本轮的任务<task></task>。",
            "other_info": {
                "from": "system_para_1",
                "first_task": "我需要使用get_legal_document来获取（2020）苏0492民初2530号案件的文档内容。"
            },
            "is_meta": true,
            "type": "semantic",
            "dimension": "conditional",
            "dependency": [],
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "检查 response 是否严格按 task 要求执行核心操作（允许 print 辅助输出），没有执行任何其他额外操作，直接回答 YES 或 NO。\n\ntask: 我需要使用get_legal_document来获取（2020）苏0492民初2530号案件的文档内容。\n\nresponse:{response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": 2,
            "desc": "在写代码时你需要综合考虑最终目标<final_goal></final_goal>和本轮的任务<task></task>。",
            "other_info": {
                "from": "system_para_1",
                "first_task": "我需要获取金诚信矿业管理股份有限公司投资的子公司信息。"
            },
            "is_meta": true,
            "type": "semantic",
            "dimension": "conditional",
            "dependency": [],
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "检查 response 是否严格按 task 要求执行核心操作（允许 print 辅助输出），没有执行任何其他额外操作，直接回答 YES 或 NO。\n\ntask: 我需要获取金诚信矿业管理股份有限公司投资的子公司信息。\n\nresponse:{response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": 44,
            "desc": "Use <lov-success> to confirm successful operations.",
            "other_info": {
                "from": "system_para_0",
                "condition_desc": "If the model is confirming successful operations, then wrap them in <lov-success>.",
                "complete_instruction_para": [],
                "evaluation_type": {
                    "constraint_type": "code",
                    "explanation": "The constraint specifies the use of a specific tag (<lov-success>) to confirm successful operations, which can be directly validated by checking the presence and correct usage of the tag in the response."
                },
                "evaluation_generation_success": true
            },
            "dimension": "conditional",
            "is_meta": false,
            "type": [
                "semantic"
            ],
            "evaluation": [
                {
                    "type": "llm_conditional_check",
                    "exec": "Is the model confirming successful operations? Please answer YES/NO directly and do not enter anything else.\n\nHere is the model response: {response}"
                },
                {
                    "type": "code",
                    "exec": "import re\n\ndef check_following(response: str) -> bool:\n    return bool(re.search(r\"<lov-success>.*?</lov-success>\", response, re.DOTALL))\n"
                }
            ],
            "score": false
        },
        {
            "id": 74,
            "desc": "If you added new files, remember that you need to implement them fully.",
            "other_info": {
                "from": "system_para_138",
                "condition_desc": "If you added new files, then remember that you need to implement them fully.",
                "complete_instruction_para": [],
                "evaluation_type": {
                    "constraint_type": "llm",
                    "explanation": "The constraint requires checking whether new files are fully implemented, which can be validated directly by code by ensuring all necessary content is present in the files."
                },
                "evaluation_generation_success": true
            },
            "dimension": "conditional",
            "is_meta": false,
            "type": [
                "semantic"
            ],
            "evaluation": [
                {
                    "type": "llm_conditional_check",
                    "exec": "Did the model add any new files? Please answer YES/NO directly and do not enter anything else.\n\nHere is the model response: {response}"
                },
                {
                    "type": "llm",
                    "exec": "Does the model implement new files fully? Please answer YES/NO directly and do not enter anything else.\n\nHere is the model response: {response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": 99,
            "desc": "Clearly communicate to the user that some specific features were not implemented.",
            "other_info": {
                "from": "system_para_207",
                "condition_desc": "If a user asks for many features at once, then you do not have to implement them all as long as you clearly communicate to the user that you didn't implement some specific features.",
                "complete_instruction_para": [],
                "evaluation_type": {
                    "constraint_type": "llm",
                    "explanation": "The constraint requires a semantic understanding of whether the response clearly communicates to the user which features were not implemented. This involves subjective assessment of clarity and completeness, which cannot be directly validated by code."
                },
                "evaluation_generation_success": true
            },
            "dimension": "conditional",
            "is_meta": false,
            "type": [
                "semantic"
            ],
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "Does the response clearly communicate to the user which specific features were not implemented, if not all features were implemented? Please answer YES/NO directly and do not enter anything else.\n\nHere is the model response: {response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": 114,
            "desc": "Make sure to close all tags when writing files, with a line break before the closing tag.",
            "other_info": {
                "from": "system_para_214",
                "condition_desc": "If you do a <lov-write> operation, then make sure to close all tags when writing files, with a line break before the closing tag.",
                "complete_instruction_para": [
                    "system_para_213"
                ],
                "evaluation_type": {
                    "constraint_type": "code",
                    "explanation": "The constraint involves checking for the presence of a line break before closing tags, which can be directly validated using code by analyzing the file's syntax and formatting."
                },
                "evaluation_generation_success": true
            },
            "dimension": "conditional",
            "is_meta": false,
            "type": [
                "formatting"
            ],
            "evaluation": [
                {
                    "type": "llm_conditional_check",
                    "exec": "Is the model performing a <lov-write> operation? Please answer YES/NO directly and do not enter anything else.\n\nHere is the model response: {response}"
                },
                {
                    "type": "code",
                    "exec": "\nimport re\nfrom typing import List\n\ndef check_following(response: str) -> bool:\n    # Find all opening and closing tags\n    tag_pattern = r'<([^>]+)>'\n    tags = re.findall(tag_pattern, response)\n    \n    # Stack to keep track of opening tags\n    stack: List[str] = []\n    \n    for tag in tags:\n        # Skip self-closing tags\n        if tag.endswith('/'):\n            continue\n            \n        # Handle closing tags\n        if tag.startswith('/'):\n            # If stack is empty, we found a closing tag without matching opening tag\n            if not stack:\n                return False\n                \n            # Get the expected opening tag name\n            expected_tag = stack[-1]\n            # Remove the '/' from the closing tag\n            actual_tag = tag[1:]\n            \n            # Check if tags match\n            if expected_tag != actual_tag:\n                return False\n                \n            # Remove the matched opening tag from stack\n            stack.pop()\n        else:\n            # Add opening tag to stack\n            stack.append(tag)\n    \n    # If stack is not empty, we have unclosed tags\n    return len(stack) == 0\n"
                }
            ],
            "score": false
        },
        {
            "id": 26,
            "desc": "When using multiple SEARCH/REPLACE blocks, list them in the order they appear in the file.",
            "other_info": {
                "from": "system_para_14",
                "condition_desc": "If you are using multiple SEARCH/REPLACE blocks, then list them in the order in which they are positioned within the file. ",
                "complete_instruction_para": [],
                "evaluation_type": {
                    "constraint_type": "llm_assisted_code",
                    "explanation": "Validating this constraint requires extracting the sequence of SEARCH/REPLACE blocks from the input and checking their order against the file's content. Code can perform the validation, but it first needs the specific blocks to be identified and extracted, which may require LLM assistance."
                },
                "evaluation_generation_success": true
            },
            "dimension": "conditional",
            "is_meta": false,
            "type": [
                "formatting"
            ],
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "Extract all SEARCH/REPLACE blocks from the response, preserving their order as they appear. Each block should include the full content between the delimiters <<<<<<< SEARCH and >>>>>>> REPLACE, including the SEARCH and REPLACE sections. Return the extracted content verbatim from the response. If multiple blocks are found, return them as a Python-style list of strings. If nothing is found, return None.\n\nHere is the model response: {response}"
                },
                {
                    "type": "code",
                    "exec": "import re\n\ndef check_following(response):\n    search_replace_pattern = r\"<<<<<<< SEARCH\\n(.*?)\\n=======\\n(.*?)\\n>>>>>> REPLACE\"\n    blocks = re.findall(search_replace_pattern, response, re.DOTALL)\n    \n    if not blocks:\n        return False\n    \n    last_search_end = -1\n    for search, _ in blocks:\n        current_search_start = response.find(search)\n        if current_search_start == -1 or current_search_start < last_search_end:\n            return False\n        last_search_end = current_search_start + len(search)\n    \n    return True\n"
                }
            ],
            "score": false
        },
        {
            "id": 98,
            "desc": "If you only need to make small changes to an existing file, consider using replace_in_file instead to avoid unnecessarily rewriting the entire file.",
            "other_info": {
                "from": "system_para_68",
                "condition_desc": "",
                "complete_instruction_para": [],
                "evaluation_type": {
                    "constraint_type": "llm",
                    "explanation": "The constraint requires a semantic understanding of whether the situation involves unnecessarily rewriting the entire file, which is subjective and context-dependent. This cannot be directly validated by code or by extracting specific content."
                },
                "evaluation_generation_success": true
            },
            "dimension": "conditional",
            "is_meta": true,
            "type": [
                "resource"
            ],
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "Does the response use replace_in_file instead of write_to_file to avoid unnecessarily rewriting the entire file? Please answer YES/NO directly and do not enter anything else.\n\nHere is the model response: {response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": 114,
            "desc": "When starting in PLAN MODE, depending on the user's request, you may need to do some information gathering e.g. using read_file or search_files to get more context about the task.",
            "other_info": {
                "from": "system_para_88",
                "condition_desc": "If you are starting in PLAN MODE, then depending on the user's request, you may need to do some information gathering, for example, using read_file or search_files to obtain more context about the task. ",
                "complete_instruction_para": [],
                "evaluation_type": {
                    "constraint_type": "llm",
                    "explanation": "The constraint involves understanding the user's request and determining whether information gathering is necessary, which requires semantic interpretation and subjective assessment of the task's context and requirements."
                },
                "evaluation_generation_success": true
            },
            "dimension": "conditional",
            "is_meta": false,
            "type": [
                "formatting"
            ],
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "Does the model's response indicate that it performed the necessary information gathering, such as using read_file or search_files, to obtain more context about the task when required by the user's request? Please answer YES/NO directly and do not enter anything else.\n\nHere is the model response: {response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": 116,
            "desc": "You may return mermaid diagrams to visually display your understanding.",
            "other_info": {
                "from": "system_para_88",
                "condition_desc": "If it seems appropriate or beneficial, then you may return mermaid diagrams to visually display your understanding. ",
                "complete_instruction_para": [],
                "evaluation_type": {
                    "constraint_type": "llm",
                    "explanation": "Determining whether a mermaid diagram visually displays understanding requires semantic assessment of the diagram's content and its alignment with the intended meaning or context, which is subjective and open-ended."
                },
                "evaluation_generation_success": true
            },
            "dimension": "conditional",
            "is_meta": false,
            "type": [
                "formatting"
            ],
            "evaluation": [
                {
                    "type": "llm_conditional_check",
                    "exec": "Does it seem appropriate or beneficial to return mermaid diagrams to visually display your understanding? Please answer YES/NO directly and do not enter anything else.\n\nHere is the model response: {response}"
                },
                {
                    "type": "llm",
                    "exec": "Does the model's response include a mermaid diagram to visually display its understanding? Please answer YES/NO directly and do not enter anything else.\n\nHere is the model response: {response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": 119,
            "desc": "You might ask the user if they are pleased with this plan, or if they would like to make any changes.",
            "other_info": {
                "from": "system_para_88",
                "condition_desc": "You might ask the user if they are pleased with this plan, or if they would like to make any changes.",
                "complete_instruction_para": [],
                "evaluation_type": {
                    "constraint_type": "llm",
                    "explanation": "Determining whether the response asks the user if they would like to make changes to the plan requires semantic understanding of the intent and phrasing, which is subjective and open-ended. This cannot be directly validated by code or simple extraction logic."
                },
                "evaluation_generation_success": true
            },
            "dimension": "conditional",
            "is_meta": false,
            "type": [
                "semantic"
            ],
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "Does the response explicitly ask the user if they would like to make any changes to the plan? Please answer YES/NO directly and do not enter anything else.\n\nHere is the model response: {response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": 120,
            "desc": "If at any point a mermaid diagram would make your plan clearer to help the user quickly see the structure, you are encouraged to include a Mermaid code block in the response.",
            "other_info": {
                "from": "system_para_88",
                "condition_desc": "",
                "complete_instruction_para": [],
                "evaluation_type": {
                    "constraint_type": "llm",
                    "explanation": "Determining whether a Mermaid code block is included requires semantic understanding of the response's content to identify if it is indeed a valid Mermaid code block and aligns with the intended purpose of clarifying the plan. This is subjective and cannot be directly validated by code alone."
                },
                "evaluation_generation_success": true
            },
            "dimension": "conditional",
            "is_meta": false,
            "type": [
                "formatting"
            ],
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "Does the response include a Mermaid code block that visually represents the plan to make it clearer and help the user quickly see the structure? Please answer YES/NO directly and do not enter anything else.\n\nHere is the model response: {response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": 121,
            "desc": "If you use colors in your mermaid diagrams, be sure to use high contrast colors so the text is readable.",
            "other_info": {
                "from": "system_para_88",
                "condition_desc": "",
                "complete_instruction_para": [],
                "evaluation_type": {
                    "constraint_type": "llm",
                    "explanation": "Determining whether the colors in mermaid diagrams are 'high contrast' and ensure text readability requires subjective semantic assessment of visual design, which is not directly verifiable by code."
                },
                "evaluation_generation_success": true
            },
            "dimension": "conditional",
            "is_meta": false,
            "type": [
                "formatting"
            ],
            "evaluation": [
                {
                    "type": "llm_conditional_check",
                    "exec": "Are colors used in the mermaid diagrams? Please answer YES/NO directly and do not enter anything else.\n\nHere is the model response: {response}"
                },
                {
                    "type": "llm",
                    "exec": "Does the mermaid diagram use high contrast colors to ensure the text is readable? Please answer YES/NO directly and do not enter anything else.\n\nHere is the model response: {response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": 122,
            "desc": "Finally once it seems like you've reached a good plan, ask the user to switch you back to ACT MODE to implement the solution.",
            "other_info": {
                "from": "system_para_88",
                "condition_desc": "",
                "complete_instruction_para": [],
                "evaluation_type": {
                    "constraint_type": "llm",
                    "explanation": "Determining whether the response asks the user to switch back to ACT MODE requires semantic understanding of the phrasing and intent, which is subjective and open-ended. This cannot be directly validated by code or simple extraction."
                },
                "evaluation_generation_success": true
            },
            "dimension": "conditional",
            "is_meta": false,
            "type": [
                "formatting"
            ],
            "evaluation": [
                {
                    "type": "llm_conditional_check",
                    "exec": "Does it seem like you've reached a good plan? Please answer YES/NO directly and do not enter anything else.\n\nHere is the model response: {response}"
                },
                {
                    "type": "llm",
                    "exec": "Does the model response explicitly ask the user to switch back to ACT MODE in order to implement the solution? Please answer YES/NO directly and do not enter anything else.\n\nHere is the model response: {response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": 124,
            "desc": "When the user initially gives you a task, a recursive list of all filepaths in the current working directory ('${cwd.toPosix()}') will be included in environment_details.",
            "other_info": {
                "from": "system_para_91",
                "condition_desc": "If the user initially provides you with a task, then a recursive list of all filepaths in the current working directory ('${cwd.toPosix()}') will be included in the environment_details. ",
                "complete_instruction_para": [],
                "evaluation_type": {
                    "constraint_type": "code",
                    "explanation": "The constraint involves generating a recursive list of filepaths in the current working directory, which can be directly validated using code by accessing the filesystem and comparing the output to the expected format."
                },
                "evaluation_generation_success": true
            },
            "dimension": "conditional",
            "is_meta": false,
            "type": [
                "formatting"
            ],
            "evaluation": [
                {
                    "type": "code",
                    "exec": "import os\n\ndef check_following(response: str) -> bool:\n    cwd = os.getcwd().replace(\"\\\\\", \"/\")\n    return f\"'{cwd}'\" in response or f'\"{cwd}\"' in response\n"
                }
            ],
            "score": false
        },
        {
            "id": 136,
            "desc": "If asked to make edits or improvements, you might analyze the file structure in the initial environment_details to get an overview of the project, then use list_code_definition_names to get further insight using source code definitions for files located in relevant directories, then read_file to examine the contents of relevant files, analyze the code and suggest improvements or make necessary edits, then use the replace_in_file tool to implement changes.",
            "other_info": {
                "from": "system_para_91",
                "condition_desc": "",
                "complete_instruction_para": [],
                "evaluation_type": {
                    "constraint_type": "llm",
                    "explanation": "The constraint involves analyzing the code structure, understanding its relevance, suggesting improvements, and making edits, which requires semantic understanding and subjective assessment of the code's quality and purpose. This cannot be validated directly by code or through extraction alone."
                },
                "evaluation_generation_success": true
            },
            "dimension": "conditional",
            "is_meta": false,
            "type": [
                "semantic"
            ],
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "Does the model response demonstrate that it analyzed the file structure, used list_code_definition_names for further insight, examined relevant files with read_file, analyzed the code, suggested improvements or edits, and implemented changes using replace_in_file as specified in the constraint? Please answer YES/NO directly and do not enter anything else.\n\nHere is the model response: {response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": 137,
            "desc": "If supportsComputerUse, you can use the browser_action tool to interact with websites (including html files and locally running development servers) through a Puppeteer-controlled browser when you feel it is necessary in accomplishing the user's task.",
            "other_info": {
                "from": "system_para_91",
                "condition_desc": "",
                "complete_instruction_para": [],
                "evaluation_type": {
                    "constraint_type": "llm",
                    "explanation": "The constraint involves determining whether the use of the browser_action tool is 'necessary' for accomplishing the user's task, which requires a semantic and subjective understanding of the task's requirements and context. This cannot be directly validated by code or through structured extraction."
                },
                "evaluation_generation_success": true
            },
            "dimension": "conditional",
            "is_meta": false,
            "type": [
                "resource"
            ],
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "Does the model response explicitly indicate that the browser_action tool was used through a Puppeteer-controlled browser to interact with websites when necessary for accomplishing the user's task? Please answer YES/NO directly and do not enter anything else.\n\nHere is the model response: {response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": 145,
            "desc": "Analyze the surrounding code when using the search_files tool to better understand the matches.",
            "other_info": {
                "from": "system_para_94",
                "condition_desc": "",
                "complete_instruction_para": [],
                "evaluation_type": {
                    "constraint_type": "llm",
                    "explanation": "The constraint requires semantic understanding of the surrounding code to assess how it relates to the matches, which involves subjective interpretation and contextual analysis that can only be performed by an LLM."
                },
                "evaluation_generation_success": true
            },
            "dimension": "conditional",
            "is_meta": false,
            "type": [
                "semantic"
            ],
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "Does the model's response demonstrate that it has analyzed the surrounding code when using the search_files tool to better understand the matches? Please answer YES/NO directly and do not enter anything else.\n\nHere is the model response: {response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": 147,
            "desc": "When creating a new project, organize all new files within a dedicated project directory unless the user specifies otherwise.",
            "other_info": {
                "from": "system_para_94",
                "condition_desc": "",
                "complete_instruction_para": [],
                "evaluation_type": {
                    "constraint_type": "llm",
                    "explanation": "Determining whether files are organized within a dedicated project directory requires semantic understanding of the context and intent, especially to assess whether the user has specified otherwise. This involves subjective interpretation that cannot be directly validated by code."
                },
                "evaluation_generation_success": true
            },
            "dimension": "conditional",
            "is_meta": false,
            "type": [
                "semantic"
            ],
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "Are all new files organized within a dedicated project directory unless the user has specified otherwise? Please answer YES/NO directly and do not enter anything else.\n\nHere is the model response: {response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": 149,
            "desc": "Use appropriate file paths when creating files, as the write_to_file tool will automatically create any necessary directories.",
            "other_info": {
                "from": "system_para_94",
                "condition_desc": "",
                "complete_instruction_para": [],
                "evaluation_type": {
                    "constraint_type": "llm",
                    "explanation": "Determining whether 'appropriate file paths' are used requires a semantic understanding of what constitutes 'appropriate' in the given context, which is subjective and cannot be directly validated by code."
                },
                "evaluation_generation_success": true
            },
            "dimension": "conditional",
            "is_meta": false,
            "type": [
                "formatting"
            ],
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "Does the response use appropriate file paths when creating files, ensuring that the write_to_file tool can automatically create any necessary directories? Please answer YES/NO directly and do not enter anything else.\n\nHere is the model response: {response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": 151,
            "desc": "Be sure to consider the type of project (e.g., Python, JavaScript, web application) when determining the appropriate structure and files to include.",
            "other_info": {
                "from": "system_para_94",
                "condition_desc": "",
                "complete_instruction_para": [],
                "evaluation_type": {
                    "constraint_type": "llm",
                    "explanation": "The constraint requires semantic understanding of the type of project and its implications for structure and files, which involves subjective and open-ended reasoning that can only be assessed by an LLM."
                },
                "evaluation_generation_success": true
            },
            "dimension": "conditional",
            "is_meta": false,
            "type": [
                "semantic"
            ],
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "Does the model response explicitly consider the type of project (e.g., Python, JavaScript, web application) when determining the appropriate structure and files to include? Please answer YES/NO directly and do not enter anything else.\n\nHere is the model response: {response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": 154,
            "desc": "When making changes to code, ensure that your changes are compatible with the existing codebase and that they follow the project's coding standards and best practices.",
            "other_info": {
                "from": "system_para_94",
                "condition_desc": "",
                "complete_instruction_para": [],
                "evaluation_type": {
                    "constraint_type": "llm",
                    "explanation": "The constraint involves assessing compatibility with an existing codebase and adherence to coding standards, which requires semantic understanding of the codebase, the changes, and the project's best practices. This is subjective and cannot be directly validated by code."
                },
                "evaluation_generation_success": true
            },
            "dimension": "conditional",
            "is_meta": false,
            "type": [
                "semantic"
            ],
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "Does the model's response ensure that the proposed code changes are compatible with the existing codebase and adhere to the project's coding standards and best practices? Please answer YES/NO directly and do not enter anything else.\n\nHere is the model response: {response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": 155,
            "desc": "When you want to modify a file, use the replace_in_file or write_to_file tool directly with the desired changes. You do not need to display the changes before using the tool.",
            "other_info": {
                "from": "system_para_94",
                "condition_desc": "",
                "complete_instruction_para": [],
                "evaluation_type": {
                    "constraint_type": "llm",
                    "explanation": "The constraint requires semantic understanding of whether the instruction to use specific tools (replace_in_file or write_to_file) is followed correctly, which involves assessing the appropriateness of the action in context. This is subjective and cannot be directly validated by code."
                },
                "evaluation_generation_success": true
            },
            "dimension": "conditional",
            "is_meta": false,
            "type": [
                "semantic"
            ],
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "Does the model response use the replace_in_file or write_to_file tool directly with the desired changes, without displaying the changes beforehand? Please answer YES/NO directly and do not enter anything else.\n\nHere is the model response: {response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": 158,
            "desc": "When you've completed your task, you must use the attempt_completion tool to present the result to the user.",
            "other_info": {
                "from": "system_para_94",
                "condition_desc": "",
                "complete_instruction_para": [],
                "evaluation_type": {
                    "constraint_type": "llm",
                    "explanation": "The constraint requires semantic understanding to assess whether the response appropriately uses the 'attempt_completion' tool to present the result to the user. This involves interpreting the intent and correctness of the action, which is subjective and cannot be directly validated by code."
                },
                "evaluation_generation_success": true
            },
            "dimension": "conditional",
            "is_meta": false,
            "type": [
                "resource"
            ],
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "Does the model's response explicitly state that the attempt_completion tool was used to present the result to the user? Please answer YES/NO directly and do not enter anything else.\n\nHere is the model response: {response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": 161,
            "desc": "Use the ask_followup_question tool only when you need additional details to complete a task.",
            "other_info": {
                "from": "system_para_94",
                "condition_desc": "",
                "complete_instruction_para": [],
                "evaluation_type": {
                    "constraint_type": "llm",
                    "explanation": "Determining whether the ask_followup_question tool is needed requires semantic understanding of the task and the context, which is subjective and open-ended. This cannot be directly validated by code or by extracting specific content."
                },
                "evaluation_generation_success": true
            },
            "dimension": "conditional",
            "is_meta": false,
            "type": [
                "resource"
            ],
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "Does the model response use the ask_followup_question tool only when additional details are needed to complete the task? Please answer YES/NO directly and do not enter anything else.\n\nHere is the model response: {response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": 198,
            "desc": "When deciding if the parameter can be inferred, carefully consider all the context to see if it supports a specific value.",
            "other_info": {
                "from": "system_para_101",
                "condition_desc": "If you are deciding whether a parameter can be inferred, then carefully consider all of the context to determine if it supports a specific value.",
                "complete_instruction_para": []
            },
            "dimension": "conditional",
            "is_meta": false,
            "type": [
                "semantic"
            ],
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "When determining whether the parameter can be inferred, has the model carefully considered all the context to check if it supports a specific value? Please answer YES/NO directly and do not enter anything else.\n\nHere is the model response: {response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": 203,
            "desc": "You may provide a CLI command to showcase the result of your task (e.g., `open index.html` for web development tasks).",
            "other_info": {
                "from": "system_para_99",
                "condition_desc": "",
                "complete_instruction_para": [],
                "evaluation_type": {
                    "constraint_type": "code",
                    "explanation": "The constraint involves providing a CLI command, which is a straightforward and objective task that can be directly validated by checking the format and correctness of the command syntax."
                },
                "evaluation_generation_success": true
            },
            "dimension": "conditional",
            "is_meta": false,
            "type": [
                "formatting"
            ],
            "evaluation": [
                {
                    "type": "code",
                    "exec": "import re\n\ndef check_following(response: str) -> bool:\n    return bool(re.search(r\"^Provide a CLI command to showcase the result of your task\\.$\", response.strip()))\n"
                }
            ],
            "score": false
        },
        {
            "id": 2,
            "desc": "在写代码时你需要综合考虑最终目标<final_goal></final_goal>和本轮的任务<task></task>。",
            "other_info": {
                "from": "system_para_1",
                "first_task": "将9000万元转换为亿元，并保留两位小数。"
            },
            "is_meta": true,
            "type": "semantic",
            "dimension": "conditional",
            "dependency": [],
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "检查 response 是否严格按 task 要求执行核心操作（允许 print 辅助输出），没有执行任何其他额外操作，直接回答 YES 或 NO。\n\ntask: 将9000万元转换为亿元，并保留两位小数。\n\nresponse:{response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": 44,
            "desc": "Use <lov-success> to confirm successful operations.",
            "other_info": {
                "from": "system_para_0",
                "condition_desc": "If the model is confirming successful operations, then wrap them in <lov-success>.",
                "complete_instruction_para": [],
                "evaluation_type": {
                    "constraint_type": "code",
                    "explanation": "The constraint specifies the use of a specific tag (<lov-success>) to confirm successful operations, which can be directly validated by checking the presence and correct usage of the tag in the response."
                },
                "evaluation_generation_success": true
            },
            "dimension": "conditional",
            "is_meta": false,
            "type": [
                "semantic"
            ],
            "evaluation": [
                {
                    "type": "llm_conditional_check",
                    "exec": "Is the model confirming successful operations? Please answer YES/NO directly and do not enter anything else.\n\nHere is the model response: {response}"
                },
                {
                    "type": "code",
                    "exec": "import re\n\ndef check_following(response: str) -> bool:\n    return bool(re.search(r\"<lov-success>.*?</lov-success>\", response, re.DOTALL))\n"
                }
            ],
            "score": false
        },
        {
            "id": 74,
            "desc": "If you added new files, remember that you need to implement them fully.",
            "other_info": {
                "from": "system_para_138",
                "condition_desc": "If you added new files, then remember that you need to implement them fully.",
                "complete_instruction_para": [],
                "evaluation_type": {
                    "constraint_type": "llm",
                    "explanation": "The constraint requires checking whether new files are fully implemented, which can be validated directly by code by ensuring all necessary content is present in the files."
                },
                "evaluation_generation_success": true
            },
            "dimension": "conditional",
            "is_meta": false,
            "type": [
                "semantic"
            ],
            "evaluation": [
                {
                    "type": "llm_conditional_check",
                    "exec": "Did the model add any new files? Please answer YES/NO directly and do not enter anything else.\n\nHere is the model response: {response}"
                },
                {
                    "type": "llm",
                    "exec": "Does the model implement new files fully? Please answer YES/NO directly and do not enter anything else.\n\nHere is the model response: {response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": 99,
            "desc": "Clearly communicate to the user that some specific features were not implemented.",
            "other_info": {
                "from": "system_para_207",
                "condition_desc": "If a user asks for many features at once, then you do not have to implement them all as long as you clearly communicate to the user that you didn't implement some specific features.",
                "complete_instruction_para": [],
                "evaluation_type": {
                    "constraint_type": "llm",
                    "explanation": "The constraint requires a semantic understanding of whether the response clearly communicates to the user which features were not implemented. This involves subjective assessment of clarity and completeness, which cannot be directly validated by code."
                },
                "evaluation_generation_success": true
            },
            "dimension": "conditional",
            "is_meta": false,
            "type": [
                "semantic"
            ],
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "Does the response clearly communicate to the user which specific features were not implemented, if not all features were implemented? Please answer YES/NO directly and do not enter anything else.\n\nHere is the model response: {response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": 114,
            "desc": "Make sure to close all tags when writing files, with a line break before the closing tag.",
            "other_info": {
                "from": "system_para_214",
                "condition_desc": "If you do a <lov-write> operation, then make sure to close all tags when writing files, with a line break before the closing tag.",
                "complete_instruction_para": [
                    "system_para_213"
                ],
                "evaluation_type": {
                    "constraint_type": "code",
                    "explanation": "The constraint involves checking for the presence of a line break before closing tags, which can be directly validated using code by analyzing the file's syntax and formatting."
                },
                "evaluation_generation_success": true
            },
            "dimension": "conditional",
            "is_meta": false,
            "type": [
                "formatting"
            ],
            "evaluation": [
                {
                    "type": "llm_conditional_check",
                    "exec": "Is the model performing a <lov-write> operation? Please answer YES/NO directly and do not enter anything else.\n\nHere is the model response: {response}"
                },
                {
                    "type": "code",
                    "exec": "\nimport re\nfrom typing import List\n\ndef check_following(response: str) -> bool:\n    # Find all opening and closing tags\n    tag_pattern = r'<([^>]+)>'\n    tags = re.findall(tag_pattern, response)\n    \n    # Stack to keep track of opening tags\n    stack: List[str] = []\n    \n    for tag in tags:\n        # Skip self-closing tags\n        if tag.endswith('/'):\n            continue\n            \n        # Handle closing tags\n        if tag.startswith('/'):\n            # If stack is empty, we found a closing tag without matching opening tag\n            if not stack:\n                return False\n                \n            # Get the expected opening tag name\n            expected_tag = stack[-1]\n            # Remove the '/' from the closing tag\n            actual_tag = tag[1:]\n            \n            # Check if tags match\n            if expected_tag != actual_tag:\n                return False\n                \n            # Remove the matched opening tag from stack\n            stack.pop()\n        else:\n            # Add opening tag to stack\n            stack.append(tag)\n    \n    # If stack is not empty, we have unclosed tags\n    return len(stack) == 0\n"
                }
            ],
            "score": false
        },
        {
            "id": 6,
            "desc": "NEVER output code to the USER, unless requested.  Instead use one of the code edit tools to implement the change.",
            "other_info": {
                "from": "system_para_3",
                "condition_desc": "",
                "complete_instruction_para": [],
                "evaluation_type": {
                    "constraint_type": "code",
                    "explanation": "This constraint can be validated directly by checking whether the output contains code or a call to a code edit tool. It involves simple logic and does not require semantic understanding or content extraction."
                },
                "evaluation_generation_success": true
            },
            "dimension": "conditional",
            "is_meta": false,
            "type": [
                "semantic"
            ],
            "evaluation": [
                {
                    "type": "code",
                    "exec": "\ndef check_following(response: str) -> bool:\n    function_names = [\n        \"codebase_search\",\n        \"read_file\",\n        \"list_dir\",\n        \"grep_search\",\n        \"file_search\",\n        \"web_search\",\n    ]\n    return any(name in response for name in function_names)\n"
                }
            ],
            "score": false
        },
        {
            "id": 26,
            "desc": "When using multiple SEARCH/REPLACE blocks, list them in the order they appear in the file.",
            "other_info": {
                "from": "system_para_14",
                "condition_desc": "If you are using multiple SEARCH/REPLACE blocks, then list them in the order in which they are positioned within the file. ",
                "complete_instruction_para": [],
                "evaluation_type": {
                    "constraint_type": "llm_assisted_code",
                    "explanation": "Validating this constraint requires extracting the sequence of SEARCH/REPLACE blocks from the input and checking their order against the file's content. Code can perform the validation, but it first needs the specific blocks to be identified and extracted, which may require LLM assistance."
                },
                "evaluation_generation_success": true
            },
            "dimension": "conditional",
            "is_meta": false,
            "type": [
                "formatting"
            ],
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "Extract all SEARCH/REPLACE blocks from the response, preserving their order as they appear. Each block should include the full content between the delimiters <<<<<<< SEARCH and >>>>>>> REPLACE, including the SEARCH and REPLACE sections. Return the extracted content verbatim from the response. If multiple blocks are found, return them as a Python-style list of strings. If nothing is found, return None.\n\nHere is the model response: {response}"
                },
                {
                    "type": "code",
                    "exec": "import re\n\ndef check_following(response):\n    search_replace_pattern = r\"<<<<<<< SEARCH\\n(.*?)\\n=======\\n(.*?)\\n>>>>>> REPLACE\"\n    blocks = re.findall(search_replace_pattern, response, re.DOTALL)\n    \n    if not blocks:\n        return False\n    \n    last_search_end = -1\n    for search, _ in blocks:\n        current_search_start = response.find(search)\n        if current_search_start == -1 or current_search_start < last_search_end:\n            return False\n        last_search_end = current_search_start + len(search)\n    \n    return True\n"
                }
            ],
            "score": false
        },
        {
            "id": 114,
            "desc": "When starting in PLAN MODE, depending on the user's request, you may need to do some information gathering e.g. using read_file or search_files to get more context about the task.",
            "other_info": {
                "from": "system_para_88",
                "condition_desc": "If you are starting in PLAN MODE, then depending on the user's request, you may need to do some information gathering, for example, using read_file or search_files to obtain more context about the task. ",
                "complete_instruction_para": [],
                "evaluation_type": {
                    "constraint_type": "llm",
                    "explanation": "The constraint involves understanding the user's request and determining whether information gathering is necessary, which requires semantic interpretation and subjective assessment of the task's context and requirements."
                },
                "evaluation_generation_success": true
            },
            "dimension": "conditional",
            "is_meta": false,
            "type": [
                "formatting"
            ],
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "Does the model's response indicate that it performed the necessary information gathering, such as using read_file or search_files, to obtain more context about the task when required by the user's request? Please answer YES/NO directly and do not enter anything else.\n\nHere is the model response: {response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": 119,
            "desc": "You might ask the user if they are pleased with this plan, or if they would like to make any changes.",
            "other_info": {
                "from": "system_para_88",
                "condition_desc": "You might ask the user if they are pleased with this plan, or if they would like to make any changes.",
                "complete_instruction_para": [],
                "evaluation_type": {
                    "constraint_type": "llm",
                    "explanation": "Determining whether the response asks the user if they would like to make changes to the plan requires semantic understanding of the intent and phrasing, which is subjective and open-ended. This cannot be directly validated by code or simple extraction logic."
                },
                "evaluation_generation_success": true
            },
            "dimension": "conditional",
            "is_meta": false,
            "type": [
                "semantic"
            ],
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "Does the response explicitly ask the user if they would like to make any changes to the plan? Please answer YES/NO directly and do not enter anything else.\n\nHere is the model response: {response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": 120,
            "desc": "If at any point a mermaid diagram would make your plan clearer to help the user quickly see the structure, you are encouraged to include a Mermaid code block in the response.",
            "other_info": {
                "from": "system_para_88",
                "condition_desc": "",
                "complete_instruction_para": [],
                "evaluation_type": {
                    "constraint_type": "llm",
                    "explanation": "Determining whether a Mermaid code block is included requires semantic understanding of the response's content to identify if it is indeed a valid Mermaid code block and aligns with the intended purpose of clarifying the plan. This is subjective and cannot be directly validated by code alone."
                },
                "evaluation_generation_success": true
            },
            "dimension": "conditional",
            "is_meta": false,
            "type": [
                "formatting"
            ],
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "Does the response include a Mermaid code block that visually represents the plan to make it clearer and help the user quickly see the structure? Please answer YES/NO directly and do not enter anything else.\n\nHere is the model response: {response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": 121,
            "desc": "If you use colors in your mermaid diagrams, be sure to use high contrast colors so the text is readable.",
            "other_info": {
                "from": "system_para_88",
                "condition_desc": "",
                "complete_instruction_para": [],
                "evaluation_type": {
                    "constraint_type": "llm",
                    "explanation": "Determining whether the colors in mermaid diagrams are 'high contrast' and ensure text readability requires subjective semantic assessment of visual design, which is not directly verifiable by code."
                },
                "evaluation_generation_success": true
            },
            "dimension": "conditional",
            "is_meta": false,
            "type": [
                "formatting"
            ],
            "evaluation": [
                {
                    "type": "llm_conditional_check",
                    "exec": "Are colors used in the mermaid diagrams? Please answer YES/NO directly and do not enter anything else.\n\nHere is the model response: {response}"
                },
                {
                    "type": "llm",
                    "exec": "Does the mermaid diagram use high contrast colors to ensure the text is readable? Please answer YES/NO directly and do not enter anything else.\n\nHere is the model response: {response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": 122,
            "desc": "Finally once it seems like you've reached a good plan, ask the user to switch you back to ACT MODE to implement the solution.",
            "other_info": {
                "from": "system_para_88",
                "condition_desc": "",
                "complete_instruction_para": [],
                "evaluation_type": {
                    "constraint_type": "llm",
                    "explanation": "Determining whether the response asks the user to switch back to ACT MODE requires semantic understanding of the phrasing and intent, which is subjective and open-ended. This cannot be directly validated by code or simple extraction."
                },
                "evaluation_generation_success": true
            },
            "dimension": "conditional",
            "is_meta": false,
            "type": [
                "formatting"
            ],
            "evaluation": [
                {
                    "type": "llm_conditional_check",
                    "exec": "Does it seem like you've reached a good plan? Please answer YES/NO directly and do not enter anything else.\n\nHere is the model response: {response}"
                },
                {
                    "type": "llm",
                    "exec": "Does the model response explicitly ask the user to switch back to ACT MODE in order to implement the solution? Please answer YES/NO directly and do not enter anything else.\n\nHere is the model response: {response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": 132,
            "desc": "When you need to execute a CLI command, you must provide a clear explanation of what the command does.",
            "other_info": {
                "from": "system_para_91",
                "condition_desc": "",
                "complete_instruction_para": [],
                "evaluation_type": {
                    "constraint_type": "llm",
                    "explanation": "Determining whether the explanation of a CLI command is clear requires semantic and subjective understanding, which can only be assessed by an LLM."
                },
                "evaluation_generation_success": true
            },
            "dimension": "conditional",
            "is_meta": false,
            "type": [
                "semantic"
            ],
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "Does the response include a clear explanation of what the CLI command does? Please answer YES/NO directly and do not enter anything else.\n\nHere is the model response: {response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": 137,
            "desc": "If supportsComputerUse, you can use the browser_action tool to interact with websites (including html files and locally running development servers) through a Puppeteer-controlled browser when you feel it is necessary in accomplishing the user's task.",
            "other_info": {
                "from": "system_para_91",
                "condition_desc": "",
                "complete_instruction_para": [],
                "evaluation_type": {
                    "constraint_type": "llm",
                    "explanation": "The constraint involves determining whether the use of the browser_action tool is 'necessary' for accomplishing the user's task, which requires a semantic and subjective understanding of the task's requirements and context. This cannot be directly validated by code or through structured extraction."
                },
                "evaluation_generation_success": true
            },
            "dimension": "conditional",
            "is_meta": false,
            "type": [
                "resource"
            ],
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "Does the model response explicitly indicate that the browser_action tool was used through a Puppeteer-controlled browser to interact with websites when necessary for accomplishing the user's task? Please answer YES/NO directly and do not enter anything else.\n\nHere is the model response: {response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": 145,
            "desc": "Analyze the surrounding code when using the search_files tool to better understand the matches.",
            "other_info": {
                "from": "system_para_94",
                "condition_desc": "",
                "complete_instruction_para": [],
                "evaluation_type": {
                    "constraint_type": "llm",
                    "explanation": "The constraint requires semantic understanding of the surrounding code to assess how it relates to the matches, which involves subjective interpretation and contextual analysis that can only be performed by an LLM."
                },
                "evaluation_generation_success": true
            },
            "dimension": "conditional",
            "is_meta": false,
            "type": [
                "semantic"
            ],
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "Does the model's response demonstrate that it has analyzed the surrounding code when using the search_files tool to better understand the matches? Please answer YES/NO directly and do not enter anything else.\n\nHere is the model response: {response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": 147,
            "desc": "When creating a new project, organize all new files within a dedicated project directory unless the user specifies otherwise.",
            "other_info": {
                "from": "system_para_94",
                "condition_desc": "",
                "complete_instruction_para": [],
                "evaluation_type": {
                    "constraint_type": "llm",
                    "explanation": "Determining whether files are organized within a dedicated project directory requires semantic understanding of the context and intent, especially to assess whether the user has specified otherwise. This involves subjective interpretation that cannot be directly validated by code."
                },
                "evaluation_generation_success": true
            },
            "dimension": "conditional",
            "is_meta": false,
            "type": [
                "semantic"
            ],
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "Are all new files organized within a dedicated project directory unless the user has specified otherwise? Please answer YES/NO directly and do not enter anything else.\n\nHere is the model response: {response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": 149,
            "desc": "Use appropriate file paths when creating files, as the write_to_file tool will automatically create any necessary directories.",
            "other_info": {
                "from": "system_para_94",
                "condition_desc": "",
                "complete_instruction_para": [],
                "evaluation_type": {
                    "constraint_type": "llm",
                    "explanation": "Determining whether 'appropriate file paths' are used requires a semantic understanding of what constitutes 'appropriate' in the given context, which is subjective and cannot be directly validated by code."
                },
                "evaluation_generation_success": true
            },
            "dimension": "conditional",
            "is_meta": false,
            "type": [
                "formatting"
            ],
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "Does the response use appropriate file paths when creating files, ensuring that the write_to_file tool can automatically create any necessary directories? Please answer YES/NO directly and do not enter anything else.\n\nHere is the model response: {response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": 151,
            "desc": "Be sure to consider the type of project (e.g., Python, JavaScript, web application) when determining the appropriate structure and files to include.",
            "other_info": {
                "from": "system_para_94",
                "condition_desc": "",
                "complete_instruction_para": [],
                "evaluation_type": {
                    "constraint_type": "llm",
                    "explanation": "The constraint requires semantic understanding of the type of project and its implications for structure and files, which involves subjective and open-ended reasoning that can only be assessed by an LLM."
                },
                "evaluation_generation_success": true
            },
            "dimension": "conditional",
            "is_meta": false,
            "type": [
                "semantic"
            ],
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "Does the model response explicitly consider the type of project (e.g., Python, JavaScript, web application) when determining the appropriate structure and files to include? Please answer YES/NO directly and do not enter anything else.\n\nHere is the model response: {response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": 153,
            "desc": "When making changes to code, always consider the context in which the code is being used.",
            "other_info": {
                "from": "system_para_94",
                "condition_desc": "",
                "complete_instruction_para": [],
                "evaluation_type": {
                    "constraint_type": "llm",
                    "explanation": "The constraint requires semantic understanding of the code's context, which involves subjective and open-ended reasoning about how the code interacts with its environment or use case. This cannot be directly validated by code or extracted content alone."
                },
                "evaluation_generation_success": true
            },
            "dimension": "conditional",
            "is_meta": false,
            "type": [
                "semantic"
            ],
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "Does the model's response demonstrate consideration of the context in which the code is being used when making changes to the code? Please answer YES/NO directly and do not enter anything else.\n\nHere is the model response: {response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": 154,
            "desc": "When making changes to code, ensure that your changes are compatible with the existing codebase and that they follow the project's coding standards and best practices.",
            "other_info": {
                "from": "system_para_94",
                "condition_desc": "",
                "complete_instruction_para": [],
                "evaluation_type": {
                    "constraint_type": "llm",
                    "explanation": "The constraint involves assessing compatibility with an existing codebase and adherence to coding standards, which requires semantic understanding of the codebase, the changes, and the project's best practices. This is subjective and cannot be directly validated by code."
                },
                "evaluation_generation_success": true
            },
            "dimension": "conditional",
            "is_meta": false,
            "type": [
                "semantic"
            ],
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "Does the model's response ensure that the proposed code changes are compatible with the existing codebase and adhere to the project's coding standards and best practices? Please answer YES/NO directly and do not enter anything else.\n\nHere is the model response: {response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": 155,
            "desc": "When you want to modify a file, use the replace_in_file or write_to_file tool directly with the desired changes. You do not need to display the changes before using the tool.",
            "other_info": {
                "from": "system_para_94",
                "condition_desc": "",
                "complete_instruction_para": [],
                "evaluation_type": {
                    "constraint_type": "llm",
                    "explanation": "The constraint requires semantic understanding of whether the instruction to use specific tools (replace_in_file or write_to_file) is followed correctly, which involves assessing the appropriateness of the action in context. This is subjective and cannot be directly validated by code."
                },
                "evaluation_generation_success": true
            },
            "dimension": "conditional",
            "is_meta": false,
            "type": [
                "semantic"
            ],
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "Does the model response use the replace_in_file or write_to_file tool directly with the desired changes, without displaying the changes beforehand? Please answer YES/NO directly and do not enter anything else.\n\nHere is the model response: {response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": 158,
            "desc": "When you've completed your task, you must use the attempt_completion tool to present the result to the user.",
            "other_info": {
                "from": "system_para_94",
                "condition_desc": "",
                "complete_instruction_para": [],
                "evaluation_type": {
                    "constraint_type": "llm",
                    "explanation": "The constraint requires semantic understanding to assess whether the response appropriately uses the 'attempt_completion' tool to present the result to the user. This involves interpreting the intent and correctness of the action, which is subjective and cannot be directly validated by code."
                },
                "evaluation_generation_success": true
            },
            "dimension": "conditional",
            "is_meta": false,
            "type": [
                "resource"
            ],
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "Does the model's response explicitly state that the attempt_completion tool was used to present the result to the user? Please answer YES/NO directly and do not enter anything else.\n\nHere is the model response: {response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": 161,
            "desc": "Use the ask_followup_question tool only when you need additional details to complete a task.",
            "other_info": {
                "from": "system_para_94",
                "condition_desc": "",
                "complete_instruction_para": [],
                "evaluation_type": {
                    "constraint_type": "llm",
                    "explanation": "Determining whether the ask_followup_question tool is needed requires semantic understanding of the task and the context, which is subjective and open-ended. This cannot be directly validated by code or by extracting specific content."
                },
                "evaluation_generation_success": true
            },
            "dimension": "conditional",
            "is_meta": false,
            "type": [
                "resource"
            ],
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "Does the model response use the ask_followup_question tool only when additional details are needed to complete the task? Please answer YES/NO directly and do not enter anything else.\n\nHere is the model response: {response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": 198,
            "desc": "When deciding if the parameter can be inferred, carefully consider all the context to see if it supports a specific value.",
            "other_info": {
                "from": "system_para_101",
                "condition_desc": "If you are deciding whether a parameter can be inferred, then carefully consider all of the context to determine if it supports a specific value.",
                "complete_instruction_para": []
            },
            "dimension": "conditional",
            "is_meta": false,
            "type": [
                "semantic"
            ],
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "When determining whether the parameter can be inferred, has the model carefully considered all the context to check if it supports a specific value? Please answer YES/NO directly and do not enter anything else.\n\nHere is the model response: {response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": 58,
            "desc": "Proceed with code edits only if the user explicitly requests changes or new features that have not already been implemented.",
            "other_info": {
                "from": "system_para_135",
                "condition_desc": "If the user explicitly requests changes or new features that have not already been implemented, then proceed with code edits.",
                "complete_instruction_para": [
                    ""
                ],
                "evaluation_type": {
                    "constraint_type": "llm",
                    "explanation": "Determining whether to proceed with code edits requires semantic understanding of the user's intent, which involves assessing whether their request explicitly indicates a desire for changes or new features. This is subjective and requires open-ended interpretation of the user's language."
                },
                "evaluation_generation_success": true
            },
            "dimension": "conditional",
            "is_meta": true,
            "type": [
                "semantic"
            ],
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "Does the model response explicitly proceed with code edits as instructed? Please answer YES/NO directly and do not enter anything else.\n\nHere is the model response: {response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": 74,
            "desc": "If you added new files, remember that you need to implement them fully.",
            "other_info": {
                "from": "system_para_138",
                "condition_desc": "If you added new files, then remember that you need to implement them fully.",
                "complete_instruction_para": [],
                "evaluation_type": {
                    "constraint_type": "llm",
                    "explanation": "The constraint requires checking whether new files are fully implemented, which can be validated directly by code by ensuring all necessary content is present in the files."
                },
                "evaluation_generation_success": true
            },
            "dimension": "conditional",
            "is_meta": false,
            "type": [
                "semantic"
            ],
            "evaluation": [
                {
                    "type": "llm_conditional_check",
                    "exec": "Did the model add any new files? Please answer YES/NO directly and do not enter anything else.\n\nHere is the model response: {response}"
                },
                {
                    "type": "llm",
                    "exec": "Does the model implement new files fully? Please answer YES/NO directly and do not enter anything else.\n\nHere is the model response: {response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": 99,
            "desc": "Clearly communicate to the user that some specific features were not implemented.",
            "other_info": {
                "from": "system_para_207",
                "condition_desc": "If a user asks for many features at once, then you do not have to implement them all as long as you clearly communicate to the user that you didn't implement some specific features.",
                "complete_instruction_para": [],
                "evaluation_type": {
                    "constraint_type": "llm",
                    "explanation": "The constraint requires a semantic understanding of whether the response clearly communicates to the user which features were not implemented. This involves subjective assessment of clarity and completeness, which cannot be directly validated by code."
                },
                "evaluation_generation_success": true
            },
            "dimension": "conditional",
            "is_meta": false,
            "type": [
                "semantic"
            ],
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "Does the response clearly communicate to the user which specific features were not implemented, if not all features were implemented? Please answer YES/NO directly and do not enter anything else.\n\nHere is the model response: {response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": 100,
            "desc": "If there's a large contiguous block of unchanged code, you may use the comment `// ... keep existing code` (in English) for large unchanged code sections.",
            "other_info": {
                "from": "system_para_209",
                "condition_desc": "If there's a large contiguous block of unchanged code, then you may use the comment `// ... keep existing code` (in English) for large unchanged code sections.",
                "complete_instruction_para": [],
                "evaluation_type": {
                    "constraint_type": "llm",
                    "explanation": "The constraint requires semantic understanding of whether the comment `// ... keep existing code` is appropriate for a given block of code, which involves subjective judgment about whether the code is 'large' and 'unchanged'. This cannot be directly validated by code or by extracting specific sections; it requires an LLM to assess the semantic appropriateness."
                },
                "evaluation_generation_success": true
            },
            "dimension": "conditional",
            "is_meta": false,
            "type": [
                "formatting"
            ],
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "Does the response use the exact comment `// ... keep existing code` (in English) for large unchanged code sections where applicable? Please answer YES/NO directly and do not enter anything else.\n\nHere is the model response: {response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": 102,
            "desc": "The comment must contain the exact string \"... keep existing code\" because a regex will look for this specific pattern.",
            "other_info": {
                "from": "system_para_209",
                "condition_desc": "If there's a large contiguous block of unchanged code, then the comment must contain the exact string \"... keep existing code\" because a regex will look for this specific pattern.",
                "complete_instruction_para": [],
                "evaluation_type": {
                    "constraint_type": "code",
                    "explanation": "The constraint specifies the exact string that must be present ('... keep existing code'), which can be validated directly by checking for the presence of this string using a regex or string comparison in code."
                },
                "evaluation_generation_success": true
            },
            "dimension": "conditional",
            "is_meta": false,
            "type": [
                "semantic"
            ],
            "evaluation": [
                {
                    "type": "code",
                    "exec": "import re\n\ndef check_following(response: str) -> bool:\n    return bool(re.search(r'\\.\\.\\. keep existing code', response))\n"
                }
            ],
            "score": false
        },
        {
            "id": 103,
            "desc": "You may add additional details about what existing code is being kept AFTER this comment, e.g. `// ... keep existing code (definitions of the functions A and B)`.",
            "other_info": {
                "from": "system_para_209",
                "condition_desc": "If there's a large contiguous block of unchanged code, then you may add additional details about what existing code is being kept AFTER the comment, e.g. `// ... keep existing code (definitions of the functions A and B)`.",
                "complete_instruction_para": [],
                "evaluation_type": {
                    "constraint_type": "llm",
                    "explanation": "The constraint requires semantic understanding to assess whether the additional details provided after the comment are relevant and correctly describe the existing code. This involves subjective judgment and cannot be directly validated by code or simple extraction."
                },
                "evaluation_generation_success": true
            },
            "dimension": "conditional",
            "is_meta": false,
            "type": [
                "semantic"
            ],
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "Does the model response include the comment `// ... keep existing code` followed by additional details about what existing code is being kept? Please answer YES/NO directly and do not enter anything else.\n\nHere is the model response: {response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": 114,
            "desc": "Make sure to close all tags when writing files, with a line break before the closing tag.",
            "other_info": {
                "from": "system_para_214",
                "condition_desc": "If you do a <lov-write> operation, then make sure to close all tags when writing files, with a line break before the closing tag.",
                "complete_instruction_para": [
                    "system_para_213"
                ],
                "evaluation_type": {
                    "constraint_type": "code",
                    "explanation": "The constraint involves checking for the presence of a line break before closing tags, which can be directly validated using code by analyzing the file's syntax and formatting."
                },
                "evaluation_generation_success": true
            },
            "dimension": "conditional",
            "is_meta": false,
            "type": [
                "formatting"
            ],
            "evaluation": [
                {
                    "type": "llm_conditional_check",
                    "exec": "Is the model performing a <lov-write> operation? Please answer YES/NO directly and do not enter anything else.\n\nHere is the model response: {response}"
                },
                {
                    "type": "code",
                    "exec": "\nimport re\nfrom typing import List\n\ndef check_following(response: str) -> bool:\n    # Find all opening and closing tags\n    tag_pattern = r'<([^>]+)>'\n    tags = re.findall(tag_pattern, response)\n    \n    # Stack to keep track of opening tags\n    stack: List[str] = []\n    \n    for tag in tags:\n        # Skip self-closing tags\n        if tag.endswith('/'):\n            continue\n            \n        # Handle closing tags\n        if tag.startswith('/'):\n            # If stack is empty, we found a closing tag without matching opening tag\n            if not stack:\n                return False\n                \n            # Get the expected opening tag name\n            expected_tag = stack[-1]\n            # Remove the '/' from the closing tag\n            actual_tag = tag[1:]\n            \n            # Check if tags match\n            if expected_tag != actual_tag:\n                return False\n                \n            # Remove the matched opening tag from stack\n            stack.pop()\n        else:\n            # Add opening tag to stack\n            stack.append(tag)\n    \n    # If stack is not empty, we have unclosed tags\n    return len(stack) == 0\n"
                }
            ],
            "score": false
        },
        {
            "id": 131,
            "desc": "If the user asks for a very specific design, you MUST follow it to the letter.",
            "other_info": {
                "from": "system_para_218",
                "condition_desc": "If the user asks for a very specific design, then you MUST follow it to the letter.",
                "complete_instruction_para": [],
                "evaluation_type": {
                    "constraint_type": "llm",
                    "explanation": "Determining whether the user's specific design has been followed 'to the letter' requires a semantic and subjective understanding of the design details and their implementation, which can only be assessed by an LLM."
                },
                "evaluation_generation_success": true
            },
            "dimension": "conditional",
            "is_meta": false,
            "type": [
                "semantic"
            ],
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "Does the model's response strictly adhere to the user's specific design instructions without deviating from the details they provided? Please answer YES/NO directly and do not enter anything else.\n\nHere is the model response: {response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": 6,
            "desc": "NEVER output code to the USER, unless requested.  Instead use one of the code edit tools to implement the change.",
            "other_info": {
                "from": "system_para_3",
                "condition_desc": "",
                "complete_instruction_para": [],
                "evaluation_type": {
                    "constraint_type": "code",
                    "explanation": "This constraint can be validated directly by checking whether the output contains code or a call to a code edit tool. It involves simple logic and does not require semantic understanding or content extraction."
                },
                "evaluation_generation_success": true
            },
            "dimension": "conditional",
            "is_meta": false,
            "type": [
                "semantic"
            ],
            "evaluation": [
                {
                    "type": "code",
                    "exec": "\ndef check_following(response: str) -> bool:\n    function_names = [\n        \"codebase_search\",\n        \"read_file\",\n        \"list_dir\",\n        \"grep_search\",\n        \"file_search\",\n        \"web_search\",\n    ]\n    return any(name in response for name in function_names)\n"
                }
            ],
            "score": false
        },
        {
            "id": 6,
            "desc": "NEVER output code to the USER, unless requested.  Instead use one of the code edit tools to implement the change.",
            "other_info": {
                "from": "system_para_3",
                "condition_desc": "",
                "complete_instruction_para": [],
                "evaluation_type": {
                    "constraint_type": "code",
                    "explanation": "This constraint can be validated directly by checking whether the output contains code or a call to a code edit tool. It involves simple logic and does not require semantic understanding or content extraction."
                },
                "evaluation_generation_success": true
            },
            "dimension": "conditional",
            "is_meta": false,
            "type": [
                "semantic"
            ],
            "evaluation": [
                {
                    "type": "code",
                    "exec": "\ndef check_following(response: str) -> bool:\n    function_names = [\n        \"codebase_search\",\n        \"read_file\",\n        \"list_dir\",\n        \"grep_search\",\n        \"file_search\",\n        \"web_search\",\n    ]\n    return any(name in response for name in function_names)\n"
                }
            ],
            "score": false
        },
        {
            "id": 9,
            "desc": "If you're creating the codebase from scratch, create an appropriate dependency management file (e.g. requirements.txt) with package versions and a helpful README.",
            "other_info": {
                "from": "system_para_3",
                "condition_desc": "",
                "complete_instruction_para": [],
                "evaluation_type": {
                    "constraint_type": "llm_assisted_code",
                    "explanation": "Creating a dependency management file and README requires extracting relevant information about the project (e.g., dependencies, purpose, usage instructions) which may not be directly accessible. An LLM can assist in extracting or generating this content before code validates the format and correctness."
                },
                "evaluation_generation_success": true
            },
            "dimension": "conditional",
            "is_meta": false,
            "type": [
                "semantic"
            ],
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "Does the model response create an appropriate dependency management file (e.g. requirements.txt) and a helpful README? Please answer YES/NO directly and do not enter anything else.\n\nHere is the model response: {response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": 21,
            "desc": "If there are no relevant tools or there are missing values for required parameters, ask the user to supply these values.",
            "other_info": {
                "from": "system_para_8",
                "condition_desc": "",
                "complete_instruction_para": [],
                "evaluation_type": {
                    "constraint_type": "llm",
                    "explanation": "Determining whether values are missing and asking the user to supply them requires semantic understanding of the context and the parameters involved, which is subjective and open-ended. This cannot be directly validated by code or through extraction-based logic."
                },
                "evaluation_generation_success": true
            },
            "dimension": "conditional",
            "is_meta": false,
            "type": [
                "semantic"
            ],
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "Does the model's response explicitly ask the user to supply the missing values for all required parameters? Please answer YES/NO directly and do not enter anything else.\n\nHere is the model response: {response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": 6,
            "desc": "NEVER output code to the USER, unless requested.  Instead use one of the code edit tools to implement the change.",
            "other_info": {
                "from": "system_para_3",
                "condition_desc": "",
                "complete_instruction_para": [],
                "evaluation_type": {
                    "constraint_type": "code",
                    "explanation": "This constraint can be validated directly by checking whether the output contains code or a call to a code edit tool. It involves simple logic and does not require semantic understanding or content extraction."
                },
                "evaluation_generation_success": true
            },
            "dimension": "conditional",
            "is_meta": false,
            "type": [
                "semantic"
            ],
            "evaluation": [
                {
                    "type": "code",
                    "exec": "\ndef check_following(response: str) -> bool:\n    function_names = [\n        \"codebase_search\",\n        \"read_file\",\n        \"list_dir\",\n        \"grep_search\",\n        \"file_search\",\n        \"web_search\",\n    ]\n    return any(name in response for name in function_names)\n"
                }
            ],
            "score": false
        },
        {
            "id": 26,
            "desc": "When using multiple SEARCH/REPLACE blocks, list them in the order they appear in the file.",
            "other_info": {
                "from": "system_para_14",
                "condition_desc": "If you are using multiple SEARCH/REPLACE blocks, then list them in the order in which they are positioned within the file. ",
                "complete_instruction_para": [],
                "evaluation_type": {
                    "constraint_type": "llm_assisted_code",
                    "explanation": "Validating this constraint requires extracting the sequence of SEARCH/REPLACE blocks from the input and checking their order against the file's content. Code can perform the validation, but it first needs the specific blocks to be identified and extracted, which may require LLM assistance."
                },
                "evaluation_generation_success": true
            },
            "dimension": "conditional",
            "is_meta": false,
            "type": [
                "formatting"
            ],
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "Extract all SEARCH/REPLACE blocks from the response, preserving their order as they appear. Each block should include the full content between the delimiters <<<<<<< SEARCH and >>>>>>> REPLACE, including the SEARCH and REPLACE sections. Return the extracted content verbatim from the response. If multiple blocks are found, return them as a Python-style list of strings. If nothing is found, return None.\n\nHere is the model response: {response}"
                },
                {
                    "type": "code",
                    "exec": "import re\n\ndef check_following(response):\n    search_replace_pattern = r\"<<<<<<< SEARCH\\n(.*?)\\n=======\\n(.*?)\\n>>>>>> REPLACE\"\n    blocks = re.findall(search_replace_pattern, response, re.DOTALL)\n    \n    if not blocks:\n        return False\n    \n    last_search_end = -1\n    for search, _ in blocks:\n        current_search_start = response.find(search)\n        if current_search_start == -1 or current_search_start < last_search_end:\n            return False\n        last_search_end = current_search_start + len(search)\n    \n    return True\n"
                }
            ],
            "score": false
        },
        {
            "id": 88,
            "desc": "If multiple actions are needed, use one tool at a time per message to accomplish the task iteratively, with each tool use being informed by the result of the previous tool use.",
            "other_info": {
                "from": "system_para_45",
                "condition_desc": "",
                "complete_instruction_para": [],
                "evaluation_type": {
                    "constraint_type": "llm",
                    "explanation": "The constraint requires semantic understanding of whether the instruction adheres to the iterative use of tools, informed by the result of the previous tool use. This involves assessing the logical flow and adherence to the iterative process, which is subjective and cannot be directly validated by code."
                },
                "evaluation_generation_success": true
            },
            "dimension": "conditional",
            "is_meta": true,
            "type": [
                "formatting"
            ],
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "Does the model's response use only one tool at a time per message, with each tool use being informed by the result of the previous tool use? Please answer YES/NO directly and do not enter anything else.\n\nHere is the model response: {response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": 105,
            "desc": "If you need multiple changes, you can stack multiple SEARCH/REPLACE blocks within a single replace_in_file call.",
            "other_info": {
                "from": "system_para_81",
                "condition_desc": "",
                "complete_instruction_para": [],
                "evaluation_type": {
                    "constraint_type": "code",
                    "explanation": "The constraint specifies a structural rule about stacking multiple SEARCH/REPLACE blocks within a single replace_in_file call, which can be directly validated by checking the code for the presence of multiple SEARCH/REPLACE blocks within a single function call."
                },
                "evaluation_generation_success": true
            },
            "dimension": "conditional",
            "is_meta": false,
            "type": [
                "semantic"
            ],
            "evaluation": [
                {
                    "type": "llm_conditional_check",
                    "exec": "Do you need to make multiple changes? Please answer YES/NO directly and do not enter anything else.\n\nHere is the model response: {response}"
                },
                {
                    "type": "code",
                    "exec": "import re\n\ndef check_following(response: str) -> bool:\n    return bool(re.search(r\"replace_in_file.*SEARCH/REPLACE.*SEARCH/REPLACE\", response, re.DOTALL))\n"
                }
            ],
            "score": false
        }
    ],
    "example_driven": [
        {
            "id": 7,
            "desc": "<code>- # 写注释",
            "other_info": {
                "from": "system_para_6"
            },
            "dimension": "example_driven",
            "type": "formatting",
            "is_meta": false,
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "检查文本是否以<code>后接注释（包括单行或多行注释）开头，或无<code>时直接以注释开头。注释可以是#、'''或\"\"\"形式。直接回答YES或NO。\n文本：{response}\n回答："
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": 7,
            "desc": "<code>- # 写注释",
            "other_info": {
                "from": "system_para_6"
            },
            "dimension": "example_driven",
            "type": "formatting",
            "is_meta": false,
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "检查文本是否以<code>后接注释（包括单行或多行注释）开头，或无<code>时直接以注释开头。注释可以是#、'''或\"\"\"形式。直接回答YES或NO。\n文本：{response}\n回答："
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": 7,
            "desc": "<code>- # 写注释",
            "other_info": {
                "from": "system_para_6"
            },
            "dimension": "example_driven",
            "type": "formatting",
            "is_meta": false,
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "检查文本是否以<code>后接注释（包括单行或多行注释）开头，或无<code>时直接以注释开头。注释可以是#、'''或\"\"\"形式。直接回答YES或NO。\n文本：{response}\n回答："
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": 7,
            "desc": "<code>- # 写注释",
            "other_info": {
                "from": "system_para_6"
            },
            "dimension": "example_driven",
            "type": "formatting",
            "is_meta": false,
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "检查文本是否以<code>后接注释（包括单行或多行注释）开头，或无<code>时直接以注释开头。注释可以是#、'''或\"\"\"形式。直接回答YES或NO。\n文本：{response}\n回答："
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": 7,
            "desc": "<code>- # 写注释",
            "other_info": {
                "from": "system_para_6"
            },
            "dimension": "example_driven",
            "type": "formatting",
            "is_meta": false,
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "检查文本是否以<code>后接注释（包括单行或多行注释）开头，或无<code>时直接以注释开头。注释可以是#、'''或\"\"\"形式。直接回答YES或NO。\n文本：{response}\n回答："
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": 7,
            "desc": "<code>- # 写注释",
            "other_info": {
                "from": "system_para_6"
            },
            "dimension": "example_driven",
            "type": "formatting",
            "is_meta": false,
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "检查文本是否以<code>后接注释（包括单行或多行注释）开头，或无<code>时直接以注释开头。注释可以是#、'''或\"\"\"形式。直接回答YES或NO。\n文本：{response}\n回答："
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": 7,
            "desc": "<code>- # 写注释",
            "other_info": {
                "from": "system_para_6"
            },
            "dimension": "example_driven",
            "type": "formatting",
            "is_meta": false,
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "检查文本是否以<code>后接注释（包括单行或多行注释）开头，或无<code>时直接以注释开头。注释可以是#、'''或\"\"\"形式。直接回答YES或NO。\n文本：{response}\n回答："
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": 7,
            "desc": "<code>- # 写注释",
            "other_info": {
                "from": "system_para_6"
            },
            "dimension": "example_driven",
            "type": "formatting",
            "is_meta": false,
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "检查文本是否以<code>后接注释（包括单行或多行注释）开头，或无<code>时直接以注释开头。注释可以是#、'''或\"\"\"形式。直接回答YES或NO。\n文本：{response}\n回答："
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": -1,
            "desc": "In case (2), write this sub-question with an [END] mark as a leaf node.",
            "other_info": {
                "from": "system_para_0",
                "condition_desc": "If case (2), write this sub-question with an [END] mark as a leaf node.",
                "complete_instruction_para": []
            },
            "dimension": "example_driven",
            "is_meta": false,
            "type": [
                "formatting"
            ],
            "evaluation": [
                {
                    "type": "llm_conditional_check",
                    "exec": "Does there exist sub-questions that satisfy case (2), which can be 'directly answered by analyzing the answers of at least two previously answered questions, such as comparing, judging, intersecting, or counting'? Please answer YES/NO directly and do not enter anything else.\n\nHere is model response: {response}"
                },
                {
                    "type": "llm",
                    "exec": "In the model response, are all sub-questions that satisfy case (2) written with an [END] mark as leaf nodes? Please answer YES/NO directly and do not enter anything else.\n\nHere is model response: {response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": -1,
            "desc": "use escape quotes \"\" to enclose work titles and function parameters",
            "other_info": {
                "from": "system_para_0",
                "condition_desc": "",
                "complete_instruction_para": []
            },
            "dimension": "example_driven",
            "is_meta": false,
            "type": [
                "formatting"
            ],
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "Did the model use escape quotes \"\" to enclose all work titles and function parameters? Please answer YES/NO directly and do not enter anything else.\n\nHere is model response: {response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": 7,
            "desc": "<code>- # 写注释",
            "other_info": {
                "from": "system_para_6"
            },
            "dimension": "example_driven",
            "type": "formatting",
            "is_meta": false,
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "检查文本是否以<code>后接注释（包括单行或多行注释）开头，或无<code>时直接以注释开头。注释可以是#、'''或\"\"\"形式。直接回答YES或NO。\n文本：{response}\n回答："
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": -1,
            "desc": "In case (2), write this sub-question with an [END] mark as a leaf node.",
            "other_info": {
                "from": "system_para_0",
                "condition_desc": "If case (2), write this sub-question with an [END] mark as a leaf node.",
                "complete_instruction_para": []
            },
            "dimension": "example_driven",
            "is_meta": false,
            "type": [
                "formatting"
            ],
            "evaluation": [
                {
                    "type": "llm_conditional_check",
                    "exec": "Does there exist sub-questions that satisfy case (2), which can be 'directly answered by analyzing the answers of at least two previously answered questions, such as comparing, judging, intersecting, or counting'? Please answer YES/NO directly and do not enter anything else.\n\nHere is model response: {response}"
                },
                {
                    "type": "llm",
                    "exec": "In the model response, are all sub-questions that satisfy case (2) written with an [END] mark as leaf nodes? Please answer YES/NO directly and do not enter anything else.\n\nHere is model response: {response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": -1,
            "desc": "In case (2), write this sub-question with an [END] mark as a leaf node.",
            "other_info": {
                "from": "system_para_0",
                "condition_desc": "If case (2), write this sub-question with an [END] mark as a leaf node.",
                "complete_instruction_para": []
            },
            "dimension": "example_driven",
            "is_meta": false,
            "type": [
                "formatting"
            ],
            "evaluation": [
                {
                    "type": "llm_conditional_check",
                    "exec": "Does there exist sub-questions that satisfy case (2), which can be 'directly answered by analyzing the answers of at least two previously answered questions, such as comparing, judging, intersecting, or counting'? Please answer YES/NO directly and do not enter anything else.\n\nHere is model response: {response}"
                },
                {
                    "type": "llm",
                    "exec": "In the model response, are all sub-questions that satisfy case (2) written with an [END] mark as leaf nodes? Please answer YES/NO directly and do not enter anything else.\n\nHere is model response: {response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": -1,
            "desc": "use escape quotes \"\" to enclose work titles and function parameters",
            "other_info": {
                "from": "system_para_0",
                "condition_desc": "",
                "complete_instruction_para": []
            },
            "dimension": "example_driven",
            "is_meta": false,
            "type": [
                "formatting"
            ],
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "Did the model use escape quotes \"\" to enclose all work titles and function parameters? Please answer YES/NO directly and do not enter anything else.\n\nHere is model response: {response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": 7,
            "desc": "<code>- # 写注释",
            "other_info": {
                "from": "system_para_6"
            },
            "dimension": "example_driven",
            "type": "formatting",
            "is_meta": false,
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "检查文本是否以<code>后接注释（包括单行或多行注释）开头，或无<code>时直接以注释开头。注释可以是#、'''或\"\"\"形式。直接回答YES或NO。\n文本：{response}\n回答："
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": -1,
            "desc": "In case (2), write this sub-question with an [END] mark as a leaf node.",
            "other_info": {
                "from": "system_para_0",
                "condition_desc": "If case (2), write this sub-question with an [END] mark as a leaf node.",
                "complete_instruction_para": []
            },
            "dimension": "example_driven",
            "is_meta": false,
            "type": [
                "formatting"
            ],
            "evaluation": [
                {
                    "type": "llm_conditional_check",
                    "exec": "Does there exist sub-questions that satisfy case (2), which can be 'directly answered by analyzing the answers of at least two previously answered questions, such as comparing, judging, intersecting, or counting'? Please answer YES/NO directly and do not enter anything else.\n\nHere is model response: {response}"
                },
                {
                    "type": "llm",
                    "exec": "In the model response, are all sub-questions that satisfy case (2) written with an [END] mark as leaf nodes? Please answer YES/NO directly and do not enter anything else.\n\nHere is model response: {response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": -1,
            "desc": "use escape quotes \"\" to enclose work titles and function parameters",
            "other_info": {
                "from": "system_para_0",
                "condition_desc": "",
                "complete_instruction_para": []
            },
            "dimension": "example_driven",
            "is_meta": false,
            "type": [
                "formatting"
            ],
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "Did the model use escape quotes \"\" to enclose all work titles and function parameters? Please answer YES/NO directly and do not enter anything else.\n\nHere is model response: {response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": -1,
            "desc": "In case (2), write this sub-question with an [END] mark as a leaf node.",
            "other_info": {
                "from": "system_para_0",
                "condition_desc": "If case (2), write this sub-question with an [END] mark as a leaf node.",
                "complete_instruction_para": []
            },
            "dimension": "example_driven",
            "is_meta": false,
            "type": [
                "formatting"
            ],
            "evaluation": [
                {
                    "type": "llm_conditional_check",
                    "exec": "Does there exist sub-questions that satisfy case (2), which can be 'directly answered by analyzing the answers of at least two previously answered questions, such as comparing, judging, intersecting, or counting'? Please answer YES/NO directly and do not enter anything else.\n\nHere is model response: {response}"
                },
                {
                    "type": "llm",
                    "exec": "In the model response, are all sub-questions that satisfy case (2) written with an [END] mark as leaf nodes? Please answer YES/NO directly and do not enter anything else.\n\nHere is model response: {response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": -1,
            "desc": "use escape quotes \"\" to enclose work titles and function parameters",
            "other_info": {
                "from": "system_para_0",
                "condition_desc": "",
                "complete_instruction_para": []
            },
            "dimension": "example_driven",
            "is_meta": false,
            "type": [
                "formatting"
            ],
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "Did the model use escape quotes \"\" to enclose all work titles and function parameters? Please answer YES/NO directly and do not enter anything else.\n\nHere is model response: {response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": -1,
            "desc": "In case (2), write this sub-question with an [END] mark as a leaf node.",
            "other_info": {
                "from": "system_para_0",
                "condition_desc": "If case (2), write this sub-question with an [END] mark as a leaf node.",
                "complete_instruction_para": []
            },
            "dimension": "example_driven",
            "is_meta": false,
            "type": [
                "formatting"
            ],
            "evaluation": [
                {
                    "type": "llm_conditional_check",
                    "exec": "Does there exist sub-questions that satisfy case (2), which can be 'directly answered by analyzing the answers of at least two previously answered questions, such as comparing, judging, intersecting, or counting'? Please answer YES/NO directly and do not enter anything else.\n\nHere is model response: {response}"
                },
                {
                    "type": "llm",
                    "exec": "In the model response, are all sub-questions that satisfy case (2) written with an [END] mark as leaf nodes? Please answer YES/NO directly and do not enter anything else.\n\nHere is model response: {response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": -1,
            "desc": "use escape quotes \"\" to enclose work titles and function parameters",
            "other_info": {
                "from": "system_para_0",
                "condition_desc": "",
                "complete_instruction_para": []
            },
            "dimension": "example_driven",
            "is_meta": false,
            "type": [
                "formatting"
            ],
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "Did the model use escape quotes \"\" to enclose all work titles and function parameters? Please answer YES/NO directly and do not enter anything else.\n\nHere is model response: {response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": -1,
            "desc": "For [END] leaf questions, format your question as 'Given answers of [q_idx_1] and [q_idx_2], ...', where [q_idx_1] and [q_idx_2] are question indices of the previously answered questions required to answer this [END] question.",
            "other_info": {
                "from": "system_para_0",
                "condition_desc": "If [END] leaf questions, format your question as 'Given answers of [q_idx_1] and [q_idx_2], ...', where [q_idx_1] and [q_idx_2] are question indices of the previously answered questions required to answer this [END] question..",
                "complete_instruction_para": []
            },
            "dimension": "example_driven",
            "is_meta": false,
            "type": [
                "formatting"
            ],
            "evaluation": [
                {
                    "type": "llm_conditional_check",
                    "exec": "Are there [END] leaf questions in the tree? Please answer YES/NO directly and do not enter anything else.\n\nHere is model response: {response}"
                },
                {
                    "type": "llm",
                    "exec": "Are all [END] leaf questions formatted as 'Given answers of [q_idx_1] and [q_idx_2], ...', where [q_idx_1] and [q_idx_2] are question indices of previously answered questions required to answer this [END] question? Please answer YES/NO directly and do not enter anything else.\n\nHere is model response: {response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": -1,
            "desc": "In case (2), write this sub-question with an [END] mark as a leaf node.",
            "other_info": {
                "from": "system_para_0",
                "condition_desc": "If case (2), write this sub-question with an [END] mark as a leaf node.",
                "complete_instruction_para": []
            },
            "dimension": "example_driven",
            "is_meta": false,
            "type": [
                "formatting"
            ],
            "evaluation": [
                {
                    "type": "llm_conditional_check",
                    "exec": "Does there exist sub-questions that satisfy case (2), which can be 'directly answered by analyzing the answers of at least two previously answered questions, such as comparing, judging, intersecting, or counting'? Please answer YES/NO directly and do not enter anything else.\n\nHere is model response: {response}"
                },
                {
                    "type": "llm",
                    "exec": "In the model response, are all sub-questions that satisfy case (2) written with an [END] mark as leaf nodes? Please answer YES/NO directly and do not enter anything else.\n\nHere is model response: {response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": -1,
            "desc": "use escape quotes \"\" to enclose work titles and function parameters",
            "other_info": {
                "from": "system_para_0",
                "condition_desc": "",
                "complete_instruction_para": []
            },
            "dimension": "example_driven",
            "is_meta": false,
            "type": [
                "formatting"
            ],
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "Did the model use escape quotes \"\" to enclose all work titles and function parameters? Please answer YES/NO directly and do not enter anything else.\n\nHere is model response: {response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": -1,
            "desc": "use escape quotes \"\" to enclose work titles and function parameters",
            "other_info": {
                "from": "system_para_0",
                "condition_desc": "",
                "complete_instruction_para": []
            },
            "dimension": "example_driven",
            "is_meta": false,
            "type": [
                "formatting"
            ],
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "Did the model use escape quotes \"\" to enclose all work titles and function parameters? Please answer YES/NO directly and do not enter anything else.\n\nHere is model response: {response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": -1,
            "desc": "In case (2), write this sub-question with an [END] mark as a leaf node.",
            "other_info": {
                "from": "system_para_0",
                "condition_desc": "If case (2), write this sub-question with an [END] mark as a leaf node.",
                "complete_instruction_para": []
            },
            "dimension": "example_driven",
            "is_meta": false,
            "type": [
                "formatting"
            ],
            "evaluation": [
                {
                    "type": "llm_conditional_check",
                    "exec": "Does there exist sub-questions that satisfy case (2), which can be 'directly answered by analyzing the answers of at least two previously answered questions, such as comparing, judging, intersecting, or counting'? Please answer YES/NO directly and do not enter anything else.\n\nHere is model response: {response}"
                },
                {
                    "type": "llm",
                    "exec": "In the model response, are all sub-questions that satisfy case (2) written with an [END] mark as leaf nodes? Please answer YES/NO directly and do not enter anything else.\n\nHere is model response: {response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": -1,
            "desc": "use escape quotes \"\" to enclose work titles and function parameters",
            "other_info": {
                "from": "system_para_0",
                "condition_desc": "",
                "complete_instruction_para": []
            },
            "dimension": "example_driven",
            "is_meta": false,
            "type": [
                "formatting"
            ],
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "Did the model use escape quotes \"\" to enclose all work titles and function parameters? Please answer YES/NO directly and do not enter anything else.\n\nHere is model response: {response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": 7,
            "desc": "<code>- # 写注释",
            "other_info": {
                "from": "system_para_6"
            },
            "dimension": "example_driven",
            "type": "formatting",
            "is_meta": false,
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "检查文本是否以<code>后接注释（包括单行或多行注释）开头，或无<code>时直接以注释开头。注释可以是#、'''或\"\"\"形式。直接回答YES或NO。\n文本：{response}\n回答："
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": -1,
            "desc": "use escape quotes \"\" to enclose work titles and function parameters",
            "other_info": {
                "from": "system_para_0",
                "condition_desc": "",
                "complete_instruction_para": []
            },
            "dimension": "example_driven",
            "is_meta": false,
            "type": [
                "formatting"
            ],
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "Did the model use escape quotes \"\" to enclose all work titles and function parameters? Please answer YES/NO directly and do not enter anything else.\n\nHere is model response: {response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": -1,
            "desc": "In case (2), write this sub-question with an [END] mark as a leaf node.",
            "other_info": {
                "from": "system_para_0",
                "condition_desc": "If case (2), write this sub-question with an [END] mark as a leaf node.",
                "complete_instruction_para": []
            },
            "dimension": "example_driven",
            "is_meta": false,
            "type": [
                "formatting"
            ],
            "evaluation": [
                {
                    "type": "llm_conditional_check",
                    "exec": "Does there exist sub-questions that satisfy case (2), which can be 'directly answered by analyzing the answers of at least two previously answered questions, such as comparing, judging, intersecting, or counting'? Please answer YES/NO directly and do not enter anything else.\n\nHere is model response: {response}"
                },
                {
                    "type": "llm",
                    "exec": "In the model response, are all sub-questions that satisfy case (2) written with an [END] mark as leaf nodes? Please answer YES/NO directly and do not enter anything else.\n\nHere is model response: {response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": -1,
            "desc": "use double quotes to enclose sub-questions and functions",
            "other_info": {
                "from": "system_para_0",
                "condition_desc": "",
                "complete_instruction_para": []
            },
            "dimension": "example_driven",
            "is_meta": false,
            "type": [
                "formatting"
            ],
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "Did the model use double quotes to enclose all sub-questions and functions? Please answer YES/NO directly and do not enter anything else.\n\nHere is model response: {response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": -1,
            "desc": "use escape quotes \"\" to enclose work titles and function parameters",
            "other_info": {
                "from": "system_para_0",
                "condition_desc": "",
                "complete_instruction_para": []
            },
            "dimension": "example_driven",
            "is_meta": false,
            "type": [
                "formatting"
            ],
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "Did the model use escape quotes \"\" to enclose all work titles and function parameters? Please answer YES/NO directly and do not enter anything else.\n\nHere is model response: {response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": -1,
            "desc": "In case (2), write this sub-question with an [END] mark as a leaf node.",
            "other_info": {
                "from": "system_para_0",
                "condition_desc": "If case (2), write this sub-question with an [END] mark as a leaf node.",
                "complete_instruction_para": []
            },
            "dimension": "example_driven",
            "is_meta": false,
            "type": [
                "formatting"
            ],
            "evaluation": [
                {
                    "type": "llm_conditional_check",
                    "exec": "Does there exist sub-questions that satisfy case (2), which can be 'directly answered by analyzing the answers of at least two previously answered questions, such as comparing, judging, intersecting, or counting'? Please answer YES/NO directly and do not enter anything else.\n\nHere is model response: {response}"
                },
                {
                    "type": "llm",
                    "exec": "In the model response, are all sub-questions that satisfy case (2) written with an [END] mark as leaf nodes? Please answer YES/NO directly and do not enter anything else.\n\nHere is model response: {response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": -1,
            "desc": "use escape quotes \"\" to enclose work titles and function parameters",
            "other_info": {
                "from": "system_para_0",
                "condition_desc": "",
                "complete_instruction_para": []
            },
            "dimension": "example_driven",
            "is_meta": false,
            "type": [
                "formatting"
            ],
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "Did the model use escape quotes \"\" to enclose all work titles and function parameters? Please answer YES/NO directly and do not enter anything else.\n\nHere is model response: {response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": 7,
            "desc": "<code>- # 写注释",
            "other_info": {
                "from": "system_para_6"
            },
            "dimension": "example_driven",
            "type": "formatting",
            "is_meta": false,
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "检查文本是否以<code>后接注释（包括单行或多行注释）开头，或无<code>时直接以注释开头。注释可以是#、'''或\"\"\"形式。直接回答YES或NO。\n文本：{response}\n回答："
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": -1,
            "desc": "use escape quotes \"\" to enclose work titles and function parameters",
            "other_info": {
                "from": "system_para_0",
                "condition_desc": "",
                "complete_instruction_para": []
            },
            "dimension": "example_driven",
            "is_meta": false,
            "type": [
                "formatting"
            ],
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "Did the model use escape quotes \"\" to enclose all work titles and function parameters? Please answer YES/NO directly and do not enter anything else.\n\nHere is model response: {response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": -1,
            "desc": "use double quotes to enclose sub-questions and functions",
            "other_info": {
                "from": "system_para_0",
                "condition_desc": "",
                "complete_instruction_para": []
            },
            "dimension": "example_driven",
            "is_meta": false,
            "type": [
                "formatting"
            ],
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "Did the model use double quotes to enclose all sub-questions and functions? Please answer YES/NO directly and do not enter anything else.\n\nHere is model response: {response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": -1,
            "desc": "use escape quotes \"\" to enclose work titles and function parameters",
            "other_info": {
                "from": "system_para_0",
                "condition_desc": "",
                "complete_instruction_para": []
            },
            "dimension": "example_driven",
            "is_meta": false,
            "type": [
                "formatting"
            ],
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "Did the model use escape quotes \"\" to enclose all work titles and function parameters? Please answer YES/NO directly and do not enter anything else.\n\nHere is model response: {response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": -1,
            "desc": "In case (2), write this sub-question with an [END] mark as a leaf node.",
            "other_info": {
                "from": "system_para_0",
                "condition_desc": "If case (2), write this sub-question with an [END] mark as a leaf node.",
                "complete_instruction_para": []
            },
            "dimension": "example_driven",
            "is_meta": false,
            "type": [
                "formatting"
            ],
            "evaluation": [
                {
                    "type": "llm_conditional_check",
                    "exec": "Does there exist sub-questions that satisfy case (2), which can be 'directly answered by analyzing the answers of at least two previously answered questions, such as comparing, judging, intersecting, or counting'? Please answer YES/NO directly and do not enter anything else.\n\nHere is model response: {response}"
                },
                {
                    "type": "llm",
                    "exec": "In the model response, are all sub-questions that satisfy case (2) written with an [END] mark as leaf nodes? Please answer YES/NO directly and do not enter anything else.\n\nHere is model response: {response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": -1,
            "desc": "use escape quotes \"\" to enclose work titles and function parameters",
            "other_info": {
                "from": "system_para_0",
                "condition_desc": "",
                "complete_instruction_para": []
            },
            "dimension": "example_driven",
            "is_meta": false,
            "type": [
                "formatting"
            ],
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "Did the model use escape quotes \"\" to enclose all work titles and function parameters? Please answer YES/NO directly and do not enter anything else.\n\nHere is model response: {response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": -1,
            "desc": "In case (2), write this sub-question with an [END] mark as a leaf node.",
            "other_info": {
                "from": "system_para_0",
                "condition_desc": "If case (2), write this sub-question with an [END] mark as a leaf node.",
                "complete_instruction_para": []
            },
            "dimension": "example_driven",
            "is_meta": false,
            "type": [
                "formatting"
            ],
            "evaluation": [
                {
                    "type": "llm_conditional_check",
                    "exec": "Does there exist sub-questions that satisfy case (2), which can be 'directly answered by analyzing the answers of at least two previously answered questions, such as comparing, judging, intersecting, or counting'? Please answer YES/NO directly and do not enter anything else.\n\nHere is model response: {response}"
                },
                {
                    "type": "llm",
                    "exec": "In the model response, are all sub-questions that satisfy case (2) written with an [END] mark as leaf nodes? Please answer YES/NO directly and do not enter anything else.\n\nHere is model response: {response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": -1,
            "desc": "use escape quotes \"\" to enclose work titles and function parameters",
            "other_info": {
                "from": "system_para_0",
                "condition_desc": "",
                "complete_instruction_para": []
            },
            "dimension": "example_driven",
            "is_meta": false,
            "type": [
                "formatting"
            ],
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "Did the model use escape quotes \"\" to enclose all work titles and function parameters? Please answer YES/NO directly and do not enter anything else.\n\nHere is model response: {response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": -1,
            "desc": "Continue decomposing until a sub-question cannot be further decomposed and could either be: (1) directly answered by calling one of the three atomic functions Search(), Relate(), Filter(), or (2) directly answered by analyzing the answers of at least two previously answered questions, such as comparing, judging, intersecting, counting, etc.",
            "other_info": {
                "from": "system_para_0",
                "condition_desc": "",
                "complete_instruction_para": []
            },
            "dimension": "example_driven",
            "is_meta": false,
            "type": [
                "semantic"
            ],
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "In the model response, does the model continue question decomposition until each sub-question cannot be further decomposed and could either be: (1) directly answered by calling one of the three atomic functions Search(), Relate(), Filter(), or (2) directly answered by analyzing the answers of at least two previously answered questions, such as comparing, judging, intersecting, or counting? Please answer YES/NO directly and do not enter anything else.\n\nHere is model response: {response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": -1,
            "desc": "In case (2), write this sub-question with an [END] mark as a leaf node.",
            "other_info": {
                "from": "system_para_0",
                "condition_desc": "If case (2), write this sub-question with an [END] mark as a leaf node.",
                "complete_instruction_para": []
            },
            "dimension": "example_driven",
            "is_meta": false,
            "type": [
                "formatting"
            ],
            "evaluation": [
                {
                    "type": "llm_conditional_check",
                    "exec": "Does there exist sub-questions that satisfy case (2), which can be 'directly answered by analyzing the answers of at least two previously answered questions, such as comparing, judging, intersecting, or counting'? Please answer YES/NO directly and do not enter anything else.\n\nHere is model response: {response}"
                },
                {
                    "type": "llm",
                    "exec": "In the model response, are all sub-questions that satisfy case (2) written with an [END] mark as leaf nodes? Please answer YES/NO directly and do not enter anything else.\n\nHere is model response: {response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": -1,
            "desc": "use escape quotes \"\" to enclose work titles and function parameters",
            "other_info": {
                "from": "system_para_0",
                "condition_desc": "",
                "complete_instruction_para": []
            },
            "dimension": "example_driven",
            "is_meta": false,
            "type": [
                "formatting"
            ],
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "Did the model use escape quotes \"\" to enclose all work titles and function parameters? Please answer YES/NO directly and do not enter anything else.\n\nHere is model response: {response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": -1,
            "desc": "use escape quotes \"\" to enclose work titles and function parameters",
            "other_info": {
                "from": "system_para_0",
                "condition_desc": "",
                "complete_instruction_para": []
            },
            "dimension": "example_driven",
            "is_meta": false,
            "type": [
                "formatting"
            ],
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "Did the model use escape quotes \"\" to enclose all work titles and function parameters? Please answer YES/NO directly and do not enter anything else.\n\nHere is model response: {response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": -1,
            "desc": "In case (2), write this sub-question with an [END] mark as a leaf node.",
            "other_info": {
                "from": "system_para_0",
                "condition_desc": "If case (2), write this sub-question with an [END] mark as a leaf node.",
                "complete_instruction_para": []
            },
            "dimension": "example_driven",
            "is_meta": false,
            "type": [
                "formatting"
            ],
            "evaluation": [
                {
                    "type": "llm_conditional_check",
                    "exec": "Does there exist sub-questions that satisfy case (2), which can be 'directly answered by analyzing the answers of at least two previously answered questions, such as comparing, judging, intersecting, or counting'? Please answer YES/NO directly and do not enter anything else.\n\nHere is model response: {response}"
                },
                {
                    "type": "llm",
                    "exec": "In the model response, are all sub-questions that satisfy case (2) written with an [END] mark as leaf nodes? Please answer YES/NO directly and do not enter anything else.\n\nHere is model response: {response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": -1,
            "desc": "use escape quotes \"\" to enclose work titles and function parameters",
            "other_info": {
                "from": "system_para_0",
                "condition_desc": "",
                "complete_instruction_para": []
            },
            "dimension": "example_driven",
            "is_meta": false,
            "type": [
                "formatting"
            ],
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "Did the model use escape quotes \"\" to enclose all work titles and function parameters? Please answer YES/NO directly and do not enter anything else.\n\nHere is model response: {response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": -1,
            "desc": "use escape quotes \"\" to enclose work titles and function parameters",
            "other_info": {
                "from": "system_para_0",
                "condition_desc": "",
                "complete_instruction_para": []
            },
            "dimension": "example_driven",
            "is_meta": false,
            "type": [
                "formatting"
            ],
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "Did the model use escape quotes \"\" to enclose all work titles and function parameters? Please answer YES/NO directly and do not enter anything else.\n\nHere is model response: {response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": -1,
            "desc": "For [END] leaf questions, format your question as 'Given answers of [q_idx_1] and [q_idx_2], ...', where [q_idx_1] and [q_idx_2] are question indices of the previously answered questions required to answer this [END] question.",
            "other_info": {
                "from": "system_para_0",
                "condition_desc": "If [END] leaf questions, format your question as 'Given answers of [q_idx_1] and [q_idx_2], ...', where [q_idx_1] and [q_idx_2] are question indices of the previously answered questions required to answer this [END] question..",
                "complete_instruction_para": []
            },
            "dimension": "example_driven",
            "is_meta": false,
            "type": [
                "formatting"
            ],
            "evaluation": [
                {
                    "type": "llm_conditional_check",
                    "exec": "Are there [END] leaf questions in the tree? Please answer YES/NO directly and do not enter anything else.\n\nHere is model response: {response}"
                },
                {
                    "type": "llm",
                    "exec": "Are all [END] leaf questions formatted as 'Given answers of [q_idx_1] and [q_idx_2], ...', where [q_idx_1] and [q_idx_2] are question indices of previously answered questions required to answer this [END] question? Please answer YES/NO directly and do not enter anything else.\n\nHere is model response: {response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": -1,
            "desc": "use escape quotes \"\" to enclose work titles and function parameters",
            "other_info": {
                "from": "system_para_0",
                "condition_desc": "",
                "complete_instruction_para": []
            },
            "dimension": "example_driven",
            "is_meta": false,
            "type": [
                "formatting"
            ],
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "Did the model use escape quotes \"\" to enclose all work titles and function parameters? Please answer YES/NO directly and do not enter anything else.\n\nHere is model response: {response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": -1,
            "desc": "In case (2), write this sub-question with an [END] mark as a leaf node.",
            "other_info": {
                "from": "system_para_0",
                "condition_desc": "If case (2), write this sub-question with an [END] mark as a leaf node.",
                "complete_instruction_para": []
            },
            "dimension": "example_driven",
            "is_meta": false,
            "type": [
                "formatting"
            ],
            "evaluation": [
                {
                    "type": "llm_conditional_check",
                    "exec": "Does there exist sub-questions that satisfy case (2), which can be 'directly answered by analyzing the answers of at least two previously answered questions, such as comparing, judging, intersecting, or counting'? Please answer YES/NO directly and do not enter anything else.\n\nHere is model response: {response}"
                },
                {
                    "type": "llm",
                    "exec": "In the model response, are all sub-questions that satisfy case (2) written with an [END] mark as leaf nodes? Please answer YES/NO directly and do not enter anything else.\n\nHere is model response: {response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": -1,
            "desc": "use double quotes to enclose sub-questions and functions",
            "other_info": {
                "from": "system_para_0",
                "condition_desc": "",
                "complete_instruction_para": []
            },
            "dimension": "example_driven",
            "is_meta": false,
            "type": [
                "formatting"
            ],
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "Did the model use double quotes to enclose all sub-questions and functions? Please answer YES/NO directly and do not enter anything else.\n\nHere is model response: {response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": -1,
            "desc": "use escape quotes \"\" to enclose work titles and function parameters",
            "other_info": {
                "from": "system_para_0",
                "condition_desc": "",
                "complete_instruction_para": []
            },
            "dimension": "example_driven",
            "is_meta": false,
            "type": [
                "formatting"
            ],
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "Did the model use escape quotes \"\" to enclose all work titles and function parameters? Please answer YES/NO directly and do not enter anything else.\n\nHere is model response: {response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": -1,
            "desc": "For [END] leaf questions, format your question as 'Given answers of [q_idx_1] and [q_idx_2], ...', where [q_idx_1] and [q_idx_2] are question indices of the previously answered questions required to answer this [END] question.",
            "other_info": {
                "from": "system_para_0",
                "condition_desc": "If [END] leaf questions, format your question as 'Given answers of [q_idx_1] and [q_idx_2], ...', where [q_idx_1] and [q_idx_2] are question indices of the previously answered questions required to answer this [END] question..",
                "complete_instruction_para": []
            },
            "dimension": "example_driven",
            "is_meta": false,
            "type": [
                "formatting"
            ],
            "evaluation": [
                {
                    "type": "llm_conditional_check",
                    "exec": "Are there [END] leaf questions in the tree? Please answer YES/NO directly and do not enter anything else.\n\nHere is model response: {response}"
                },
                {
                    "type": "llm",
                    "exec": "Are all [END] leaf questions formatted as 'Given answers of [q_idx_1] and [q_idx_2], ...', where [q_idx_1] and [q_idx_2] are question indices of previously answered questions required to answer this [END] question? Please answer YES/NO directly and do not enter anything else.\n\nHere is model response: {response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": -1,
            "desc": "In case (2), write this sub-question with an [END] mark as a leaf node.",
            "other_info": {
                "from": "system_para_0",
                "condition_desc": "If case (2), write this sub-question with an [END] mark as a leaf node.",
                "complete_instruction_para": []
            },
            "dimension": "example_driven",
            "is_meta": false,
            "type": [
                "formatting"
            ],
            "evaluation": [
                {
                    "type": "llm_conditional_check",
                    "exec": "Does there exist sub-questions that satisfy case (2), which can be 'directly answered by analyzing the answers of at least two previously answered questions, such as comparing, judging, intersecting, or counting'? Please answer YES/NO directly and do not enter anything else.\n\nHere is model response: {response}"
                },
                {
                    "type": "llm",
                    "exec": "In the model response, are all sub-questions that satisfy case (2) written with an [END] mark as leaf nodes? Please answer YES/NO directly and do not enter anything else.\n\nHere is model response: {response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": -1,
            "desc": "use escape quotes \"\" to enclose work titles and function parameters",
            "other_info": {
                "from": "system_para_0",
                "condition_desc": "",
                "complete_instruction_para": []
            },
            "dimension": "example_driven",
            "is_meta": false,
            "type": [
                "formatting"
            ],
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "Did the model use escape quotes \"\" to enclose all work titles and function parameters? Please answer YES/NO directly and do not enter anything else.\n\nHere is model response: {response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": -1,
            "desc": "For [END] leaf questions, format your question as 'Given answers of [q_idx_1] and [q_idx_2], ...', where [q_idx_1] and [q_idx_2] are question indices of the previously answered questions required to answer this [END] question.",
            "other_info": {
                "from": "system_para_0",
                "condition_desc": "If [END] leaf questions, format your question as 'Given answers of [q_idx_1] and [q_idx_2], ...', where [q_idx_1] and [q_idx_2] are question indices of the previously answered questions required to answer this [END] question..",
                "complete_instruction_para": []
            },
            "dimension": "example_driven",
            "is_meta": false,
            "type": [
                "formatting"
            ],
            "evaluation": [
                {
                    "type": "llm_conditional_check",
                    "exec": "Are there [END] leaf questions in the tree? Please answer YES/NO directly and do not enter anything else.\n\nHere is model response: {response}"
                },
                {
                    "type": "llm",
                    "exec": "Are all [END] leaf questions formatted as 'Given answers of [q_idx_1] and [q_idx_2], ...', where [q_idx_1] and [q_idx_2] are question indices of previously answered questions required to answer this [END] question? Please answer YES/NO directly and do not enter anything else.\n\nHere is model response: {response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": -1,
            "desc": "use escape quotes \"\" to enclose work titles and function parameters",
            "other_info": {
                "from": "system_para_0",
                "condition_desc": "",
                "complete_instruction_para": []
            },
            "dimension": "example_driven",
            "is_meta": false,
            "type": [
                "formatting"
            ],
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "Did the model use escape quotes \"\" to enclose all work titles and function parameters? Please answer YES/NO directly and do not enter anything else.\n\nHere is model response: {response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": -1,
            "desc": "In case (2), write this sub-question with an [END] mark as a leaf node.",
            "other_info": {
                "from": "system_para_0",
                "condition_desc": "If case (2), write this sub-question with an [END] mark as a leaf node.",
                "complete_instruction_para": []
            },
            "dimension": "example_driven",
            "is_meta": false,
            "type": [
                "formatting"
            ],
            "evaluation": [
                {
                    "type": "llm_conditional_check",
                    "exec": "Does there exist sub-questions that satisfy case (2), which can be 'directly answered by analyzing the answers of at least two previously answered questions, such as comparing, judging, intersecting, or counting'? Please answer YES/NO directly and do not enter anything else.\n\nHere is model response: {response}"
                },
                {
                    "type": "llm",
                    "exec": "In the model response, are all sub-questions that satisfy case (2) written with an [END] mark as leaf nodes? Please answer YES/NO directly and do not enter anything else.\n\nHere is model response: {response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": -1,
            "desc": "use escape quotes \"\" to enclose work titles and function parameters",
            "other_info": {
                "from": "system_para_0",
                "condition_desc": "",
                "complete_instruction_para": []
            },
            "dimension": "example_driven",
            "is_meta": false,
            "type": [
                "formatting"
            ],
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "Did the model use escape quotes \"\" to enclose all work titles and function parameters? Please answer YES/NO directly and do not enter anything else.\n\nHere is model response: {response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": -1,
            "desc": "use escape quotes \"\" to enclose work titles and function parameters",
            "other_info": {
                "from": "system_para_0",
                "condition_desc": "",
                "complete_instruction_para": []
            },
            "dimension": "example_driven",
            "is_meta": false,
            "type": [
                "formatting"
            ],
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "Did the model use escape quotes \"\" to enclose all work titles and function parameters? Please answer YES/NO directly and do not enter anything else.\n\nHere is model response: {response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": -1,
            "desc": "In case (2), write this sub-question with an [END] mark as a leaf node.",
            "other_info": {
                "from": "system_para_0",
                "condition_desc": "If case (2), write this sub-question with an [END] mark as a leaf node.",
                "complete_instruction_para": []
            },
            "dimension": "example_driven",
            "is_meta": false,
            "type": [
                "formatting"
            ],
            "evaluation": [
                {
                    "type": "llm_conditional_check",
                    "exec": "Does there exist sub-questions that satisfy case (2), which can be 'directly answered by analyzing the answers of at least two previously answered questions, such as comparing, judging, intersecting, or counting'? Please answer YES/NO directly and do not enter anything else.\n\nHere is model response: {response}"
                },
                {
                    "type": "llm",
                    "exec": "In the model response, are all sub-questions that satisfy case (2) written with an [END] mark as leaf nodes? Please answer YES/NO directly and do not enter anything else.\n\nHere is model response: {response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": -1,
            "desc": "use escape quotes \"\" to enclose work titles and function parameters",
            "other_info": {
                "from": "system_para_0",
                "condition_desc": "",
                "complete_instruction_para": []
            },
            "dimension": "example_driven",
            "is_meta": false,
            "type": [
                "formatting"
            ],
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "Did the model use escape quotes \"\" to enclose all work titles and function parameters? Please answer YES/NO directly and do not enter anything else.\n\nHere is model response: {response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": -1,
            "desc": "In case (2), write this sub-question with an [END] mark as a leaf node.",
            "other_info": {
                "from": "system_para_0",
                "condition_desc": "If case (2), write this sub-question with an [END] mark as a leaf node.",
                "complete_instruction_para": []
            },
            "dimension": "example_driven",
            "is_meta": false,
            "type": [
                "formatting"
            ],
            "evaluation": [
                {
                    "type": "llm_conditional_check",
                    "exec": "Does there exist sub-questions that satisfy case (2), which can be 'directly answered by analyzing the answers of at least two previously answered questions, such as comparing, judging, intersecting, or counting'? Please answer YES/NO directly and do not enter anything else.\n\nHere is model response: {response}"
                },
                {
                    "type": "llm",
                    "exec": "In the model response, are all sub-questions that satisfy case (2) written with an [END] mark as leaf nodes? Please answer YES/NO directly and do not enter anything else.\n\nHere is model response: {response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": -1,
            "desc": "use escape quotes \"\" to enclose work titles and function parameters",
            "other_info": {
                "from": "system_para_0",
                "condition_desc": "",
                "complete_instruction_para": []
            },
            "dimension": "example_driven",
            "is_meta": false,
            "type": [
                "formatting"
            ],
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "Did the model use escape quotes \"\" to enclose all work titles and function parameters? Please answer YES/NO directly and do not enter anything else.\n\nHere is model response: {response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": -1,
            "desc": "In case (2), write this sub-question with an [END] mark as a leaf node.",
            "other_info": {
                "from": "system_para_0",
                "condition_desc": "If case (2), write this sub-question with an [END] mark as a leaf node.",
                "complete_instruction_para": []
            },
            "dimension": "example_driven",
            "is_meta": false,
            "type": [
                "formatting"
            ],
            "evaluation": [
                {
                    "type": "llm_conditional_check",
                    "exec": "Does there exist sub-questions that satisfy case (2), which can be 'directly answered by analyzing the answers of at least two previously answered questions, such as comparing, judging, intersecting, or counting'? Please answer YES/NO directly and do not enter anything else.\n\nHere is model response: {response}"
                },
                {
                    "type": "llm",
                    "exec": "In the model response, are all sub-questions that satisfy case (2) written with an [END] mark as leaf nodes? Please answer YES/NO directly and do not enter anything else.\n\nHere is model response: {response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": -1,
            "desc": "use escape quotes \"\" to enclose work titles and function parameters",
            "other_info": {
                "from": "system_para_0",
                "condition_desc": "",
                "complete_instruction_para": []
            },
            "dimension": "example_driven",
            "is_meta": false,
            "type": [
                "formatting"
            ],
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "Did the model use escape quotes \"\" to enclose all work titles and function parameters? Please answer YES/NO directly and do not enter anything else.\n\nHere is model response: {response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": -1,
            "desc": "In case (2), write this sub-question with an [END] mark as a leaf node.",
            "other_info": {
                "from": "system_para_0",
                "condition_desc": "If case (2), write this sub-question with an [END] mark as a leaf node.",
                "complete_instruction_para": []
            },
            "dimension": "example_driven",
            "is_meta": false,
            "type": [
                "formatting"
            ],
            "evaluation": [
                {
                    "type": "llm_conditional_check",
                    "exec": "Does there exist sub-questions that satisfy case (2), which can be 'directly answered by analyzing the answers of at least two previously answered questions, such as comparing, judging, intersecting, or counting'? Please answer YES/NO directly and do not enter anything else.\n\nHere is model response: {response}"
                },
                {
                    "type": "llm",
                    "exec": "In the model response, are all sub-questions that satisfy case (2) written with an [END] mark as leaf nodes? Please answer YES/NO directly and do not enter anything else.\n\nHere is model response: {response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": -1,
            "desc": "use escape quotes \"\" to enclose work titles and function parameters",
            "other_info": {
                "from": "system_para_0",
                "condition_desc": "",
                "complete_instruction_para": []
            },
            "dimension": "example_driven",
            "is_meta": false,
            "type": [
                "formatting"
            ],
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "Did the model use escape quotes \"\" to enclose all work titles and function parameters? Please answer YES/NO directly and do not enter anything else.\n\nHere is model response: {response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": -1,
            "desc": "use escape quotes \"\" to enclose work titles and function parameters",
            "other_info": {
                "from": "system_para_0",
                "condition_desc": "",
                "complete_instruction_para": []
            },
            "dimension": "example_driven",
            "is_meta": false,
            "type": [
                "formatting"
            ],
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "Did the model use escape quotes \"\" to enclose all work titles and function parameters? Please answer YES/NO directly and do not enter anything else.\n\nHere is model response: {response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": -1,
            "desc": "In case (2), write this sub-question with an [END] mark as a leaf node.",
            "other_info": {
                "from": "system_para_0",
                "condition_desc": "If case (2), write this sub-question with an [END] mark as a leaf node.",
                "complete_instruction_para": []
            },
            "dimension": "example_driven",
            "is_meta": false,
            "type": [
                "formatting"
            ],
            "evaluation": [
                {
                    "type": "llm_conditional_check",
                    "exec": "Does there exist sub-questions that satisfy case (2), which can be 'directly answered by analyzing the answers of at least two previously answered questions, such as comparing, judging, intersecting, or counting'? Please answer YES/NO directly and do not enter anything else.\n\nHere is model response: {response}"
                },
                {
                    "type": "llm",
                    "exec": "In the model response, are all sub-questions that satisfy case (2) written with an [END] mark as leaf nodes? Please answer YES/NO directly and do not enter anything else.\n\nHere is model response: {response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": -1,
            "desc": "use escape quotes \"\" to enclose work titles and function parameters",
            "other_info": {
                "from": "system_para_0",
                "condition_desc": "",
                "complete_instruction_para": []
            },
            "dimension": "example_driven",
            "is_meta": false,
            "type": [
                "formatting"
            ],
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "Did the model use escape quotes \"\" to enclose all work titles and function parameters? Please answer YES/NO directly and do not enter anything else.\n\nHere is model response: {response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": -1,
            "desc": "In case (2), write this sub-question with an [END] mark as a leaf node.",
            "other_info": {
                "from": "system_para_0",
                "condition_desc": "If case (2), write this sub-question with an [END] mark as a leaf node.",
                "complete_instruction_para": []
            },
            "dimension": "example_driven",
            "is_meta": false,
            "type": [
                "formatting"
            ],
            "evaluation": [
                {
                    "type": "llm_conditional_check",
                    "exec": "Does there exist sub-questions that satisfy case (2), which can be 'directly answered by analyzing the answers of at least two previously answered questions, such as comparing, judging, intersecting, or counting'? Please answer YES/NO directly and do not enter anything else.\n\nHere is model response: {response}"
                },
                {
                    "type": "llm",
                    "exec": "In the model response, are all sub-questions that satisfy case (2) written with an [END] mark as leaf nodes? Please answer YES/NO directly and do not enter anything else.\n\nHere is model response: {response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": -1,
            "desc": "In case (2), write this sub-question with an [END] mark as a leaf node.",
            "other_info": {
                "from": "system_para_0",
                "condition_desc": "If case (2), write this sub-question with an [END] mark as a leaf node.",
                "complete_instruction_para": []
            },
            "dimension": "example_driven",
            "is_meta": false,
            "type": [
                "formatting"
            ],
            "evaluation": [
                {
                    "type": "llm_conditional_check",
                    "exec": "Does there exist sub-questions that satisfy case (2), which can be 'directly answered by analyzing the answers of at least two previously answered questions, such as comparing, judging, intersecting, or counting'? Please answer YES/NO directly and do not enter anything else.\n\nHere is model response: {response}"
                },
                {
                    "type": "llm",
                    "exec": "In the model response, are all sub-questions that satisfy case (2) written with an [END] mark as leaf nodes? Please answer YES/NO directly and do not enter anything else.\n\nHere is model response: {response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": -1,
            "desc": "use escape quotes \"\" to enclose work titles and function parameters",
            "other_info": {
                "from": "system_para_0",
                "condition_desc": "",
                "complete_instruction_para": []
            },
            "dimension": "example_driven",
            "is_meta": false,
            "type": [
                "formatting"
            ],
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "Did the model use escape quotes \"\" to enclose all work titles and function parameters? Please answer YES/NO directly and do not enter anything else.\n\nHere is model response: {response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": -1,
            "desc": "In case (2), write this sub-question with an [END] mark as a leaf node.",
            "other_info": {
                "from": "system_para_0",
                "condition_desc": "If case (2), write this sub-question with an [END] mark as a leaf node.",
                "complete_instruction_para": []
            },
            "dimension": "example_driven",
            "is_meta": false,
            "type": [
                "formatting"
            ],
            "evaluation": [
                {
                    "type": "llm_conditional_check",
                    "exec": "Does there exist sub-questions that satisfy case (2), which can be 'directly answered by analyzing the answers of at least two previously answered questions, such as comparing, judging, intersecting, or counting'? Please answer YES/NO directly and do not enter anything else.\n\nHere is model response: {response}"
                },
                {
                    "type": "llm",
                    "exec": "In the model response, are all sub-questions that satisfy case (2) written with an [END] mark as leaf nodes? Please answer YES/NO directly and do not enter anything else.\n\nHere is model response: {response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": -1,
            "desc": "use escape quotes \"\" to enclose work titles and function parameters",
            "other_info": {
                "from": "system_para_0",
                "condition_desc": "",
                "complete_instruction_para": []
            },
            "dimension": "example_driven",
            "is_meta": false,
            "type": [
                "formatting"
            ],
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "Did the model use escape quotes \"\" to enclose all work titles and function parameters? Please answer YES/NO directly and do not enter anything else.\n\nHere is model response: {response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": -1,
            "desc": "use escape quotes \"\" to enclose work titles and function parameters",
            "other_info": {
                "from": "system_para_0",
                "condition_desc": "",
                "complete_instruction_para": []
            },
            "dimension": "example_driven",
            "is_meta": false,
            "type": [
                "formatting"
            ],
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "Did the model use escape quotes \"\" to enclose all work titles and function parameters? Please answer YES/NO directly and do not enter anything else.\n\nHere is model response: {response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": -1,
            "desc": "use escape quotes \"\" to enclose work titles and function parameters",
            "other_info": {
                "from": "system_para_0",
                "condition_desc": "",
                "complete_instruction_para": []
            },
            "dimension": "example_driven",
            "is_meta": false,
            "type": [
                "formatting"
            ],
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "Did the model use escape quotes \"\" to enclose all work titles and function parameters? Please answer YES/NO directly and do not enter anything else.\n\nHere is model response: {response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": -1,
            "desc": "In case (2), write this sub-question with an [END] mark as a leaf node.",
            "other_info": {
                "from": "system_para_0",
                "condition_desc": "If case (2), write this sub-question with an [END] mark as a leaf node.",
                "complete_instruction_para": []
            },
            "dimension": "example_driven",
            "is_meta": false,
            "type": [
                "formatting"
            ],
            "evaluation": [
                {
                    "type": "llm_conditional_check",
                    "exec": "Does there exist sub-questions that satisfy case (2), which can be 'directly answered by analyzing the answers of at least two previously answered questions, such as comparing, judging, intersecting, or counting'? Please answer YES/NO directly and do not enter anything else.\n\nHere is model response: {response}"
                },
                {
                    "type": "llm",
                    "exec": "In the model response, are all sub-questions that satisfy case (2) written with an [END] mark as leaf nodes? Please answer YES/NO directly and do not enter anything else.\n\nHere is model response: {response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": -1,
            "desc": "use escape quotes \"\" to enclose work titles and function parameters",
            "other_info": {
                "from": "system_para_0",
                "condition_desc": "",
                "complete_instruction_para": []
            },
            "dimension": "example_driven",
            "is_meta": false,
            "type": [
                "formatting"
            ],
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "Did the model use escape quotes \"\" to enclose all work titles and function parameters? Please answer YES/NO directly and do not enter anything else.\n\nHere is model response: {response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": -1,
            "desc": "In case (2), write this sub-question with an [END] mark as a leaf node.",
            "other_info": {
                "from": "system_para_0",
                "condition_desc": "If case (2), write this sub-question with an [END] mark as a leaf node.",
                "complete_instruction_para": []
            },
            "dimension": "example_driven",
            "is_meta": false,
            "type": [
                "formatting"
            ],
            "evaluation": [
                {
                    "type": "llm_conditional_check",
                    "exec": "Does there exist sub-questions that satisfy case (2), which can be 'directly answered by analyzing the answers of at least two previously answered questions, such as comparing, judging, intersecting, or counting'? Please answer YES/NO directly and do not enter anything else.\n\nHere is model response: {response}"
                },
                {
                    "type": "llm",
                    "exec": "In the model response, are all sub-questions that satisfy case (2) written with an [END] mark as leaf nodes? Please answer YES/NO directly and do not enter anything else.\n\nHere is model response: {response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": -1,
            "desc": "In case (2), write this sub-question with an [END] mark as a leaf node.",
            "other_info": {
                "from": "system_para_0",
                "condition_desc": "If case (2), write this sub-question with an [END] mark as a leaf node.",
                "complete_instruction_para": []
            },
            "dimension": "example_driven",
            "is_meta": false,
            "type": [
                "formatting"
            ],
            "evaluation": [
                {
                    "type": "llm_conditional_check",
                    "exec": "Does there exist sub-questions that satisfy case (2), which can be 'directly answered by analyzing the answers of at least two previously answered questions, such as comparing, judging, intersecting, or counting'? Please answer YES/NO directly and do not enter anything else.\n\nHere is model response: {response}"
                },
                {
                    "type": "llm",
                    "exec": "In the model response, are all sub-questions that satisfy case (2) written with an [END] mark as leaf nodes? Please answer YES/NO directly and do not enter anything else.\n\nHere is model response: {response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": -1,
            "desc": "use escape quotes \"\" to enclose work titles and function parameters",
            "other_info": {
                "from": "system_para_0",
                "condition_desc": "",
                "complete_instruction_para": []
            },
            "dimension": "example_driven",
            "is_meta": false,
            "type": [
                "formatting"
            ],
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "Did the model use escape quotes \"\" to enclose all work titles and function parameters? Please answer YES/NO directly and do not enter anything else.\n\nHere is model response: {response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": -1,
            "desc": "In case (2), write this sub-question with an [END] mark as a leaf node.",
            "other_info": {
                "from": "system_para_0",
                "condition_desc": "If case (2), write this sub-question with an [END] mark as a leaf node.",
                "complete_instruction_para": []
            },
            "dimension": "example_driven",
            "is_meta": false,
            "type": [
                "formatting"
            ],
            "evaluation": [
                {
                    "type": "llm_conditional_check",
                    "exec": "Does there exist sub-questions that satisfy case (2), which can be 'directly answered by analyzing the answers of at least two previously answered questions, such as comparing, judging, intersecting, or counting'? Please answer YES/NO directly and do not enter anything else.\n\nHere is model response: {response}"
                },
                {
                    "type": "llm",
                    "exec": "In the model response, are all sub-questions that satisfy case (2) written with an [END] mark as leaf nodes? Please answer YES/NO directly and do not enter anything else.\n\nHere is model response: {response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": -1,
            "desc": "use escape quotes \"\" to enclose work titles and function parameters",
            "other_info": {
                "from": "system_para_0",
                "condition_desc": "",
                "complete_instruction_para": []
            },
            "dimension": "example_driven",
            "is_meta": false,
            "type": [
                "formatting"
            ],
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "Did the model use escape quotes \"\" to enclose all work titles and function parameters? Please answer YES/NO directly and do not enter anything else.\n\nHere is model response: {response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": -1,
            "desc": "In case (2), write this sub-question with an [END] mark as a leaf node.",
            "other_info": {
                "from": "system_para_0",
                "condition_desc": "If case (2), write this sub-question with an [END] mark as a leaf node.",
                "complete_instruction_para": []
            },
            "dimension": "example_driven",
            "is_meta": false,
            "type": [
                "formatting"
            ],
            "evaluation": [
                {
                    "type": "llm_conditional_check",
                    "exec": "Does there exist sub-questions that satisfy case (2), which can be 'directly answered by analyzing the answers of at least two previously answered questions, such as comparing, judging, intersecting, or counting'? Please answer YES/NO directly and do not enter anything else.\n\nHere is model response: {response}"
                },
                {
                    "type": "llm",
                    "exec": "In the model response, are all sub-questions that satisfy case (2) written with an [END] mark as leaf nodes? Please answer YES/NO directly and do not enter anything else.\n\nHere is model response: {response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": -1,
            "desc": "use escape quotes \"\" to enclose work titles and function parameters",
            "other_info": {
                "from": "system_para_0",
                "condition_desc": "",
                "complete_instruction_para": []
            },
            "dimension": "example_driven",
            "is_meta": false,
            "type": [
                "formatting"
            ],
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "Did the model use escape quotes \"\" to enclose all work titles and function parameters? Please answer YES/NO directly and do not enter anything else.\n\nHere is model response: {response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": 7,
            "desc": "<code>- # 写注释",
            "other_info": {
                "from": "system_para_6"
            },
            "dimension": "example_driven",
            "type": "formatting",
            "is_meta": false,
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "检查文本是否以<code>后接注释（包括单行或多行注释）开头，或无<code>时直接以注释开头。注释可以是#、'''或\"\"\"形式。直接回答YES或NO。\n文本：{response}\n回答："
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": -1,
            "desc": "use escape quotes \"\" to enclose work titles and function parameters",
            "other_info": {
                "from": "system_para_0",
                "condition_desc": "",
                "complete_instruction_para": []
            },
            "dimension": "example_driven",
            "is_meta": false,
            "type": [
                "formatting"
            ],
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "Did the model use escape quotes \"\" to enclose all work titles and function parameters? Please answer YES/NO directly and do not enter anything else.\n\nHere is model response: {response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": -1,
            "desc": "In case (2), write this sub-question with an [END] mark as a leaf node.",
            "other_info": {
                "from": "system_para_0",
                "condition_desc": "If case (2), write this sub-question with an [END] mark as a leaf node.",
                "complete_instruction_para": []
            },
            "dimension": "example_driven",
            "is_meta": false,
            "type": [
                "formatting"
            ],
            "evaluation": [
                {
                    "type": "llm_conditional_check",
                    "exec": "Does there exist sub-questions that satisfy case (2), which can be 'directly answered by analyzing the answers of at least two previously answered questions, such as comparing, judging, intersecting, or counting'? Please answer YES/NO directly and do not enter anything else.\n\nHere is model response: {response}"
                },
                {
                    "type": "llm",
                    "exec": "In the model response, are all sub-questions that satisfy case (2) written with an [END] mark as leaf nodes? Please answer YES/NO directly and do not enter anything else.\n\nHere is model response: {response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": 7,
            "desc": "<code>- # 写注释",
            "other_info": {
                "from": "system_para_6"
            },
            "dimension": "example_driven",
            "type": "formatting",
            "is_meta": false,
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "检查文本是否以<code>后接注释（包括单行或多行注释）开头，或无<code>时直接以注释开头。注释可以是#、'''或\"\"\"形式。直接回答YES或NO。\n文本：{response}\n回答："
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": -1,
            "desc": "In case (2), write this sub-question with an [END] mark as a leaf node.",
            "other_info": {
                "from": "system_para_0",
                "condition_desc": "If case (2), write this sub-question with an [END] mark as a leaf node.",
                "complete_instruction_para": []
            },
            "dimension": "example_driven",
            "is_meta": false,
            "type": [
                "formatting"
            ],
            "evaluation": [
                {
                    "type": "llm_conditional_check",
                    "exec": "Does there exist sub-questions that satisfy case (2), which can be 'directly answered by analyzing the answers of at least two previously answered questions, such as comparing, judging, intersecting, or counting'? Please answer YES/NO directly and do not enter anything else.\n\nHere is model response: {response}"
                },
                {
                    "type": "llm",
                    "exec": "In the model response, are all sub-questions that satisfy case (2) written with an [END] mark as leaf nodes? Please answer YES/NO directly and do not enter anything else.\n\nHere is model response: {response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": -1,
            "desc": "use escape quotes \"\" to enclose work titles and function parameters",
            "other_info": {
                "from": "system_para_0",
                "condition_desc": "",
                "complete_instruction_para": []
            },
            "dimension": "example_driven",
            "is_meta": false,
            "type": [
                "formatting"
            ],
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "Did the model use escape quotes \"\" to enclose all work titles and function parameters? Please answer YES/NO directly and do not enter anything else.\n\nHere is model response: {response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": -1,
            "desc": "In case (2), write this sub-question with an [END] mark as a leaf node.",
            "other_info": {
                "from": "system_para_0",
                "condition_desc": "If case (2), write this sub-question with an [END] mark as a leaf node.",
                "complete_instruction_para": []
            },
            "dimension": "example_driven",
            "is_meta": false,
            "type": [
                "formatting"
            ],
            "evaluation": [
                {
                    "type": "llm_conditional_check",
                    "exec": "Does there exist sub-questions that satisfy case (2), which can be 'directly answered by analyzing the answers of at least two previously answered questions, such as comparing, judging, intersecting, or counting'? Please answer YES/NO directly and do not enter anything else.\n\nHere is model response: {response}"
                },
                {
                    "type": "llm",
                    "exec": "In the model response, are all sub-questions that satisfy case (2) written with an [END] mark as leaf nodes? Please answer YES/NO directly and do not enter anything else.\n\nHere is model response: {response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": -1,
            "desc": "use escape quotes \"\" to enclose work titles and function parameters",
            "other_info": {
                "from": "system_para_0",
                "condition_desc": "",
                "complete_instruction_para": []
            },
            "dimension": "example_driven",
            "is_meta": false,
            "type": [
                "formatting"
            ],
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "Did the model use escape quotes \"\" to enclose all work titles and function parameters? Please answer YES/NO directly and do not enter anything else.\n\nHere is model response: {response}"
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": 7,
            "desc": "<code>- # 写注释",
            "other_info": {
                "from": "system_para_6"
            },
            "dimension": "example_driven",
            "type": "formatting",
            "is_meta": false,
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "检查文本是否以<code>后接注释（包括单行或多行注释）开头，或无<code>时直接以注释开头。注释可以是#、'''或\"\"\"形式。直接回答YES或NO。\n文本：{response}\n回答："
                }
            ],
            "llm_output": "NO",
            "score": false
        },
        {
            "id": 0,
            "desc": "<task>\n本周的最佳球员是谁？他的主教练叫什么名字？他最初就读的是哪所学校？\n</task>",
            "other_info": {
                "shot_query": "\n本周最佳球员奖花落谁家？这位球员的教练是谁？他的启蒙学校又是哪一所？\n",
                "shot_function": [
                    "search"
                ],
                "shot_paraphrase": "本周的最佳球员是谁？他的主教练叫什么名字？他最初就读的是哪所学校？",
                "from": "system_para_4"
            },
            "dimension": "example_driven",
            "type": "resource",
            "is_meat": false,
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "请帮我提取出下面这段话中**当前步**使用的函数名称，不关注未来要做的事。\n请仅回答函数名称，并以list形式直接返回给我，不要回答其他的内容。\n\n{response}\n"
                },
                {
                    "type": "code",
                    "exec": "\ndef check_following(response):\n    import re\n\n    def extract_tool(text):\n        pattern = r'\\[([^]]+)\\]'\n        matches = re.findall(pattern, text)\n        if not matches:\n            return []\n        try:\n            tools = eval(f\"[{matches[-1]}]\")\n            return tools\n        except Exception as e:\n            # print(\"error in extract_tool:\", e)\n            return []\n\n    try:\n        tools = ['search']\n        functions = extract_tool(response)\n        if isinstance(functions, list):\n            for item in functions:\n                if item not in tools:\n                    return False\n        return True\n    except Exception as e:\n        print(\"error in resource checker:\", e)\n        return False\n"
                }
            ],
            "score": false
        },
        {
            "id": 7,
            "desc": "<code>- # 写注释",
            "other_info": {
                "from": "system_para_6"
            },
            "dimension": "example_driven",
            "type": "formatting",
            "is_meta": false,
            "evaluation": [
                {
                    "type": "llm",
                    "exec": "检查文本是否以<code>后接注释（包括单行或多行注释）开头，或无<code>时直接以注释开头。注释可以是#、'''或\"\"\"形式。直接回答YES或NO。\n文本：{response}\n回答："
                }
            ],
            "llm_output": "NO",
            "score": false
        }
    ]
}